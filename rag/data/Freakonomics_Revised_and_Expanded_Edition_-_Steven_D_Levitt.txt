Freakonomics


A Rogue Economist Explores the Hidden Side of Everything

Revised and Expanded Edition

Steven D. Levitt

and

Stephen J. Dubner

Contents

An Explanatory Note


Preface to the Revised and Expanded Edition


Introduction: The Hidden Side of Everything


1. What Do Schoolteachers and Sumo Wrestlers Have in Common?


2. How Is the Ku Klux Klan Like a Group of Real-Estate Agents?


3. Why Do Drug Dealers Still Live with Their Moms?


4. Where Have All the Criminals Gone?


5. What Makes a Perfect Parent?


6. Perfect Parenting, Part II; or: Would a Roshanda by Any Other Name Smell as Sweet?


Epilogue: Two Paths to Harvard


Bonus Material Added to the Revised and Expanded 2006 Edition


Notes


Acknowledgments


Searchable Terms


About the Authors


Credits


Copyright


About the Publisher

AN EXPLANATORY NOTE


In the summer of 2003, the New York Times Magazine sent Stephen J. Dubner, an author and journalist, to write a profile of Steven D. Levitt, a heralded young economist at the University of Chicago.

Dubner, who was researching a book about the psychology of money, had lately been interviewing many economists and found that they often spoke English as if it were a fourth or fifth language. Levitt, who had just won the John Bates Clark Medal (a sort of junior Nobel Prize for young economists), had lately been interviewed by many journalists and found that their thinking wasn’t very…robust, as an economist might say.

But Levitt decided that Dubner wasn’t a complete idiot. And Dubner found that Levitt wasn’t a human slide rule. The writer was dazzled by the inventiveness of the economist’s work and his knack for explaining it. Despite Levitt’s elite credentials (Harvard undergrad, a PhD from MIT, a stack of awards), he approached economics in a notably unorthodox way. He seemed to look at the world not so much as an academic but as a very smart and curious explorer—a documentary filmmaker, perhaps, or a forensic investigator or a bookie whose markets ranged from sports to crime to pop culture. He professed little interest in the sort of monetary issues that come to mind when most people think about economics; he practically blustered with self-effacement. “I just don’t know very much about the field of economics,” he told Dubner at one point, swiping the hair from his eyes. “I’m not good at math, I don’t know a lot of econometrics, and I also don’t know how to do theory. If you ask me about whether the stock market’s going to go up or down, if you ask me whether the economy’s going to grow or shrink, if you ask me whether deflation’s good or bad, if you ask me about taxes—I mean, it would be total fakery if I said I knew anything about any of those things.”

What interested Levitt were the riddles of everyday life. His investigations were a feast for anyone wanting to know how the world really works. His singular attitude was evoked in Dubner’s resulting article:


As Levitt sees it, economics is a science with excellent tools for gaining answers but a serious shortage of interesting questions. His particular gift is the ability to ask such questions. For instance: If drug dealers make so much money, why do they still live with their mothers? Which is more dangerous, a gun or a swimming pool? What really caused crime rates to plunge during the past decade? Do real-estate agents have their clients’ best interests at heart? Why do black parents give their children names that may hurt their career prospects? Do schoolteachers cheat to meet high-stakes testing standards? Is sumo wrestling corrupt?

Many people—including a fair number of his peers—might not recognize Levitt’s work as economics at all. But he has merely distilled the so-called dismal science to its most primal aim: explaining how people get what they want. Unlike most academics, he is unafraid of using personal observations and curiosities; he is also unafraid of anecdote and storytelling (although he is afraid of calculus). He is an intuitionist. He sifts through a pile of data to find a story that no one else has found. He figures a way to measure an effect that veteran economists had declared unmeasurable. His abiding interests—though he says he has never trafficked in them himself—are cheating, corruption, and crime.

Levitt’s blazing curiosity also proved attractive to thousands of New York Times readers. He was beset by questions and queries, riddles and requests—from General Motors and the New York Yankees and U.S. senators but also from prisoners and parents and a man who for twenty years had kept precise data on his sales of bagels. A former Tour de France champion called Levitt to ask his help in proving that the current Tour is rife with doping; the Central Intelligence Agency wanted to know how Levitt might use data to catch money launderers and terrorists.

What they were all responding to was the force of Levitt’s underlying belief: that the modern world, despite a surfeit of obfuscation, complication, and downright deceit, is not impenetrable, is not unknowable, and—if the right questions are asked—is even more intriguing than we think. All it takes is a new way of looking.

In New York City, the publishers were telling Levitt he should write a book.

“Write a book?” he said. “I don’t want to write a book.” He already had a million more riddles to solve than time to solve them. Nor did he think himself much of a writer. So he said that no, he wasn’t interested—“unless,” he proposed, “maybe Dubner and I could do it together.”

Collaboration isn’t for everyone. But the two of them—henceforth known as the two of us—decided to talk things over to see if such a book might work. We decided it could. We hope you agree.

PREFACE TO THE REVISED AND EXPANDED EDITION


As we were writing Freakonomics, we had grave doubts that anyone would actually read it—and we certainly never envisioned the need for this revised and expanded edition. But we are very happy, and grateful, to have been wrong.

So why bother with a revised edition?

There are a few reasons. The first is that the world is a living, breathing, changing thing, whereas a book is not. Once a manuscript is finished, it sits, dead in the water, for nearly a year until it is made ready by the publisher for its debut. This doesn’t pose much of a problem if you have written, say, a history of the Third Punic War. But because Freakonomics explores all sorts of modern real-world issues, and because the modern world tends to change quite fast, we have gone through the book and made a number of minor updates.

Also, we made some mistakes. It was usually a reader who would bring a mistake to our attention, and we very much appreciate this input. Again, most of these changes are quite minor.

The most aggressively revised section of the book is the beginning of chapter 2, which tells the story of one man’s crusade against the Ku Klux Klan. Several months after Freakonomics was first published, it was brought to our attention that this man’s portrayal of his crusade, and of various other Klan matters, was considerably overstated. (For a fuller explanation, see an essay called “Hoodwinked?”.) As unpleasant as it was to acknowledge this error, and to diminish the reputation of a man beloved in many quarters, we felt it was important to set straight the historical record.

We have also futzed a bit with the architecture of the book. In the original version, each chapter was preceded by an excerpt from the New York Times Magazine profile that one of us (Dubner) wrote about the other (Levitt), and which led to our collaboration on this book. Because some readers found these excerpts intrusive (and/or egomaniacal, and/or sycophantic), we have removed them, instead reprinting the complete Times profile in the back of this edition in the section called “Bonus Material”. There, it can be easily skipped over if one so chooses, or read in isolation.

The further bonus material is what accounts for our having called this edition “expanded” in addition to “revised.” Soon after the original publication of Freakonomics, in April 2005, we began writing a monthly column for the New York Times Magazine. We have included in this edition several of these columns, on subjects ranging from voting behavior to dog poop to the economics of sexual preference.

We have also included a variety of writings from our blog (www.freakonomics.com/blog/)—which, like this revised edition, was not planned. In the beginning, we built a website merely to perform archival and trafficking functions. We blogged reluctantly, tentatively, infrequently. But as the months went on, and as we discovered an audience of people who had read Freakonomics and were eager to bat its ideas back and forth, we took to it more enthusiastically.

A blog, as it turns out, is an author’s perfect antidote for that sickening feeling of being dead in the water once a manuscript has been completed. Particularly for a book like this one, a book of ideas, there is nothing more intoxicating than to be able to extend those ideas, to continue to refine and challenge and wrestle with them, even as the world marches on.

INTRODUCTION:

The Hidden Side of Everything


Anyone living in the United States in the early 1990s and paying even a whisper of attention to the nightly news or a daily paper could be forgiven for having been scared out of his skin.

The culprit was crime. It had been rising relentlessly—a graph plotting the crime rate in any American city over recent decades looked like a ski slope in profile—and it seemed now to herald the end of the world as we knew it. Death by gunfire, intentional and otherwise, had become commonplace. So too had carjacking and crack dealing, robbery and rape. Violent crime was a gruesome, constant companion. And things were about to get even worse. Much worse. All the experts were saying so.

The cause was the so-called superpredator. For a time, he was everywhere. Glowering from the cover of newsweeklies. Swaggering his way through foot-thick government reports. He was a scrawny, big-city teenager with a cheap gun in his hand and nothing in his heart but ruthlessness. There were thousands out there just like him, we were told, a generation of killers about to hurl the country into deepest chaos.

In 1995 the criminologist James Alan Fox wrote a report for the U.S. attorney general that grimly detailed the coming spike in murders by teenagers. Fox proposed optimistic and pessimistic scenarios. In the optimistic scenario, he believed, the rate of teen homicides would rise another 15 percent over the next decade; in the pessimistic scenario, it would more than double. “The next crime wave will get so bad,” he said, “that it will make 1995 look like the good old days.”

Other criminologists, political scientists, and similarly learned forecasters laid out the same horrible future, as did President Clinton. “We know we’ve got about six years to turn this juvenile crime thing around,” Clinton said, “or our country is going to be living with chaos. And my successors will not be giving speeches about the wonderful opportunities of the global economy; they’ll be trying to keep body and soul together for people on the streets of these cities.” The smart money was plainly on the criminals.

And then, instead of going up and up and up, crime began to fall. And fall and fall and fall some more. The crime drop was startling in several respects. It was ubiquitous, with every category of crime falling in every part of the country. It was persistent, with incremental decreases year after year. And it was entirely unanticipated—especially by the very experts who had been predicting the opposite.

The magnitude of the reversal was astounding. The teenage murder rate, instead of rising 100 percent or even 15 percent as James Alan Fox had warned, fell more than 50 percent within five years. By 2000 the overall murder rate in the United States had dropped to its lowest level in thirty-five years. So had the rate of just about every other sort of crime, from assault to car theft.

Even though the experts had failed to anticipate the crime drop—which was in fact well under way even as they made their horrifying predictions—they now hurried to explain it. Most of their theories sounded perfectly logical. It was the roaring 1990s economy, they said, that helped turn back crime. It was the proliferation of gun control laws, they said. It was the sort of innovative policing strategies put into place in New York City, where murders would fall from 2,262 in 1990 to 540 in 2005.

These theories were not only logical; they were also encouraging, for they attributed the crime drop to specific and recent human initiatives. If it was gun control and clever police strategies and better-paying jobs that quelled crime—well then, the power to stop criminals had been within our reach all along. As it would be the next time, God forbid, that crime got so bad.

These theories made their way, seemingly without friction, from the experts’ mouths to journalists’ ears to the public’s mind. In short course, they became conventional wisdom.

There was only one problem: they weren’t true.

There was another factor, meanwhile, that had greatly contributed to the massive crime drop of the 1990s. It had taken shape more than twenty years earlier and concerned a young woman in Dallas named Norma McCorvey.

Like the proverbial butterfly that flaps its wings on one continent and eventually causes a hurricane on another, Norma McCorvey dramatically altered the course of events without intending to. All she had wanted was an abortion. She was a poor, uneducated, unskilled, alcoholic, drug-using twenty-one-year-old woman who had already given up two children for adoption and now, in 1970, found herself pregnant again. But in Texas, as in all but a few states at that time, abortion was illegal. McCorvey’s cause came to be adopted by people far more powerful than she. They made her the lead plaintiff in a class-action lawsuit seeking to legalize abortion. The defendant was Henry Wade, the Dallas County district attorney. The case ultimately made it to the U.S. Supreme Court, by which time McCorvey’s name had been disguised as Jane Roe. On January 22, 1973, the court ruled in favor of Ms. Roe, allowing legalized abortion throughout the United States. By this time, of course, it was far too late for Ms. McCorvey/Roe to have her abortion. She had given birth and put the child up for adoption. (Years later she would renounce her allegiance to legalized abortion and become a pro-life activist.)

So how did Roe v. Wade help trigger, a generation later, the greatest crime drop in recorded history?

As far as crime is concerned, it turns out that not all children are born equal. Not even close. Decades of studies have shown that a child born into an adverse family environment is far more likely than other children to become a criminal. And the millions of women most likely to have an abortion in the wake of Roe v. Wade—poor, unmarried, and teenage mothers for whom illegal abortions had been too expensive or too hard to get—were often models of adversity. They were the very women whose children, if born, would have been much more likely than average to become criminals. But because of Roe v. Wade, these children weren’t being born. This powerful cause would have a drastic, distant effect: years later, just as these unborn children would have entered their criminal primes, the rate of crime began to plummet.

It wasn’t gun control or a strong economy or new police strategies that finally blunted the American crime wave. It was, among other factors, the reality that the pool of potential criminals had dramatically shrunk.

Now, as the crime-drop experts (the former crime doomsayers) spun their theories to the media, how many times did they cite legalized abortion as a cause?

Zero.

It is the quintessential blend of commerce and camaraderie: you hire a real-estate agent to sell your home.

She sizes up its charms, snaps some pictures, sets the price, writes a seductive ad, shows the house aggressively, negotiates the offers, and sees the deal through to its end. Sure, it’s a lot of work, but she’s getting a nice cut. On the sale of a $300,000 house, a typical 6 percent agent fee yields $18,000. Eighteen thousand dollars, you say to yourself: that’s a lot of money. But you also tell yourself that you never could have sold the house for $300,000 on your own. The agent knew how to—what’s that phrase she used?—“maximize the house’s value.” She got you top dollar, right?

Right?

A real-estate agent is a different breed of expert than a criminologist, but she is every bit the expert. That is, she knows her field far better than the layman on whose behalf she is acting. She is better informed about the house’s value, the state of the housing market, even the buyer’s frame of mind. You depend on her for this information. That, in fact, is why you hired an expert.

As the world has grown more specialized, countless such experts have made themselves similarly indispensable. Doctors, lawyers, contractors, stockbrokers, auto mechanics, mortgage brokers, financial planners: they all enjoy a gigantic informational advantage. And they use that advantage to help you, the person who hired them, get exactly what you want for the best price.

Right?

It would be lovely to think so. But experts are human, and humans respond to incentives. How any given expert treats you, therefore, will depend on how that expert’s incentives are set up. Sometimes his incentives may work in your favor. For instance: a study of California auto mechanics found they often passed up a small repair bill by letting failing cars pass emissions inspections—the reason being that lenient mechanics are rewarded with repeat business. But in a different case, an expert’s incentives may work against you. In a medical study, it turned out that obstetricians in areas with declining birth rates are much more likely to perform cesarean-section deliveries than obstetricians in growing areas—suggesting that, when business is tough, doctors try to ring up more expensive procedures.

It is one thing to muse about experts’ abusing their position and another to prove it. The best way to do so would be to measure how an expert treats you versus how he performs the same service for himself. Unfortunately a surgeon doesn’t operate on himself. Nor is his medical file a matter of public record; neither is an auto mechanic’s repair log for his own car.

Real-estate sales, however, are a matter of public record. And real-estate agents often do sell their own homes. A recent set of data covering the sale of nearly 100,000 houses in suburban Chicago shows that more than 3,000 of those houses were owned by the agents themselves.

Before plunging into the data, it helps to ask a question: what is the real-estate agent’s incentive when she is selling her own home? Simple: to make the best deal possible. Presumably this is also your incentive when you are selling your home. And so your incentive and the real-estate agent’s incentive would seem to be nicely aligned. Her commission, after all, is based on the sale price.

But as incentives go, commissions are tricky. First of all, a 6 percent real-estate commission is typically split between the seller’s agent and the buyer’s. Each agent then kicks back roughly half of her take to the agency. Which means that only 1.5 percent of the purchase price goes directly into your agent’s pocket.

So on the sale of your $300,000 house, her personal take of the $18,000 commission is $4,500. Still not bad, you say. But what if the house was actually worth more than $300,000? What if, with a little more effort and patience and a few more newspaper ads, she could have sold it for $310,000? After the commission, that puts an additional $9,400 in your pocket. But the agent’s additional share—her personal 1.5 percent of the extra $10,000—is a mere $150. If you earn $9,400 while she earns only $150, maybe your incentives aren’t aligned after all. (Especially when she’s the one paying for the ads and doing all the work.) Is the agent willing to put out all that extra time, money, and energy for just $150?

There’s one way to find out: measure the difference between the sales data for houses that belong to real-estate agents themselves and the houses they sold on behalf of clients. Using the data from the sales of those 100,000 Chicago homes, and controlling for any number of variables—location, age and quality of the house, aesthetics, whether or not the property was an investment, and so on—it turns out that a real-estate agent keeps her own home on the market an average of ten days longer and sells it for an extra 3-plus percent, or $10,000 on a $300,000 house. When she sells her own house, an agent holds out for the best offer; when she sells yours, she encourages you to take the first decent offer that comes along. Like a stockbroker churning commissions, she wants to make deals and make them fast. Why not? Her share of a better offer—$150—is too puny an incentive to encourage her to do otherwise.

Of all the truisms about politics, one is held to be truer than the rest: money buys elections. Arnold Schwarzenegger, Michael Bloomberg, Jon Corzine—these are but a few recent, dramatic examples of the truism at work. (Disregard for a moment the contrary examples of Steve Forbes, Michael Huffington, and especially Thomas Golisano, who over the course of three gubernatorial elections in New York spent $93 million of his own money and won 4 percent, 8 percent, and 14 percent, respectively, of the vote.) Most people would agree that money has an undue influence on elections and that far too much money is spent on political campaigns.

Indeed, election data show it is true that the candidate who spends more money in a campaign usually wins. But is money the cause of the victory?

It might seem logical to think so, much as it might have seemed logical that a booming 1990s economy helped reduce crime. But just because two things are correlated does not mean that one causes the other. A correlation simply means that a relationship exists between two factors—let’s call them X and Y—but it tells you nothing about the direction of that relationship. It’s possible that X causes Y; it’s also possible that Y causes X; and it may be that X and Y are both being caused by some other factor, Z.

Think about this correlation: cities with a lot of murders also tend to have a lot of police officers. Consider now the police/murder correlation in a pair of real cities. Denver and Washington, D.C., have about the same population—but Washington has nearly three times as many police as Denver, and it also has eight times the number of murders. Unless you have more information, however, it’s hard to say what’s causing what. Someone who didn’t know better might contemplate these figures and conclude that it is all those extra police in Washington who are causing the extra murders. Such wayward thinking, which has a long history, generally provokes a wayward response. Consider the folktale of the czar who learned that the most disease-ridden province in his empire was also the province with the most doctors. His solution? He promptly ordered all the doctors shot dead.

Now, returning to the issue of campaign spending: in order to figure out the relationship between money and elections, it helps to consider the incentives at play in campaign finance. Let’s say you are the kind of person who might contribute $1,000 to a candidate. Chances are you’ll give the money in one of two situations: a close race, in which you think the money will influence the outcome; or a campaign in which one candidate is a sure winner and you would like to bask in reflected glory or receive some future in-kind consideration. The one candidate you won’t contribute to is a sure loser. (Just ask any presidential hopeful who bombs in Iowa and New Hampshire.) So front-runners and incumbents raise a lot more money than long shots. And what about spending that money? Incumbents and front-runners obviously have more cash, but they only spend a lot of it when they stand a legitimate chance of losing; otherwise, why dip into a war chest that might be more useful later on, when a more formidable opponent appears?

Now picture two candidates, one intrinsically appealing and the other not so. The appealing candidate raises much more money and wins easily. But was it the money that won him the votes, or was it his appeal that won the votes and the money?

That’s a crucial question but a very hard one to answer. Voter appeal, after all, isn’t easy to quantify. How can it be measured?

It can’t, really—except in one special case. The key is to measure a candidate against…himself. That is, Candidate A today is likely to be similar to Candidate A two or four years hence. The same could be said for Candidate B. If only Candidate A ran against Candidate B in two consecutive elections but in each case spent different amounts of money. Then, with the candidates’ appeal more or less constant, we could measure the money’s impact.

As it turns out, the same two candidates run against each other in consecutive elections all the time—indeed, in nearly a thousand U.S. congressional races since 1972. What do the numbers have to say about such cases?

Here’s the surprise: the amount of money spent by the candidates hardly matters at all. A winning candidate can cut his spending in half and lose only 1 percent of the vote. Meanwhile, a losing candidate who doubles his spending can expect to shift the vote in his favor by only that same 1 percent. What really matters for a political candidate is not how much you spend; what matters is who you are. (The same could be said—and will be said, in chapter 5—about parents.) Some politicians are inherently attractive to voters and others simply aren’t, and no amount of money can do much about it. (Messrs. Forbes, Huffington, and Golisano already know this, of course.)

And what about the other half of the election truism—that the amount of money spent on campaign finance is obscenely huge? In a typical election period that includes campaigns for the presidency, the Senate, and the House of Representatives, about $1 billion is spent per year—which sounds like a lot of money, unless you care to measure it against something seemingly less important than democratic elections.

It is the same amount, for instance, that Americans spend every year on chewing gum.

This isn’t a book about the cost of chewing gum versus campaign spending per se, or about disingenuous real-estate agents, or the impact of legalized abortion on crime. It will certainly address these scenarios and dozens more, from the art of parenting to the mechanics of cheating, from the inner workings of a crack-selling gang to racial discrimination on The Weakest Link. What this book is about is stripping a layer or two from the surface of modern life and seeing what is happening underneath. We will ask a lot of questions, some frivolous and some about life-and-death issues. The answers may often seem odd but, after the fact, also rather obvious. We will seek out these answers in the data—whether those data come in the form of schoolchildren’s test scores or New York City’s crime statistics or a crack dealer’s financial records. Often we will take advantage of patterns in the data that were incidentally left behind, like an airplane’s sharp contrail in a high sky. It is well and good to opine or theorize about a subject, as humankind is wont to do, but when moral posturing is replaced by an honest assessment of the data, the result is often a new, surprising insight.

Morality, it could be argued, represents the way that people would like the world to work—whereas economics represents how it actually does work. Economics is above all a science of measurement. It comprises an extraordinarily powerful and flexible set of tools that can reliably assess a thicket of information to determine the effect of any one factor, or even the whole effect. That’s what “the economy” is, after all: a thicket of information about jobs and real estate and banking and investment. But the tools of economics can be just as easily applied to subjects that are more—well, more interesting.

This book, then, has been written from a very specific worldview, based on a few fundamental ideas:

Incentives are the cornerstone of modern life. And understanding them—or, often, ferreting them out—is the key to solving just about any riddle, from violent crime to sports cheating to online dating.

The conventional wisdom is often wrong. Crime didn’t keep soaring in the 1990s, money alone doesn’t win elections, and—surprise—drinking eight glasses of water a day has never actually been shown to do a thing for your health. Conventional wisdom is often shoddily formed and devilishly difficult to see through, but it can be done.

Dramatic effects often have distant, even subtle, causes. The answer to a given riddle is not always right in front of you. Norma McCorvey had a far greater impact on crime than did the combined forces of gun control, a strong economy, and innovative police strategies. So did, as we shall see, a man named Oscar Danilo Blandon, aka the Johnny Appleseed of Crack.

“Experts”—from criminologists to real-estate agents—use their informational advantage to serve their own agenda. However, they can be beat at their own game. And in the face of the Internet, their informational advantage is shrinking every day—as evidenced by, among other things, the falling price of coffins and life-insurance premiums.

Knowing what to measure and how to measure it makes a complicated world much less so. If you learn to look at data in the right way, you can explain riddles that otherwise might have seemed impossible. Because there is nothing like the sheer power of numbers to scrub away layers of confusion and contradiction.

So the aim of this book is to explore the hidden side of…everything. This may occasionally be a frustrating exercise. It may sometimes feel as if we are peering at the world through a straw or even staring into a funhouse mirror; but the idea is to look at many different scenarios and examine them in a way they have rarely been examined. In some regards, this is a strange concept for a book. Most books put forth a single theme, crisply expressed in a sentence or two, and then tell the entire story of that theme: the history of salt; the fragility of democracy; the use and misuse of punctuation. This book has no such unifying theme. We did consider, for about six minutes, writing a book that would revolve around a single theme—the theory and practice of applied microeconomics, anyone?—but opted instead for a sort of treasure-hunt approach. Yes, this approach employs the best analytical tools that economics can offer, but it also allows us to follow whatever freakish curiosities may occur to us. Thus our invented field of study: Freakonomics. The sort of stories told in this book are not often covered in Econ 101, but that may change. Since the science of economics is primarily a set of tools, as opposed to a subject matter, then no subject, however offbeat, need be beyond its reach.

It is worth remembering that Adam Smith, the founder of classical economics, was first and foremost a philosopher. He strove to be a moralist and, in doing so, became an economist. When he published The Theory of Moral Sentiments in 1759, modern capitalism was just getting under way. Smith was entranced by the sweeping changes wrought by this new force, but it wasn’t just the numbers that interested him. It was the human effect, the fact that economic forces were vastly changing the way a person thought and behaved in a given situation. What might lead one person to cheat or steal while another didn’t? How would one person’s seemingly innocuous choice, good or bad, affect a great number of people down the line? In Smith’s era, cause and effect had begun to wildly accelerate; incentives were magnified tenfold. The gravity and shock of these changes were as overwhelming to the citizens of his time as the gravity and shock of modern life may seem to us today.

Smith’s true subject was the friction between individual desire and societal norms. The economic historian Robert Heilbroner, writing in The Worldly Philosophers, wondered how Smith was able to separate the doings of man, a creature of self-interest, from the greater moral plane in which man operated. “Smith held that the answer lay in our ability to put ourselves in the position of a third person, an impartial observer,” Heilbroner wrote, “and in this way to form a notion of the objective…merits of a case.”

Consider yourself, then, in the company of a third person—or, if you will, a pair of third people—eager to explore the objective merits of interesting cases. These explorations generally begin with the asking of a simple unasked question. Such as: what do schoolteachers and sumo wrestlers have in common?

1


What Do Schoolteachers and Sumo Wrestlers Have in Common?


Imagine for a moment that you are the manager of a day-care center. You have a clearly stated policy that children are supposed to be picked up by 4 p.m. But very often parents are late. The result: at day’s end, you have some anxious children and at least one teacher who must wait around for the parents to arrive. What to do?

A pair of economists who heard of this dilemma—it turned out to be a rather common one—offered a solution: fine the tardy parents. Why, after all, should the day-care center take care of these kids for free?

The economists decided to test their solution by conducting a study of ten day-care centers in Haifa, Israel. The study lasted twenty weeks, but the fine was not introduced immediately. For the first four weeks, the economists simply kept track of the number of parents who came late; there were, on average, eight late pickups per week per day-care center. In the fifth week, the fine was enacted. It was announced that any parent arriving more than ten minutes late would pay $3 per child for each incident. The fee would be added to the parents’ monthly bill, which was roughly $380.

After the fine was enacted, the number of late pickups promptly went…up. Before long there were twenty late pickups per week, more than double the original average. The incentive had plainly backfired.

Economics is, at root, the study of incentives: how people get what they want, or need, especially when other people want or need the same thing. Economists love incentives. They love to dream them up and enact them, study them and tinker with them. The typical economist believes the world has not yet invented a problem that he cannot fix if given a free hand to design the proper incentive scheme. His solution may not always be pretty—it may involve coercion or exorbitant penalties or the violation of civil liberties—but the original problem, rest assured, will be fixed. An incentive is a bullet, a lever, a key: an often tiny object with astonishing power to change a situation.

We all learn to respond to incentives, negative and positive, from the outset of life. If you toddle over to the hot stove and touch it, you burn a finger. But if you bring home straight A’s from school, you get a new bike. If you are spotted picking your nose in class, you get ridiculed. But if you make the basketball team, you move up the social ladder. If you break curfew, you get grounded. But if you ace your SATs, you get to go to a good college. If you flunk out of law school, you have to go to work at your father’s insurance company. But if you perform so well that a rival company comes calling, you become a vice president and no longer have to work for your father. If you become so excited about your new vice president job that you drive home at eighty mph, you get pulled over by the police and fined $100. But if you hit your sales projections and collect a year-end bonus, you not only aren’t worried about the $100 ticket but can also afford to buy that Viking range you’ve always wanted—and on which your toddler can now burn her own finger.

An incentive is simply a means of urging people to do more of a good thing and less of a bad thing. But most incentives don’t come about organically. Someone—an economist or a politician or a parent—has to invent them. Your three-year-old eats all her vegetables for a week? She wins a trip to the toy store. A big steelmaker belches too much smoke into the air? The company is fined for each cubic foot of pollutants over the legal limit. Too many Americans aren’t paying their share of income tax? It was the economist Milton Friedman who helped come up with a solution to this one: automatic tax withholding from employees’ paychecks.

There are three basic flavors of incentive: economic, social, and moral. Very often a single incentive scheme will include all three varieties. Think about the anti-smoking campaign of recent years. The addition of a $3-per-pack “sin tax” is a strong economic incentive against buying cigarettes. The banning of cigarettes in restaurants and bars is a powerful social incentive. And when the U.S. government asserts that terrorists raise money by selling black-market cigarettes, that acts as a rather jarring moral incentive.

Some of the most compelling incentives yet invented have been put in place to deter crime. Considering this fact, it might be worthwhile to take a familiar question—why is there so much crime in modern society?—and stand it on its head: why isn’t there a lot more crime?

After all, every one of us regularly passes up opportunities to maim, steal, and defraud. The chance of going to jail—thereby losing your job, your house, and your freedom, all of which are essentially economic penalties—is certainly a strong incentive. But when it comes to crime, people also respond to moral incentives (they don’t want to do something they consider wrong) and social incentives (they don’t want to be seen by others as doing something wrong). For certain types of misbehavior, social incentives are terribly powerful. In an echo of Hester Prynne’s scarlet letter, many American cities now fight prostitution with a “shaming” offensive, posting pictures of convicted johns (and prostitutes) on websites or on local-access television. Which is a more horrifying deterrent: a $500 fine for soliciting a prostitute or the thought of your friends and family ogling you on www.HookersAndJohns.com?

So through a complicated, haphazard, and constantly readjusted web of economic, social, and moral incentives, modern society does its best to militate against crime. Some people would argue that we don’t do a very good job. But taking the long view, that is clearly not true. Consider the historical trend in homicide (not including wars), which is both the most reliably measured crime and the best barometer of a society’s overall crime rate. These statistics, compiled by the criminologist Manuel Eisner, track the historical homicide levels in five European regions.

HOMICIDES

(per 100,000 People)

The steep decline of these numbers over the centuries suggests that, for one of the gravest human concerns—getting murdered—the incentives that we collectively cook up are working better and better.

So what was wrong with the incentive at the Israeli day-care centers?

You have probably already guessed that the $3 fine was simply too small. For that price, a parent with one child could afford to be late every day and only pay an extra $60 each month—just one-sixth of the base fee. As babysitting goes, that’s pretty cheap. What if the fine had been set at $100 instead of $3? That would have likely put an end to the late pickups, though it would have also engendered plenty of ill will. (Any incentive is inherently a trade-off; the trick is to balance the extremes.)

But there was another problem with the day-care center fine. It substituted an economic incentive (the $3 penalty) for a moral incentive (the guilt that parents were supposed to feel when they came late). For just a few dollars each day, parents could buy off their guilt. Furthermore, the small size of the fine sent a signal to the parents that late pickups weren’t such a big problem. If the day-care center suffers only $3 worth of pain for each late pickup, why bother to cut short your tennis game? Indeed, when the economists eliminated the $3 fine in the seventeenth week of their study, the number of late-arriving parents didn’t change. Now they could arrive late, pay no fine, and feel no guilt.

Such is the strange and powerful nature of incentives. A slight tweak can produce drastic and often unforeseen results. Thomas Jefferson noted this while reflecting on the tiny incentive that led to the Boston Tea Party and, in turn, the American Revolution: “So inscrutable is the arrangement of causes and consequences in this world that a two-penny duty on tea, unjustly imposed in a sequestered part of it, changes the condition of all its inhabitants.”

In the 1970s, researchers conducted a study that, like the Israeli day-care study, pitted a moral incentive against an economic incentive. In this case, they wanted to learn about the motivation behind blood donations. Their discovery: when people are given a small stipend for donating blood rather than simply being praised for their altruism, they tend to donate less blood. The stipend turned a noble act of charity into a painful way to make a few dollars, and it wasn’t worth it.

What if the blood donors had been offered an incentive of $50, or $500, or $5,000? Surely the number of donors would have changed dramatically.

But something else would have changed dramatically as well, for every incentive has its dark side. If a pint of blood were suddenly worth $5,000, you can be sure that plenty of people would take note. They might literally steal blood at knifepoint. They might pass off pig blood as their own. They might circumvent donation limits by using fake IDs. Whatever the incentive, whatever the situation, dishonest people will try to gain an advantage by whatever means necessary.

Or, as W. C. Fields once said: a thing worth having is a thing worth cheating for.

Who cheats?

Well, just about anyone, if the stakes are right. You might say to yourself, I don’t cheat, regardless of the stakes. And then you might remember the time you cheated on, say, a board game. Last week. Or the golf ball you nudged out of its bad lie. Or the time you really wanted a bagel in the office break room but couldn’t come up with the dollar you were supposed to drop in the coffee can. And then took the bagel anyway. And told yourself you’d pay double the next time. And didn’t.

For every clever person who goes to the trouble of creating an incentive scheme, there is an army of people, clever and otherwise, who will inevitably spend even more time trying to beat it. Cheating may or may not be human nature, but it is certainly a prominent feature in just about every human endeavor. Cheating is a primordial economic act: getting more for less. So it isn’t just the boldface names—inside-trading CEOs and pill-popping ballplayers and perk-abusing politicians—who cheat. It is the waitress who pockets her tips instead of pooling them. It is the Wal-Mart payroll manager who goes into the computer and shaves his employees’ hours to make his own performance look better. It is the third grader who, worried about not making it to the fourth grade, copies test answers from the kid sitting next to him.

Some cheating leaves barely a shadow of evidence. In other cases, the evidence is massive. Consider what happened one spring evening at midnight in 1987: seven million American children suddenly disappeared. The worst kidnapping wave in history? Hardly. It was the night of April 15, and the Internal Revenue Service had just changed a rule. Instead of merely listing the name of each dependent child, tax filers were now required to provide a Social Security number. Suddenly, seven million children—children who had existed only as phantom exemptions on the previous year’s 1040 forms—vanished, representing about one in ten of all dependent children in the United States.

The incentive for those cheating taxpayers was quite clear. The same for the waitress, the payroll manager, and the third grader. But what about that third grader’s teacher? Might she have an incentive to cheat? And if so, how would she do it?

Imagine now that instead of running a day-care center in Haifa, you are running the Chicago Public Schools, a system that educates 400,000 students each year.

The most volatile current debate among American school administrators, teachers, parents, and students concerns “high-stakes” testing. The stakes are considered high because instead of simply testing students to measure their progress, schools are increasingly held accountable for the results.

The federal government mandated high-stakes testing as part of the No Child Left Behind law, signed by President Bush in 2002. But even before that law, most states gave annual standardized tests to students in elementary and secondary school. Twenty states rewarded individual schools for good test scores or dramatic improvement; thirty-two states sanctioned the schools that didn’t do well.

The Chicago Public School system embraced high-stakes testing in 1996. Under the new policy, a school with low reading scores would be placed on probation and face the threat of being shut down, its staff to be dismissed or reassigned. The CPS also did away with what is known as social promotion. In the past, only a dramatically inept or difficult student was held back a grade. Now, in order to be promoted, every student in third, sixth, and eighth grade had to manage a minimum score on the standardized, multiple-choice exam known as the Iowa Test of Basic Skills.

Advocates of high-stakes testing argue that it raises the standards of learning and gives students more incentive to study. Also, if the test prevents poor students from advancing without merit, they won’t clog up the higher grades and slow down good students. Opponents, meanwhile, worry that certain students will be unfairly penalized if they don’t happen to test well, and that teachers may concentrate on the test topics at the exclusion of more important lessons.

Schoolchildren, of course, have had incentive to cheat for as long as there have been tests. But high-stakes testing has so radically changed the incentives for teachers that they too now have added reason to cheat. With high-stakes testing, a teacher whose students test poorly can be censured or passed over for a raise or promotion. If the entire school does poorly, federal funding can be withheld; if the school is put on probation, the teacher stands to be fired. High-stakes testing also presents teachers with some positive incentives. If her students do well enough, she might find herself praised, promoted, and even richer: the state of California at one point introduced bonuses of $25,000 for teachers who produced big test-score gains.

And if a teacher were to survey this newly incentivized landscape and consider somehow inflating her students’ scores, she just might be persuaded by one final incentive: teacher cheating is rarely looked for, hardly ever detected, and just about never punished.

How might a teacher go about cheating? There are any number of possibilities, from brazen to subtle. A fifth-grade student in Oakland recently came home from school and gaily told her mother that her super-nice teacher had written the answers to the state exam right there on the chalkboard. Such instances are certainly rare, for placing your fate in the hands of thirty prepubescent witnesses doesn’t seem like a risk that even the worst teacher would take. (The Oakland teacher was duly fired.) There are more nuanced ways to inflate students’ scores. A teacher can simply give students extra time to complete the test. If she obtains a copy of the exam early—that is, illegitimately—she can prepare them for specific questions. More broadly, she can “teach to the test,” basing her lesson plans on questions from past years’ exams, which isn’t considered cheating but may well violate the spirit of the test. Since these tests all have multiple-choice answers, with no penalty for wrong guesses, a teacher might instruct her students to randomly fill in every blank as the clock is winding down, perhaps inserting a long string of Bs or an alternating pattern of Bs and Cs. She might even fill in the blanks for them after they’ve left the room.

But if a teacher really wanted to cheat—and make it worth her while—she might collect her students’ answer sheets and, in the hour or so before turning them in to be read by an electronic scanner, erase the wrong answers and fill in correct ones. (And you always thought that no. 2 pencil was for the children to change their answers.) If this kind of teacher cheating is truly going on, how might it be detected?

To catch a cheater, it helps to think like one. If you were willing to erase your students’ wrong answers and fill in correct ones, you probably wouldn’t want to change too many wrong answers. That would clearly be a tip-off. You probably wouldn’t even want to change answers on every student’s test—another tip-off. Nor, in all likelihood, would you have enough time, because the answer sheets have to be turned in soon after the test is over. So what you might do is select a string of eight or ten consecutive questions and fill in the correct answers for, say, one-half or two-thirds of your students. You could easily memorize a short pattern of correct answers, and it would be a lot faster to erase and change that pattern than to go through each student’s answer sheet individually. You might even think to focus your activity toward the end of the test, where the questions tend to be harder than the earlier questions. In that way, you’d be most likely to substitute correct answers for wrong ones.

If economics is a science primarily concerned with incentives, it is also—fortunately—a science with statistical tools to measure how people respond to those incentives. All you need are some data.

In this case, the Chicago Public School system obliged. It made available a database of the test answers for every CPS student from third grade through seventh grade from 1993 to 2000. This amounts to roughly 30,000 students per grade per year, more than 700,000 sets of test answers, and nearly 100 million individual answers. The data, organized by classroom, included each student’s question-by-question answer strings for reading and math tests. (The actual paper answer sheets were not included; they were habitually shredded soon after a test.) The data also included some information about each teacher and demographic information for every student, as well as his or her past and future test scores—which would prove a key element in detecting the teacher cheating.

Now it was time to construct an algorithm that could tease some conclusions from this mass of data. What might a cheating teacher’s classroom look like?

The first thing to search for would be unusual answer patterns in a given classroom: blocks of identical answers, for instance, especially among the harder questions. If ten very bright students (as indicated by past and future test scores) gave correct answers to the exam’s first five questions (typically the easiest ones), such an identical block shouldn’t be considered suspicious. But if ten poor students gave correct answers to the last five questions on the exam (the hardest ones), that’s worth looking into. Another red flag would be a strange pattern within any one student’s exam—such as getting the hard questions right while missing the easy ones—especially when measured against the thousands of students in other classrooms who scored similarly on the same test. Furthermore, the algorithm would seek out a classroom full of students who performed far better than their past scores would have predicted and who then went on to score significantly lower the following year. A dramatic one-year spike in test scores might initially be attributed to a good teacher; but with a dramatic fall to follow, there’s a strong likelihood that the spike was brought about by artificial means.

Consider now the answer strings from the students in two sixth-grade Chicago classrooms who took the identical math test. Each horizontal row represents one student’s answers. The letter a, b, c, or d indicates a correct answer; a number indicates a wrong answer, with 1 corresponding to a, 2 corresponding to b, and so on. A zero represents an answer that was left blank. One of these classrooms almost certainly had a cheating teacher and the other did not. Try to tell the difference—although be forewarned that it’s not easy with the naked eye.


Classroom A

112a4a342cb214d0001acd24a3a12dadbcb4a0000000

d4a2341cacbddad3142a2344a2ac23421c00adb4b3cb

1b2a34d4ac42d23b141acd24a3a12dadbcb4a2134141

dbaab3dcacb1dadbc42ac2cc31012dadbcb4adb40000

d12443d43232d32323c213c22d2c23234c332db4b300

db2abad1acbdda212b1acd24a3a12dadbcb400000000

d4aab2124cbddadbcb1a42cca3412dadbcb423134bc1

1b33b4d4a2b1dadbc3ca22c000000000000000000000

d43a3a24acb1d32b412acd24a3a12dadbcb422143bc0

313a3ad1ac3d2a23431223c000012dadbcb400000000

db2a33dcacbd32d313c21142323cc300000000000000

d43ab4d1ac3dd43421240d24a3a12dadbcb400000000

db223a24acb11a3b24cacd12a241cdadbcb4adb4b300

db4abadcacb1dad3141ac212a3a1c3a144ba2db41b43

1142340c2cbddadb4b1acd24a3a12dadbcb43d133bc4

214ab4dc4cbdd31b1b2213c4ad412dadbcb4adb00000

1423b4d4a23d24131413234123a243a2413a21441343

3b3ab4d14c3d2ad4cbcac1c003a12dadbcb4adb40000

dba2ba21ac3d2ad3c4c4cd40a3a12dadbcb400000000

d122ba2cacbd1a13211a2d02a2412d0dbcb4adb4b3c0

144a3adc4cbddadbcbc2c2cc43a12dadbcb4211ab343

d43aba3cacbddadbcbca42c2a3212dadbcb42344b3cb

Classroom B

db3a431422bd131b4413cd422a1acda332342d3ab4c4

d1aa1a11acb2d3dbc1ca22c23242c3a142b3adb243c1

d42a12d2a4b1d32b21ca2312a3411d00000000000000

3b2a34344c32d21b1123cdc000000000000000000000

34aabad12cbdd3d4c1ca112cad2ccd00000000000000

d33a3431a2b2d2d44b2acd2cad2c2223b40000000000

23aa32d2a1bd2431141342c13d212d233c34a3b3b000

d32234d4a1bdd23b242a22c2a1a1cda2b1baa33a0000

d3aab23c4cbddadb23c322c2a222223232b443b24bc3

d13a14313c31d42b14c421c42332cd2242b3433a3343

d13a3ad122b1da2b11242dc1a3a12100000000000000

d12a3ad1a13d23d3cb2a21ccada24d2131b440000000

314a133c4cbd142141ca424cad34c122413223ba4b40

d42a3adcacbddadbc42ac2c2ada2cda341baa3b24321

db1134dc2cb2dadb24c412c1ada2c3a341ba20000000

d1341431acbddad3c4c213412da22d3d1132a1344b1b

1ba41a21a1b2dadb24ca22c1ada2cd32413200000000

dbaa33d2a2bddadbcbca11c2a2accda1b2ba20000000

If you guessed that classroom A was the cheating classroom, congratulations. Here again are the answer strings from classroom A, now reordered by a computer that has been asked to apply the cheating algorithm and seek out suspicious patterns.

Classroom A

(With cheating algorithm applied)

112a4a342cb214d0001acd24a3a12dadbcb4a0000000

1b2a34d4ac42d23b141acd24a3a12dadbcb4a2134141

db2abad1acbdda212b1acd24a3a12dadbcb400000000

d43a3a24acb1d32b412acd24a3a12dadbcb422143bc0

1142340c2cbddadb4b1acd24a3a12dadbcb43d133bc4

d43ab4d1ac3dd43421240d24a3a12dadbcb400000000

dba2ba21ac3d2ad3c4c4cd40a3a12dadbcb400000000

144a3adc4cbddadbcbc2c2cc43a12dadbcb4211ab343

3b3ab4d14c3d2ad4cbcac1c003a12dadbcb4adb40000

d43aba3cacbddadbcbca42c2a3212dadbcb42344b3cb

214ab4dc4cbdd31b1b2213c4ad412dadbcb4adb00000

313a3ad1ac3d2a23431223c000012dadbcb400000000

d4aab2124cbddadbcb1a42cca3412dadbcb423134bc1

dbaab3dcacb1dadbc42ac2cc31012dadbcb4adb40000

db223a24acb11a3b24cacd12a241cdadbcb4adb4b300

d122ba2cacbd1a13211a2d02a2412d0dbcb4adb4b3c0

1423b4d4a23d24131413234123a243a2413a21441343

db4abadcacb1dad3141ac212a3a1c3a144ba2db41b43

db2a33dcacbd32d313c21142323cc300000000000000

1b33b4d4a2b1dadbc3ca22c000000000000000000000

d12443d43232d32323c213c22d2c23234c332db4b300

d4a2341cacbddad3142a2344a2ac23421c00adb4b3cb

Take a look at the answers in bold. Did fifteen out of twenty-two students somehow manage to reel off the same six consecutive correct answers (the d-a-d-b-c-b string) all by themselves?

There are at least four reasons this is unlikely. One: those questions, coming near the end of the test, were harder than the earlier questions. Two: these were mainly subpar students to begin with, few of whom got six consecutive right answers elsewhere on the test, making it all the more unlikely they would get right the same six hard questions. Three: up to this point in the test, the fifteen students’ answers were virtually uncorrelated. Four: three of the students (numbers 1, 9, and 12) left more than one answer blank before the suspicious string and then ended the test with another string of blanks. This suggests that a long, unbroken string of blank answers was broken not by the student but by the teacher.

There is another oddity about the suspicious answer string. On nine of the fifteen tests, the six correct answers are preceded by another identical string, 3-a-1-2, which includes three of four incorrect answers. And on all fifteen tests, the six correct answers are followed by the same incorrect answer, a 4. Why on earth would a cheating teacher go to the trouble of erasing a student’s test sheet and then fill in the wrong answer?

Perhaps she is merely being strategic. In case she is caught and hauled into the principal’s office, she could point to the wrong answers as proof that she didn’t cheat. Or perhaps—and this is a less charitable but just as likely answer—she doesn’t know the right answers herself. (With standardized tests, the teacher is typically not given an answer key.) If this is the case, then we have a pretty good clue as to why her students are in need of inflated grades in the first place: they have a bad teacher.

Another indication of teacher cheating in classroom A is the class’s overall performance. As sixth graders who were taking the test in the eighth month of the academic year, these students needed to achieve an average score of 6.8 to be considered up to national standards. (Fifth graders taking the test in the eighth month of the year needed to score 5.8, seventh graders 7.8, and so on.) The students in classroom A averaged 5.8 on their sixth-grade tests, which is a full grade level below where they should be. So plainly these are poor students. A year earlier, however, these students did even worse, averaging just 4.1 on their fifth-grade tests. Instead of improving by one full point between fifth and sixth grade, as would be expected, they improved by 1.7 points, nearly two grades’ worth. But this miraculous improvement was short-lived. When these sixth-grade students reached seventh grade, they averaged 5.5—more than two grade levels below standard and even worse than they did in sixth grade. Consider the erratic year-to-year scores of three particular students from classroom A:

The three-year scores from classroom B, meanwhile, are also poor but at least indicate an honest effort: 4.2, 5.1, and 6.0. So an entire roomful of children in classroom A suddenly got very smart one year and very dim the next, or more likely, their sixth-grade teacher worked some magic with her pencil.

There are two noteworthy points to be made about the children in classroom A, tangential to the cheating itself. The first is that they are obviously in poor academic shape, which makes them the very children whom high-stakes testing is promoted as helping the most. The second point is that these students (and their parents) would be in for a terrible shock once they reached the seventh grade. All they knew was that they had been successfully promoted due to their test scores. (No child left behind, indeed.) They weren’t the ones who artificially jacked up their scores; they probably expected to do great in the seventh grade—and then they failed miserably. This may be the cruelest twist yet in high-stakes testing. A cheating teacher may tell herself that she is helping her students, but the fact is that she would appear far more concerned with helping herself.

An analysis of the entire Chicago data reveals evidence of teacher cheating in more than two hundred classrooms per year, roughly 5 percent of the total. This is a conservative estimate, since the algorithm was able to identify only the most egregious form of cheating—in which teachers systematically changed students’ answers—and not the many subtler ways a teacher might cheat. In a recent study among North Carolina schoolteachers, some 35 percent of the respondents said they had witnessed their colleagues cheating in some fashion, whether by giving students extra time, suggesting answers, or manually changing students’ answers.

What are the characteristics of a cheating teacher? The Chicago data shows that male and female teachers are equally prone to cheating. A cheating teacher tends to be younger and less qualified than average. She is also more likely to cheat after her incentives change. Because the Chicago data ran from 1993 to 2000, it bracketed the introduction of high-stakes testing in 1996. Sure enough, there was a pronounced spike in cheating in 1996. Nor was the cheating random. It was the teachers in the lowest-scoring classrooms who were most likely to cheat. It should also be noted that the $25,000 bonus for California teachers was eventually revoked, in part because of suspicions that too much of the money was going to cheaters.

Not every result of the Chicago cheating analysis was so dour. In addition to detecting cheaters, the algorithm could also identify the best teachers in the school system. A good teacher’s impact was nearly as distinctive as a cheater’s. Instead of getting random answers correct, her students would show real improvement on the easier types of questions they had previously missed, an indication of actual learning. And a good teacher’s students carried over all their gains into the next grade.

Most academic analyses of this sort tend to languish, unread, on a dusty library shelf. But in early 2002, the new CEO of the Chicago Public Schools, Arne Duncan, contacted the study’s authors. He didn’t want to protest or hush up their findings. Rather, he wanted to make sure that the teachers identified by the algorithm as cheaters were truly cheating—and then do something about it.

Duncan was an unlikely candidate to hold such a powerful job. He was only thirty-six when appointed, a onetime academic all-American at Harvard who later played pro basketball in Australia. He had spent just three years with the CPS—and never in a job important enough to have his own secretary—before becoming its CEO. It didn’t hurt that Duncan had grown up in Chicago. His father taught psychology at the University of Chicago; his mother ran an afterschool program for forty years, without pay, in a poor neighborhood. When Duncan was a boy, his afterschool playmates were the underprivileged kids his mother cared for. So when he took over the public schools, his allegiance lay more with schoolchildren and their families than with teachers and their union.

The best way to get rid of cheating teachers, Duncan had decided, was to readminister the standardized exam. He only had the resources to retest 120 classrooms, however, so he asked the creators of the cheating algorithm to help choose which classrooms to test.

How could those 120 retests be used most effectively? It might have seemed sensible to retest only the classrooms that likely had a cheating teacher. But even if their retest scores were lower, the teachers could argue that the students did worse merely because they were told that the scores wouldn’t count in their official record—which, in fact, all retested students would be told. To make the retest results convincing, some non-cheaters were needed as a control group. The best control group? The classrooms shown by the algorithm to have the best teachers, in which big gains were thought to have been legitimately attained. If those classrooms held their gains while the classrooms with a suspected cheater lost ground, the cheating teachers could hardly argue that their students did worse only because the scores wouldn’t count.

So a blend was settled upon. More than half of the 120 retested classrooms were those suspected of having a cheating teacher. The remainder were divided between the supposedly excellent teachers (high scores but no suspicious answer patterns) and, as a further control, classrooms with mediocre scores and no suspicious answers.

The retest was given a few weeks after the original exam. The children were not told the reason for the retest. Neither were the teachers. But they may have gotten the idea when it was announced that CPS officials, not the teachers, would administer the test. The teachers were asked to stay in the classroom with their students, but they would not be allowed to even touch the answer sheets.

The results were as compelling as the cheating algorithm had predicted. In the classrooms chosen as controls, where no cheating was suspected, scores stayed about the same or even rose. In contrast, the students with the teachers identified as cheaters scored far worse, by an average of more than a full grade level.

As a result, the Chicago Public School system began to fire its cheating teachers. The evidence was only strong enough to get rid of a dozen of them, but the many other cheaters had been duly warned. The final outcome of the Chicago study is further testament to the power of incentives: the following year, cheating by teachers fell more than 30 percent.

You might think that the sophistication of teachers who cheat would increase along with the level of schooling. But an exam given at the University of Georgia in the fall of 2001 disputes that idea. The course was called Coaching Principles and Strategies of Basketball, and the final grade was based on a single exam that had twenty questions. Among the questions:

How many halves are in a college basketball game?


a. 1 b. 2 c. 3 d. 4

How many points does a 3-pt. field goal account for in a basketball game?


a. 1 b. 2 c. 3 d. 4

What is the name of the exam which all high school seniors in the state of Georgia must pass?


a. Eye Exam

b. How Do the Grits Taste Exam

c. Bug Control Exam

d. Georgia Exit Exam

In your opinion, who is the best Division I assistant coach in the country?


a. Ron Jirsa

b. John Pelphrey

c. Jim Harrick Jr.

d. Steve Wojciechowski

If you are stumped by the final question, it might help to know that Coaching Principles was taught by Jim Harrick Jr., an assistant coach with the university’s basketball team. It might also help to know that his father, Jim Harrick Sr., was the head basketball coach. Not surprisingly, Coaching Principles was a favorite course among players on the Harricks’ team. Every student in the class received an A. Not long afterward, both Harricks were relieved of their coaching duties.

If it strikes you as disgraceful that Chicago schoolteachers and University of Georgia professors will cheat—a teacher, after all, is meant to instill values along with the facts—then the thought of cheating among sumo wrestlers may also be deeply disturbing. In Japan, sumo is not only the national sport but also a repository of the country’s religious, military, and historical emotion. With its purification rituals and its imperial roots, sumo is sacrosanct in a way that American sports will never be. Indeed, sumo is said to be less about competition than about honor itself.

It is true that sports and cheating go hand in hand. That’s because cheating is more common in the face of a bright-line incentive (the line between winning and losing, for instance) than with a murky incentive. Olympic sprinters and weightlifters, cyclists in the Tour de France, football linemen and baseball sluggers: they have all been shown to swallow whatever pill or powder may give them an edge. It is not only the participants who cheat. Cagey baseball managers try to steal an opponent’s signs. In the 2002 Winter Olympic figure-skating competition, a French judge and a Russian judge were caught trying to swap votes to make sure their skaters medaled. (The man accused of orchestrating the vote swap, a reputed Russian mob boss named Alimzhan Tokhtakhounov, was also suspected of rigging beauty pageants in Moscow.)

An athlete who gets caught cheating is generally condemned, but most fans at least appreciate his motive: he wanted so badly to win that he bent the rules. (As the baseball player Mark Grace once said, “If you’re not cheating, you’re not trying.”) An athlete who cheats to lose, meanwhile, is consigned to a deep circle of sporting hell. The 1919 Chicago White Sox, who conspired with gamblers to throw the World Series (and are therefore known forever as the Black Sox), retain a stench of iniquity among even casual baseball fans. The City College of New York’s championship basketball team, once beloved for its smart and scrappy play, was instantly reviled when it was discovered in 1951 that several players had taken mob money to shave points—intentionally missing baskets to help gamblers beat the point spread. Remember Terry Malloy, the tormented former boxer played by Marlon Brando in On the Waterfront? As Malloy saw it, all his troubles stemmed from the one fight in which he took a dive. Otherwise, he could have had class; he could have been a contender.

If cheating to lose is sport’s premier sin, and if sumo wrestling is the premier sport of a great nation, cheating to lose couldn’t possibly exist in sumo. Could it?

Once again, the data can tell the story. As with the Chicago school tests, the data set under consideration here is surpassingly large: the results from nearly every official match among the top rank of Japanese sumo wrestlers between January 1989 and January 2000, a total of 32,000 bouts fought by 281 different wrestlers.

The incentive scheme that rules sumo is intricate and extraordinarily powerful. Each wrestler maintains a ranking that affects every slice of his life: how much money he makes, how large an entourage he carries, how much he gets to eat, sleep, and otherwise take advantage of his success. The sixty-six highest-ranked wrestlers in Japan, comprising the makuuchi and juryo divisions, make up the sumo elite. A wrestler near the top of this elite pyramid may earn millions and is treated like royalty. Any wrestler in the top forty earns at least $170,000 a year. The seventieth-ranked wrestler in Japan, meanwhile, earns only $15,000 a year. Life isn’t very sweet outside the elite. Low-ranked wrestlers must tend to their superiors, preparing their meals, cleaning their quarters, and even soaping up their hardest-to-reach body parts. So ranking is everything.

A wrestler’s ranking is based on his performance in the elite tournaments that are held six times a year. Each wrestler has fifteen bouts per tournament, one per day over fifteen consecutive days. If he finishes the tournament with a winning record (eight victories or better), his ranking will rise. If he has a losing record, his ranking falls. If it falls far enough, he is booted from the elite rank entirely. The eighth victory in any tournament is therefore critical, the difference between promotion and demotion; it is roughly four times as valuable in the rankings as the typical victory.

So a wrestler entering the final day of a tournament on the bubble, with a 7–7 record, has far more to gain from a victory than an opponent with a record of 8–6 has to lose.

Is it possible, then, that an 8–6 wrestler might allow a 7–7 wrestler to beat him? A sumo bout is a concentrated flurry of force and speed and leverage, often lasting only a few seconds. It wouldn’t be very hard to let yourself be tossed. Let’s imagine for a moment that sumo wrestling is rigged. How might we measure the data to prove it?

The first step would be to isolate the bouts in question: those fought on a tournament’s final day between a wrestler on the bubble and a wrestler who has already secured his eighth win. (Because more than half of all wrestlers end a tournament with either seven, eight, or nine victories, hundreds of bouts fit these criteria.) A final-day match between two 7–7 wrestlers isn’t likely to be fixed, since both fighters badly need the victory. A wrestler with ten or more victories probably wouldn’t throw a match either, since he has his own strong incentive to win: the $100,000 prize for overall tournament champion and a series of $20,000 prizes for the “outstanding technique” award, “fighting spirit” award, and others.

Let’s now consider the following statistic, which represents the hundreds of matches in which a 7–7 wrestler faced an 8–6 wrestler on a tournament’s final day. The left column tallies the probability, based on all past meetings between the two wrestlers fighting that day, that the 7–7 wrestler will win. The right column shows how often the 7–7 wrestler actually did win.

So the 7–7 wrestler, based on past outcomes, was expected to win just less than half the time. This makes sense; their records in this tournament indicate that the 8–6 wrestler is slightly better. But in actuality, the wrestler on the bubble won almost eight out of ten matches against his 8–6 opponent. Wrestlers on the bubble also do astonishingly well against 9–5 opponents:

As suspicious as this looks, a high winning percentage alone isn’t enough to prove that a match is rigged. Since so much depends on a wrestler’s eighth win, he should be expected to fight harder in a crucial bout. But perhaps there are further clues in the data that prove collusion.

It’s worth thinking about the incentive a wrestler might have to throw a match. Maybe he accepts a bribe (which would obviously not be recorded in the data). Or perhaps some other arrangement is made between the two wrestlers. Keep in mind that the pool of elite sumo wrestlers is extraordinarily tight-knit. Each of the sixty-six elite wrestlers fights fifteen of the others in a tournament every two months. Furthermore, each wrestler belongs to a stable that is typically managed by a former sumo champion, so even the rival stables have close ties. (Wrestlers from the same stable do not wrestle one another.)

Now let’s look at the win-loss percentage between the 7–7 wrestlers and the 8–6 wrestlers the next time they meet, when neither one is on the bubble. In this case, there is no great pressure on the individual match. So you might expect the wrestlers who won their 7–7 matches in the previous tournament to do about as well as they had in earlier matches against these same opponents—that is, winning roughly 50 percent of the time. You certainly wouldn’t expect them to uphold their 80 percent clip.

As it turns out, the data show that the 7–7 wrestlers win only 40 percent of the rematches. Eighty percent in one match and 40 percent in the next? How do you make sense of that?

The most logical explanation is that the wrestlers made a quid pro quo agreement: you let me win today, when I really need the victory, and I’ll let you win the next time. (Such an arrangement wouldn’t preclude a cash bribe.) It’s especially interesting to note that by the two wrestlers’ second subsequent meeting, the win percentages revert to the expected level of about 50 percent, suggesting that the collusion spans only two matches.

And it isn’t only the individual wrestlers whose records are suspect. The collective records of the various sumo stables are similarly aberrational. When one stable’s wrestlers fare well on the bubble against wrestlers from a second stable, they tend to do especially poorly when the second stable’s wrestlers are on the bubble. This indicates that some match rigging may be choreographed at the highest level of the sport—much like the Olympic skating judges’ vote swapping.

No formal disciplinary action has ever been taken against a Japanese sumo wrestler for match rigging. Officials from the Japanese Sumo Association typically dismiss any such charges as fabrications by disgruntled former wrestlers. In fact, the mere utterance of the words “sumo” and “rigged” in the same sentence can cause a national furor. People tend to get defensive when the integrity of their national sport is impugned.

Still, allegations of match rigging do occasionally find their way into the Japanese media. These occasional media storms offer one more chance to measure possible corruption in sumo. Media scrutiny, after all, creates a powerful incentive: if two sumo wrestlers or their stables have been rigging matches, they might be leery to continue when a swarm of journalists and TV cameras descend upon them.

So what happens in such cases? The data show that in the sumo tournaments held immediately after allegations of match rigging, 7–7 wrestlers win only 50 percent of their final-day matches against 8–6 opponents instead of the typical 80 percent. No matter how the data are sliced, they inevitably suggest one thing: it is hard to argue that sumo wrestling isn’t rigged.

Several years ago, two former sumo wrestlers came forward with extensive allegations of match rigging—and more. Aside from the crooked matches, they said, sumo was rife with drug use and sexcapades, bribes and tax evasion, and close ties to the yakuza, the Japanese mafia. The two men began to receive threatening phone calls; one of them told friends he was afraid he would be killed by the yakuza. Still, they went forward with plans to hold a press conference at the Foreign Correspondents’ Club in Tokyo. But shortly beforehand, the two men died—hours apart, in the same hospital, of a similar respiratory ailment. The police declared there had been no foul play but did not conduct an investigation. “It seems very strange for these two people to die on the same day at the same hospital,” said Mitsuru Miyake, the editor of a sumo magazine. “But no one has seen them poisoned, so you can’t prove the skepticism.”

Whether or not their deaths were intentional, these two men had done what no other sumo insider had previously done: named names. Of the 281 wrestlers covered in the data cited above, they identified 29 crooked wrestlers and 11 who were said to be incorruptible.

What happens when the whistle-blowers’ corroborating evidence is factored into the analysis of the match data? In matches between two supposedly corrupt wrestlers, the wrestler who was on the bubble won about 80 percent of the time. In bubble matches against a supposedly clean opponent, meanwhile, the bubble wrestler was no more likely to win than his record would predict. Furthermore, when a supposedly corrupt wrestler faced an opponent whom the whistle-blowers did not name as either corrupt or clean, the results were nearly as skewed as when two corrupt wrestlers met—suggesting that most wrestlers who weren’t specifically named were also corrupt.

So if sumo wrestlers, schoolteachers, and day-care parents all cheat, are we to assume that mankind is innately and universally corrupt? And if so, how corrupt?

The answer may lie in…bagels. Consider this story about a man named Paul Feldman.

Once upon a time, Feldman dreamed big dreams. With early training in agricultural economics, he wanted to tackle world hunger. Instead, he took a job in Washington, analyzing weapons expenditures for the U.S. Navy. This was in 1962. For the next twenty-odd years, he did further analytic work in Washington. He held senior-level jobs and earned good money, but he wasn’t always recognized for his best work. At the office Christmas party, colleagues would introduce him to their wives not as “the head of the public research group” (which he was) but as “the guy who brings in the bagels.”

The bagels had begun as a casual gesture: a boss treating his employees whenever they won a research contract. Then he made it a habit. Every Friday, he would bring in some bagels, a serrated knife, and cream cheese. When employees from neighboring floors heard about the bagels, they wanted some too. Eventually he was bringing in fifteen dozen bagels a week. In order to recoup his costs, he set out a cash basket and a sign with the suggested price. His collection rate was about 95 percent; he attributed the underpayment to oversight, not fraud.

In 1984, when his research institute fell under new management, Feldman took a look at his future and grimaced. He decided to quit his job and sell bagels. His economist friends thought he had lost his mind, but his wife supported him. The last of their three children was finishing college, and they had retired their mortgage.

Driving around the office parks that encircle Washington, he solicited customers with a simple pitch: early in the morning, he would deliver some bagels and a cash basket to a company’s snack room; he would return before lunch to pick up the money and the leftovers. It was an honor-system commerce scheme, and it worked. Within a few years, Feldman was delivering 8,400 bagels a week to 140 companies and earning as much as he had ever made as a research analyst. He had thrown off the shackles of cubicle life and made himself happy.

He had also—quite without meaning to—designed a beautiful economic experiment. From the beginning, Feldman kept rigorous data on his bagel business. So by measuring the money collected against the bagels taken, he found it possible to tell, down to the penny, just how honest his customers were. Did they steal from him? If so, what were the characteristics of a company that stole versus a company that did not? Under what circumstances did people tend to steal more, or less?

As it happens, Feldman’s accidental study provides a window onto a form of cheating that has long stymied academics: white-collar crime. (Yes, shorting the bagel man is white-collar crime, writ however small.) It might seem ludicrous to address as large and intractable a problem as white-collar crime through the life of a bagel man. But often a small and simple question can help chisel away at the biggest problems.

Despite all the attention paid to rogue companies like Enron, academics know very little about the practicalities of white-collar crime. The reason? There are no good data. A key fact of white-collar crime is that we hear about only the very slim fraction of people who are caught cheating. Most embezzlers lead quiet and theoretically happy lives; employees who steal company property are rarely detected.

With street crime, meanwhile, that is not the case. A mugging or a burglary or a murder is usually tallied whether or not the criminal is caught. A street crime has a victim, who typically reports the crime to the police, who generate data, which in turn generate thousands of academic papers by criminologists, sociologists, and economists. But white-collar crime presents no obvious victim. From whom, exactly, did the masters of Enron steal? And how can you measure something if you don’t know to whom it happened, or with what frequency, or in what magnitude?

Paul Feldman’s bagel business was different. It did present a victim. The victim was Paul Feldman.

When he started his business, he expected a 95 percent payment rate, based on the experience at his own office. But just as crime tends to be low on a street where a police car is parked, the 95 percent rate was artificially high: Feldman’s presence had deterred theft. Not only that, but those bagel eaters knew the provider and had feelings (presumably good ones) about him. A broad swath of psychological and economic research has shown that people will pay different amounts for the same item depending on who is providing it. The economist Richard Thaler, in his 1985 “Beer on the Beach” study, showed that a thirsty sunbather would pay $2.65 for a beer delivered from a resort hotel but only $1.50 for the same beer if it came from a shabby grocery store.

In the real world, Feldman learned to settle for less than 95 percent. He came to consider a company “honest” if its payment rate was above 90 percent. He considered a rate between 80 and 90 percent “annoying but tolerable.” If a company habitually paid below 80 percent, Feldman might post a hectoring note, like this one:


The cost of bagels has gone up dramatically since the beginning of the year. Unfortunately, the number of bagels that disappear without being paid for has also gone up. Don’t let that continue. I don’t imagine that you would teach your children to cheat, so why do it yourselves?

In the beginning, Feldman left behind an open basket for the cash, but too often the money vanished. Then he tried a coffee can with a money slot in its plastic lid, which also proved too tempting. In the end, he resorted to making small plywood boxes with a slot cut into the top. The wooden box has worked well. Each year he drops off about seven thousand boxes and loses, on average, just one to theft. This is an intriguing statistic: the same people who routinely steal more than 10 percent of his bagels almost never stoop to stealing his money box—a tribute to the nuanced social calculus of theft. From Feldman’s perspective, an office worker who eats a bagel without paying is committing a crime; the office worker probably doesn’t think so. This distinction probably has less to do with the admittedly small amount of money involved (Feldman’s bagels cost one dollar each, cream cheese included) than with the context of the “crime.” The same office worker who fails to pay for his bagel might also help himself to a long slurp of soda while filling a glass in a self-serve restaurant, but he is very unlikely to leave the restaurant without paying.

So what do the bagel data have to say? In recent years, there have been two noteworthy trends in the overall payment rate. The first was a long, slow decline that began in 1992. By the summer of 2001, the overall rate had slipped to about 87 percent. But immediately after September 11 of that year, the rate spiked a full 2 percent and hasn’t slipped much since. (If a 2 percent gain in payment doesn’t sound like much, think of it this way: the nonpayment rate fell from 13 to 11 percent, which amounts to a 15 percent decline in theft.) Because many of Feldman’s customers are affiliated with national security, there may have been a patriotic element to this 9/11 Effect. Or it may have represented a more general surge in empathy.

The data also show that smaller offices are more honest than big ones. An office with a few dozen employees generally outpays by 3 to 5 percent an office with a few hundred employees. This may seem counterintuitive. In a bigger office, a bigger crowd is bound to convene around the bagel table, providing more witnesses to make sure you drop your money in the box. But in the big-office/small-office comparison, bagel crime seems to mirror street crime. There is far less street crime per capita in rural areas than in cities, in large part because a rural criminal is more likely to be known (and therefore caught). Also, a smaller community tends to exert greater social incentives against crime, the main one being shame.

The bagel data also reflect how much personal mood seems to affect honesty. Weather, for instance, is a major factor. Unseasonably pleasant weather inspires people to pay at a higher rate. Unseasonably cold weather, meanwhile, makes people cheat prolifically; so do heavy rain and wind. Worst are the holidays. The week of Christmas produces a 2 percent drop in payment rates—again, a 15 percent increase in theft, an effect on the same magnitude, in reverse, as that of 9/11. Thanksgiving is nearly as bad; the week of Valentine’s Day is also lousy, as is the week straddling April 15. There are, however, several good holidays: the weeks that include the Fourth of July, Labor Day, and Columbus Day. The difference in the two sets of holidays? The low-cheating holidays represent little more than an extra day off from work. The high-cheating holidays are fraught with miscellaneous anxieties and the high expectations of loved ones.

Feldman has also reached some of his own conclusions about honesty, based more on his experience than the data. He has come to believe that morale is a big factor—that an office is more honest when the employees like their boss and their work. He also believes that employees further up the corporate ladder cheat more than those down below. He got this idea after delivering for years to one company spread out over three floors—an executive floor on top and two lower floors with sales, service, and administrative employees. (Feldman wondered if perhaps the executives cheated out of an overdeveloped sense of entitlement. What he didn’t consider is that perhaps cheating was how they got to be executives.)

If morality represents the way we would like the world to work and economics represents how it actually does work, then the story of Feldman’s bagel business lies at the very intersection of morality and economics. Yes, a lot of people steal from him, but the vast majority, even though no one is watching over them, do not. This outcome may surprise some people—including Feldman’s economist friends, who counseled him twenty years ago that his honor-system scheme would never work. But it would not have surprised Adam Smith. In fact, the theme of Smith’s first book, The Theory of Moral Sentiments, was the innate honesty of mankind. “How selfish soever man may be supposed,” Smith wrote, “there are evidently some principles in his nature, which interest him in the fortune of others, and render their happiness necessary to him, though he derives nothing from it, except the pleasure of seeing it.”

There is a tale, “The Ring of Gyges,” that Feldman sometimes tells his economist friends. It comes from Plato’s Republic. A student named Glaucon offered the story in response to a lesson by Socrates—who, like Adam Smith, argued that people are generally good even without enforcement. Glaucon, like Feldman’s economist friends, disagreed. He told of a shepherd named Gyges who stumbled upon a secret cavern with a corpse inside that wore a ring. When Gyges put on the ring, he found that it made him invisible. With no one able to monitor his behavior, Gyges proceeded to do woeful things—seduce the queen, murder the king, and so on. Glaucon’s story posed a moral question: could any man resist the temptation of evil if he knew his acts could not be witnessed? Glaucon seemed to think the answer was no. But Paul Feldman sides with Socrates and Adam Smith—for he knows that the answer, at least 87 percent of the time, is yes.

2


How Is the Ku Klux Klan Like a Group of Real-Estate Agents?


As institutions go, the Ku Klux Klan has had a markedly up-and-down history. It was founded in the immediate aftermath of the Civil War by six former Confederate soldiers in Pulaski, Tennessee. The six young men, four of whom were budding lawyers, saw themselves as merely a circle of like-minded friends. Thus the name they chose, “kuklux,” a slight mangling of kuklos, the Greek word for “circle.” In the beginning, their activities were said to be harmless midnight pranks—for instance, riding horses through the countryside while draped in white sheets and pillowcase hoods. But soon the Klan evolved into a multistate terrorist organization designed to frighten and kill emancipated slaves. Among its regional leaders were five former Confederate generals; its staunchest supporters were the plantation owners for whom Reconstruction posed an economic and political nightmare. In 1872, President Ulysses S. Grant spelled out for the House of Representatives the true aims of the Ku Klux Klan: “By force and terror, to prevent all political action not in accord with the views of the members, to deprive colored citizens of the right to bear arms and of the right of a free ballot, to suppress the schools in which colored children were taught, and to reduce the colored people to a condition closely allied to that of slavery.”

The early Klan did its work through pamphleteering, lynching, shooting, burning, castrating, pistol-whipping, and a thousand forms of intimidation. They targeted former slaves and any whites who supported the blacks’ rights to vote, acquire land, or gain an education. But within barely a decade, the Klan had been extinguished, largely by legal and military interventions out of Washington, D.C.

If the Klan itself was defeated, however, its aims had largely been achieved through the establishment of Jim Crow laws. Congress, which during Reconstruction had been quick to enact measures of legal, social, and economic freedom for blacks, just as quickly began to roll them back. The federal government agreed to withdraw its occupation troops from the South, allowing the restoration of white rule. In Plessy v. Ferguson, the U.S. Supreme Court gave the go-ahead to full-scale racial segregation.

The Ku Klux Klan lay largely dormant until 1915, when D. W. Griffith’s film The Birth of a Nation (originally titled The Clansman) helped spark its rebirth. Griffith presented the Klan as crusaders for white civilization itself, and as one of the noblest forces in American history. The film quoted a line from A History of the American People, written by a renowned historian: “At last there had sprung into existence a great Ku Klux Klan, a veritable empire of the South, to protect the Southern country.” The historian in question was U.S. president Woodrow Wilson, onetime scholar and president of Princeton University.

By the 1920s, a revived Klan claimed eight million members. This time around, the Klan was not confined to the South but ranged throughout the country; this time, it concerned itself not only with blacks but also with Catholics, Jews, communists, unionists, immigrants, agitators, and other disrupters of the status quo. In 1933, with Hitler ascendant in Germany, Will Rogers was the first to draw a line between the new Klan and the new threat in Europe: “Papers all state Hitler is trying to copy Mussolini,” he wrote. “Looks to me like it’s the Ku Klux that he is copying.”

The onset of World War II and a number of internal scandals once again laid the Klan low. Public sentiment turned against the Klan as the unity of a country at war trumped its message of separatism.

But within a few years, there were already signs of a massive revival. As wartime anxiety gave way to postwar uncertainty, Klan membership flourished. Barely two months after V-J Day, the Klan in Atlanta burned a 300-foot cross on the face of Stone Mountain, site of a storied rock carving of Robert E. Lee. The extravagant cross burning, one Klansman later said, was intended “just to let the niggers know the war is over and that the Klan is back on the market.”

Atlanta had by now become Klan headquarters. The Klan was thought to hold great sway with key Georgia politicians, and its Georgia chapters were said to include many policemen and sheriff’s deputies. Yes, the Klan was a secret society, reveling in passwords and cloak-and-dagger ploys, but its real power lay in the very public fear that it fostered, exemplified by the open secret that the Ku Klux Klan and the law-enforcement establishment were brothers in arms.

Atlanta—the Imperial City of the KKK’s Invisible Empire, in Klan jargon—was also home to Stetson Kennedy, a thirty-year-old man with the bloodlines of a Klansman but a temperament that ran opposite. He came from a good southern family which claimed ancestors including two signers of the Declaration of Independence, an officer in the Confederate Army, and John B. Stetson, founder of the famed hat company and the man for whom Stetson University was named.

Stetson Kennedy grew up in a fourteen-room house in Jacksonville, Florida, the youngest of five children. His uncle Brady was a Klansman. But Kennedy would go on to become a self-described “dissident at large,” writing numberless articles and several books that railed against bigotry. He first worked as a folklorist, traveling around Florida to collect old native tales and songs. Years later, when he served as a rare white correspondent for the Pittsburgh Courier, the country’s largest black newspaper, he wrote under the pseudonym Daddy Mention—after a black folk hero who, as myth told it, could outrun the blast of a sheriff’s shotgun.

What drove Kennedy was a hatred of small-mindedness, ignorance, obstructionism, and intimidation—which, in his view, were displayed by no organization more proudly than the Ku Klux Klan. Kennedy saw the Klan as the terrorist arm of the white establishment itself. This struck him as an intractable problem, for a variety of reasons. The Klan was in cahoots with political, business, and law-enforcement leaders. The public was frightened and felt powerless to act against the Klan. And the few anti-hate groups that existed at the time had little leverage or even information about the Klan. “Almost all of the things written on the subject were editorials, not exposés,” Kennedy would later explain. “The writers were against the Klan, all right, but they had precious few inside facts about it.”

So Kennedy set out to gather those facts. He would spend years interviewing Klan leaders and sympathizers, sometimes taking advantage of his own background and lineage to pretend that he was on their side of the issues. He also attended public Klan events and, as he would later write, he even set about to infiltrate the Klan in Atlanta.

The Klan Unmasked, Kennedy’s memoir of his exploits “inside” the Klan, is in fact more of a novelization than a straight nonfiction account. Kennedy, a folklorist at heart, apparently wanted to put across the most dramatic story possible, and therefore included not only his own anti-Klan activities but those of another man, code-named John Brown. Brown was a union worker and a former Klan official who had changed his ways and offered to infiltrate the Klan. It was John Brown who apparently performed many of the most dramatic and dangerous episodes portrayed in The Klan Unmasked—physically attending Klan meetings and other functions in Atlanta—but since Stetson Kennedy was the man who later wrote the book, he rendered Brown’s actions as his own.

Regardless, there was a great deal of information to be gleaned from this Brown/Kennedy collaboration. Brown divulged what he was learning at the weekly Klan meetings: the identities of the Klan’s local and regional leaders; their upcoming plans; the Klan’s current rituals, passwords, and language. It was Klan custom, for instance, to append a Kl to many words. (Thus would two Klansmen hold a Klonversation in the local Klavern.) The secret Klan handshake was a left-handed, limp-wristed fish wiggle. When a traveling Klansman wanted to locate brethren in a strange town, he would ask for a “Mr. Ayak”—“Ayak” being code for “Are You a Klansman?” He would hope to hear this response: “Yes, and I also know a Mr. Akai”—code for “A Klansman Am I.”

Before long, John Brown was invited to join the Klavaliers, the Klan’s secret police and “flog squad.” For an infiltrator, this posed a particularly sticky problem: What would happen if he were called upon to inflict violence?

But as it happened, a central tenet of life in the Klan—and of terrorism in general—is that most of the threatened violence never goes beyond the threat stage.

Consider lynching, the Klan’s hallmark sign of violence. Here, compiled by the Tuskegee Institute, are the decade-by-decade statistics on the lynching of blacks in the United States:

Bear in mind that these figures represent not only lynchings attributed to the Ku Klux Klan but the total number of reported lynchings. The statistics reveal at least three noteworthy facts. The first is the obvious decrease in lynchings over time. The second is the absence of a correlation between lynchings and Klan membership: there were actually more lynchings of blacks between 1900 and 1909, when the Klan was dormant, than during the 1920s, when the Klan had millions of members—which suggests that the Ku Klux Klan carried out far fewer lynchings than is generally thought.

Third, relative to the size of the black population, lynchings were exceedingly rare. To be sure, one lynching is one too many. But by the turn of the century, lynchings were hardly the everyday occurrence that they are often considered in the public recollection. Compare the 281 victims of lynchings in the 1920s to the number of black infants who were dying at that time as a result of malnutrition, pneumonia, diarrhea, and the like. As of 1920, about 13 out of every 100 black children died in infancy, or roughly 20,000 children each year—compared to 28 people who were lynched in a year. As late as 1940, about 10,000 black infants died each year.

What larger truths do these lynching figures suggest? What does it mean that lynchings were relatively rare and that they fell precipitously over time, even in the face of a boom in Klan membership?

The most compelling explanation is that all those early lynchings worked. White racists—whether or not they belonged to the Ku Klux Klan—had through their actions and their rhetoric developed a strong incentive scheme that was terribly clear and terribly frightening. If a black person violated the accepted code of behavior, whether by talking back to a bus driver or daring to try to vote, he knew he might well be punished, perhaps by death.

So it may be that by the mid-1940s, when Stetson Kennedy was trying to bust up the Klan, it didn’t really need to use as much violence. Many blacks, having long been told to behave like second-class citizens—or else—simply obliged. One or two lynchings went a long way toward inducing docility among even a large group of people, for people respond strongly to strong incentives. And there are few incentives more powerful than the fear of random violence—which, in essence, is why terrorism is so effective.

But if the Ku Klux Klan of the 1940s wasn’t uniformly violent, what was it? The Klan that Stetson Kennedy wrote about was in fact a sorry fraternity of men, most of them poorly educated and with poor prospects, who needed a place to vent—and an excuse for occasionally staying out all night. That their fraternity engaged in quasi-religious chanting and oath taking and hosanna hailing, all of it top secret, made it that much more appealing.

Kennedy also found the Klan to be a slick money-making operation, at least for those near the top of the organization. Klan leaders had any number of revenue sources: thousands of dues-paying rank-and-file members; business owners who hired the Klan to scare off the unions or who paid the Klan protection money; Klan rallies that generated huge cash donations; even the occasional gunrunning or moonshine operation. Then there were rackets like the Klan’s Death Benefit Association, which sold insurance policies to Klan members and accepted only cash or personal checks made out to the Grand Dragon himself.

And, even though the Klan may not have been as deadly as generally thought, it was plenty violent and, perhaps worse, had ever greater designs on political influence. Kennedy was therefore eager to damage the Klan in any way he could. When he heard about Klan plans for a union-busting rally, he fed the information to a union friend. He passed along Klan information to the assistant attorney general of Georgia, an established Klan buster. After researching the Klan’s corporate charter, Kennedy wrote to the governor of Georgia suggesting the grounds upon which the charter should be revoked: the Klan had been designated a non-profit, non-political organization, but Kennedy had proof that it was clearly devoted to both profits and politics.

The problem was that most of Kennedy’s efforts weren’t producing the desired effect. The Klan was so entrenched and broad-based that Kennedy felt as if he were tossing pebbles at a giant. And even if he could somehow damage the Klan in Atlanta, the hundreds of other chapters around the country would go untouched.

Kennedy was supremely frustrated, and out of this frustration was born a new strategy. He had noticed one day a group of young boys playing some kind of spy game in which they exchanged silly secret passwords. It reminded him of the Klan. Wouldn’t it be nice, he thought, to get the Klan’s passwords and the rest of its secrets into the hands of kids all across the country—and their parents too? What better way to defang a secret society than to make public its most secret information? Instead of futilely attacking the Klan from the outside, what if he could somehow unleash all the secret inside information that John Brown was gathering from the Klan’s weekly meetings? Between Brown’s inside dope and everything that Kennedy had learned via his own investigations, he probably knew more Klan secrets than the average Klansman.

Kennedy turned to the most powerful mass medium of his day: radio. He began feeding Klan reports to the journalist Drew Pearson, whose Washington Merry-Go-Round program was heard by millions of adults every day, and to the producers of the Adventures of Superman show, which reached millions of children each night. He told them about Mr. Ayak and Mr. Akai, and he passed along overheated passages from the Klan’s bible, which was called the Kloran. (Kennedy never did learn why a white Christian supremacist group would give its bible essentially the same name as the most holy book of Islam.) He explained the role of Klan officers in any local Klavern: the Klaliff (vice president), Klokard (lecturer), Kludd (chaplain), Kligrapp (secretary), Klabee (treasurer), Kladd (conductor), Klarogo (inner guard), Klexter (outer guard), the Klokann (a five-man investigative committee), and the Klavaliers (whose leader was called Chief Ass Tearer). He spelled out the Klan hierarchy as it proceeded from the local to the national level: an Exalted Cyclops and his twelve Terrors; a Great Titan and his twelve Furies; a Grand Dragon and his nine Hydras; and the Imperial Wizard and his fifteen Genii. And Kennedy passed along all the information and gossip that John Brown gleaned by infiltrating the main Klan chapter, Nathan Bedford Forrest Klavern No. 1, Atlanta, Realm of Georgia.

During the war, the Adventures of Superman program had portrayed its hero fighting Hitler and Mussolini and Hirohito. But now he was in need of fresh villains. The Klan was a perfect target, and Superman turned his powers against them. Drew Pearson, an avowed Klan hater, now began giving regular Klan updates on his radio show, and then gave further updates, based on John Brown’s inside reports, to show how the original updates were infuriating Klan officials. Pearson’s work created an echo chamber that seemed to be driving Grand Dragon Samuel Green crazy. Here is Pearson’s radio report from November 17, 1948:


Speaking at Klavern No. 1, Atlanta, Ga., the week after elections, the Grand Dragon wrung his hands and once again cautioned Klansmen to be careful about leaks.

“I have to talk frankly at these meetings,” he said, “but I might as well call Drew Pearson before I come to the meeting and give him the information, for [the] next day he gives it out to everybody from coast to coast. The A.P. and U.P. are both calling me about it next morning while I am eating breakfast.”…

The Grand Dragon spoke about plans for a big cross-burning to be held in Macon, Ga., on Dec. 10. It would be the biggest in Klan history, he said, and he expected 10,000 Klansmen to be there—in their robes….

He added that the Klavalier Klub—the Klan’s whipping and flogging department—was now on the job and had plenty of friends on the Atlanta police force.

As the Pearson and Superman radio shows played on, and as Stetson Kennedy continued to relay the Klan secrets obtained by John Brown to other broadcast and print outlets, a funny thing happened: attendance at Klan meetings began to fall, as did applications for new membership. Of all the ideas that Kennedy had thought up to fight bigotry, this campaign was easily the cleverest. He turned the Klan’s secrecy against itself by making its private information public; he converted heretofore precious knowledge into ammunition for mockery.

Americans who might have been philosophically inclined to oppose the Klan had now been given enough specific information to oppose them more actively, and public sentiment began to shift. Americans who might have been philosophically inclined to embrace the Klan had now been given all sorts of caution against doing so. Although the Klan would never quite die, especially down south—David Duke, a smooth-talking Klan leader from Louisiana, mounted substantive bids for the U.S. Senate and other offices—it was certainly handicapped, at least in the short term, by Kennedy’s brazen dissemination of inside information. While it is impossible to tease out the exact impact that his work had on the Klan, many people have given him a great deal of credit for damaging an institution that was in grave need of being damaged.

This did not come about because Stetson Kennedy was courageous or resolute or unflappable, even though he was all of these. It happened because he understood the raw power of information. The Ku Klux Klan—much like politicians or real-estate agents or stockbrokers—was a group whose power was derived in large part from the fact that it hoarded information. Once that information falls into the wrong hands (or, depending on your point of view, the right hands), much of the group’s advantage disappears.

In the late 1990s, the price of term life insurance fell dramatically. This posed something of a mystery, for the decline had no obvious cause. Other types of insurance, including health and automobile and homeowners’ coverage, were certainly not falling in price. Nor had there been any radical changes among insurance companies, insurance brokers, or the people who buy term life insurance. So what happened?

The Internet happened. In the spring of 1996, Quotesmith.com became the first of several websites that enabled a customer to compare, within seconds, the price of term life insurance sold by dozens of different companies. For such websites, term life insurance was a perfect product. Unlike other forms of insurance—especially whole life insurance, which is a far more complicated financial instrument—term life policies are fairly homogeneous: any given thirty-year, guaranteed policy for $1 million is essentially identical to the next. So what really matters is the price. Shopping around for the cheapest policy, a process that had been convoluted and time-consuming, was suddenly made simple. With customers able to instantaneously find the cheapest policy, the more expensive companies had no choice but to lower their prices. Suddenly customers were paying $1 billion less a year for term life insurance.

It is worth noting that these websites only listed prices; they didn’t even sell the policies. So it wasn’t really insurance they were peddling. Like Stetson Kennedy, they were dealing in information. (Had the Internet been around when Kennedy was attacking the Klan, he probably would have been blogging his brains out.) To be sure, there are differences between exposing the Ku Klux Klan and exposing insurance companies’ high premiums. The Klan trafficked in secret information whose secrecy engendered fear, while insurance prices were less a secret than a set of facts dispensed in a way that made comparisons difficult. But in both instances, the dissemination of the information diluted its power. As Supreme Court Justice Louis D. Brandeis once wrote, “Sunlight is said to be the best of disinfectants.”

Information is a beacon, a cudgel, an olive branch, a deterrent—all depending on who wields it and how. Information is so powerful that the assumption of information, even if the information does not actually exist, can have a sobering effect. Consider the case of a one-day-old car.

The day that a car is driven off the lot is the worst day in its life, for it instantly loses as much as a quarter of its value. This might seem absurd, but we know it to be true. A new car that was bought for $20,000 cannot be resold for more than perhaps $15,000. Why? Because the only person who might logically want to resell a brand-new car is someone who found the car to be a lemon. So even if the car isn’t a lemon, a potential buyer assumes that it is. He assumes that the seller has some information about the car that he, the buyer, does not have—and the seller is punished for this assumed information.

And if the car is a lemon? The seller would do well to wait a year to sell it. By then, the suspicion of lemonness will have faded; by then, some people will be selling their perfectly good year-old cars, and the lemon can blend in with them, likely selling for more than it is truly worth.

It is common for one party to a transaction to have better information than another party. In the parlance of economists, such a case is known as an information asymmetry. We accept as a verity of capitalism that someone (usually an expert) knows more than someone else (usually a consumer). But information asymmetries everywhere have in fact been gravely wounded by the Internet.

Information is the currency of the Internet. As a medium, the Internet is brilliantly efficient at shifting information from the hands of those who have it into the hands of those who do not. Often, as in the case of term life insurance prices, the information existed but in a woefully scattered way. (In such instances, the Internet acts like a gigantic horseshoe magnet waved over an endless sea of haystacks, plucking the needle out of each one.) The Internet has accomplished what even the most fervent consumer advocates usually cannot: it has vastly shrunk the gap between the experts and the public.

The Internet has proven particularly fruitful for situations in which a face-to-face encounter with an expert might actually exacerbate the problem of asymmetrical information—situations in which an expert uses his informational advantage to make us feel stupid or rushed or cheap or ignoble. Consider a scenario in which your loved one has just died and now the funeral director (who knows that you know next to nothing about his business and are under emotional duress to boot) steers you to the $8,000 mahogany casket. Or consider the automobile dealership: a salesman does his best to obscure the car’s base price under a mountain of add-ons and incentives. Later, however, in the cool-headed calm of your home, you can use the Internet to find out exactly how much the dealer paid the manufacturer for that car. Or you might just log on to www.TributeDirect.com and buy that mahogany casket yourself for only $3,595, delivered overnight. Unless you decide to spend $2,300 for “The Last Hole” (a casket with golf scenes) or “Memories of the Hunt” (featuring big-racked bucks and other prey) or one of the much cheaper models that the funeral director somehow failed even to mention.

The Internet, powerful as it is, has hardly slain the beast that is information asymmetry. Consider the so-called corporate scandals of the early 2000s. The crimes committed by Enron included hidden partnerships, disguised debt, and the manipulation of energy markets. Henry Blodget of Merrill Lynch and Jack Grubman of Salomon Smith Barney wrote glowing research reports of companies they knew to be junk. Sam Waksal dumped his ImClone stock when he got early word of a damaging report from the Food and Drug Administration; his friend Martha Stewart also dumped her shares, then lied about the reason. WorldCom and Global Crossing fabricated billions of dollars in revenues to pump up their stock prices. One group of mutual fund companies let preferred customers trade at preferred prices, and another group was charged with hiding management fees.

Though extraordinarily diverse, these crimes all have a common trait: they were sins of information. Most of them involved an expert, or a gang of experts, promoting false information or hiding true information; in each case the experts were trying to keep the information asymmetry as asymmetrical as possible.

The practitioners of such acts, especially in the realm of high finance, inevitably offer this defense: “Everybody else was doing it.” Which may be largely true. One characteristic of information crimes is that very few of them are detected. Unlike street crimes, they do not leave behind a corpse or a broken window. Unlike a bagel criminal—that is, someone who eats one of Paul Feldman’s bagels but doesn’t pay—an information criminal typically doesn’t have someone like Feldman tallying every nickel. For an information crime to reach the surface, something drastic must happen. When it does, the results tend to be pretty revealing. The perpetrators, after all, weren’t thinking about their private actions being made public. Consider the “Enron tapes,” the secretly recorded conversations of Enron employees that surfaced after the company imploded. During a phone conversation on August 5, 2000, two traders chatted about how a wildfire in California would allow Enron to jack up its electricity prices. “The magical word of the day,” one trader said, “is ‘Burn, Baby, Burn.’” A few months later, a pair of Enron traders named Kevin and Bob talked about how California officials wanted to make Enron refund the profits of its price gouging.


KEVIN: They’re fucking taking all the money back from you guys? All the money you guys stole from those poor grandmas in California?

BOB: Yeah, Grandma Millie, man.

KEVIN: Yeah, now she wants her fucking money back for all the power you jammed right up her ass for fucking $250 a megawatt hour.

If you were to assume that many experts use their information to your detriment, you’d be right. Experts depend on the fact that you don’t have the information they do. Or that you are so befuddled by the complexity of their operation that you wouldn’t know what to do with the information if you had it. Or that you are so in awe of their expertise that you wouldn’t dare challenge them. If your doctor suggests that you have angioplasty—even though some current research suggests that angioplasty often does little to prevent heart attacks—you aren’t likely to think that the doctor is using his informational advantage to make a few thousand dollars for himself or his buddy. But as David Hillis, an interventional cardiologist at the University of Texas Southwestern Medical Center in Dallas, explained to the New York Times, a doctor may have the same economic incentives as a car salesman or a funeral director or a mutual fund manager: “If you’re an invasive cardiologist and Joe Smith, the local internist, is sending you patients, and if you tell them they don’t need the procedure, pretty soon Joe Smith doesn’t send patients anymore.”

Armed with information, experts can exert a gigantic, if unspoken, leverage: fear. Fear that your children will find you dead on the bathroom floor of a heart attack if you do not have angioplasty surgery. Fear that a cheap casket will expose your grandmother to a terrible underground fate. Fear that a $25,000 car will crumple like a toy in an accident, whereas a $50,000 car will wrap your loved ones in a cocoon of impregnable steel. The fear created by commercial experts may not quite rival the fear created by terrorists like the Ku Klux Klan, but the principle is the same.

Consider a transaction that wouldn’t seem, on the surface, to create much fear: selling your house. What’s so scary about that? Aside from the fact that selling a house is typically the largest financial transaction in your life, and that you probably have scant experience in real estate, and that you may have an enormous emotional attachment to your house, there are at least two pressing fears: that you will sell the house for far less than it is worth and that you will not be able to sell it at all.

In the first case, you fear setting the price too low; in the second, you fear setting it too high. It is the job of your real-estate agent, of course, to find the golden mean. She is the one with all the information: the inventory of similar houses, the recent sales trends, the tremors of the mortgage market, perhaps even a lead on an interested buyer. You feel fortunate to have such a knowledgeable expert as an ally in this most confounding enterprise.

Too bad she sees things differently. A real-estate agent may see you not so much as an ally but as a mark. Think back to the study cited at the beginning of this book, which measured the difference between the sale prices of homes that belonged to real-estate agents themselves and the houses they sold for their clients. The study found that an agent keeps her own house on the market an average ten extra days, waiting for a better offer, and sells it for over 3 percent more than your house—or $10,000 on the sale of a $300,000 house. That’s $10,000 going into her pocket that does not go into yours, a nifty profit produced by the abuse of information and a keen understanding of incentives. The problem is that the agent only stands to personally gain an additional $150 by selling your house for $10,000 more, which isn’t much reward for a lot of extra work. So her job is to convince you that a $300,000 offer is in fact a very good offer, even a generous one, and that only a fool would refuse it.

This can be tricky. The agent does not want to come right out and call you a fool. So she merely implies it—perhaps by telling you about the much bigger, nicer, newer house down the block that has sat unsold for six months. Here is the agent’s main weapon: the conversion of information into fear. Consider this true story, related by John Donohue, a law professor who in 2001 was teaching at Stanford University: “I was just about to buy a house on the Stanford campus,” he recalls, “and the seller’s agent kept telling me what a good deal I was getting because the market was about to zoom. As soon as I signed the purchase contract, he asked me if I would need an agent to sell my previous Stanford house. I told him that I would probably try to sell without an agent, and he replied, ‘John, that might work under normal conditions, but with the market tanking now, you really need the help of a broker.’”

Within five minutes, a zooming market had tanked. Such are the marvels that can be conjured by an agent in search of the next deal.

Consider now another true story of a real-estate agent’s information abuse. The tale involves K., a close friend of one of this book’s authors. K. wanted to buy a house that was listed at $469,000. He was prepared to offer $450,000 but he first called the seller’s agent and asked her to name the lowest price that she thought the homeowner might accept. The agent promptly scolded K. “You ought to be ashamed of yourself,” she said. “That is clearly a violation of real-estate ethics.”

K. apologized. The conversation turned to other, more mundane issues. After ten minutes, as the conversation was ending, the agent told K., “Let me say one last thing. My client is willing to sell this house for a lot less than you might think.”

Based on this conversation, K. then offered $425,000 for the house instead of the $450,000 he had planned to offer. In the end, the seller accepted $430,000. Thanks to his own agent’s intervention, the seller lost at least $20,000. The agent, meanwhile, only lost $300—a small price to pay to ensure that she would quickly and easily lock up the sale, which netted her a commission of $6,450.

So a big part of a real-estate agent’s job, it would seem, is to persuade the homeowner to sell for less than he would like while at the same time letting potential buyers know that a house can be bought for less than its listing price. To be sure, there are more subtle means of doing so than coming right out and telling the buyer to bid low. The study of real-estate agents cited above also includes data that reveals how agents convey information through the for-sale ads they write. A phrase like “well maintained,” for instance, is as full of meaning to an agent as “Mr. Ayak” was to a Klansman; it means that a house is old but not quite falling down. A savvy buyer will know this (or find out for himself once he sees the house), but to the sixty-five-year-old retiree who is selling his house, “well maintained” might sound like a compliment, which is just what the agent intends.

An analysis of the language used in real-estate ads shows that certain words are powerfully correlated with the final sale price of a house. This doesn’t necessarily mean that labeling a house “well maintained” causes it to sell for less than an equivalent house. It does, however, indicate that when a real-estate agent labels a house “well maintained,” she may be subtly encouraging a buyer to bid low.

Listed below are ten terms commonly used in real-estate ads. Five of them have a strong positive correlation to the ultimate sale price, and five have a strong negative correlation. Guess which are which.

Ten Common Real-Estate Ad Terms


Fantastic

Granite

Spacious

State-of-the-Art

!

Corian

Charming

Maple

Great Neighborhood

Gourmet

A “fantastic” house is surely fantastic enough to warrant a high price, isn’t it? What about a “charming” and “spacious” house in a “great neighborhood!”? No, no, no, and no. Here’s the breakdown:

Five Terms Correlated to a Higher Sale Price


Granite

State-of-the-Art

Corian

Maple

Gourmet

Five Terms Correlated to a Lower Sale Price


Fantastic

Spacious

!

Charming

Great Neighborhood

Three of the five terms correlated with a higher sale price are physical descriptions of the house itself: granite, Corian, and maple. As information goes, such terms are specific and straightforward—and therefore pretty useful. If you like granite, you might like the house; but even if you don’t, “granite” certainly doesn’t connote a fixer-upper. Nor does “gourmet” or “state-of-the-art,” both of which seem to tell a buyer that a house is, on some level, truly fantastic.

“Fantastic,” meanwhile, is a dangerously ambiguous adjective, as is “charming.” Both these words seem to be real-estate agent code for a house that doesn’t have many specific attributes worth describing. “Spacious” homes, meanwhile, are often decrepit or impractical. “Great neighborhood” signals a buyer that, well, this house isn’t very nice but others nearby may be. And an exclamation point in a real-estate ad is bad news for sure, a bid to paper over real shortcomings with false enthusiasm.

If you study the words in ads for a real-estate agent’s own home, meanwhile, you see that she indeed emphasizes descriptive terms (especially “new,” “granite,” “maple,” and “move-in condition”) and avoids empty adjectives (including “wonderful,” “immaculate,” and the telltale “!”). Then she patiently waits for the best buyer to come along. She might tell this buyer about a house nearby that just sold for $25,000 above the asking price, or another house that is currently the subject of a bidding war. She is careful to exercise every advantage of the information asymmetry she enjoys.

Does this make her a bad person? That’s hard to say, at least hard for us to say. The point here is not that real-estate agents are bad people, but that they simply are people—and people inevitably respond to incentives. The incentives of the real-estate business, as currently configured, plainly encourage some agents to act against the best interests of their customers.

But like the funeral director and the car salesman and the life-insurance company, the real-estate agent has also seen her advantage eroded by the Internet. After all, anyone selling a home can now get online and gather her own information about sales trends and housing inventory and mortgage rates. The information has been set loose. And recent sales data show the results. Real-estate agents still get a higher price for their own homes than comparable homes owned by their clients, but since the proliferation of real-estate websites, the gap between the two prices has shrunk by a third.

It would be naïve to suppose that people abuse information only when they are acting as experts or as agents of commerce. After all, agents and experts are people too—which suggests that we are likely to abuse information in our personal lives as well, whether by withholding true information or editing the information we choose to put forth. A real-estate agent may wink and nod when she lists a “well-maintained” house, but we each have our equivalent hedges.

Think about how you describe yourself during a job interview versus how you might describe yourself on a first date. (For even more fun, compare that first-date conversation to a conversation with the same person during your tenth year of marriage.) Or think about how you might present yourself if you were going on national television for the first time. What sort of image would you want to project? Perhaps you want to seem clever or kind or good-looking; presumably you don’t want to come off as cruel or bigoted. During the heyday of the Ku Klux Klan, its members took pride in publicly disparaging anybody who wasn’t a conservative white Christian. But public bigotry has since been vastly curtailed. Even subtle displays of bigotry, if they become public, are now costly. Trent Lott, the majority leader of the U.S. Senate, learned this in 2002 after making a toast at a one hundredth birthday party for Strom Thurmond, his fellow senator and fellow southerner. Lott made a reference in his toast to Thurmond’s 1948 campaign for president, which was built on a platform of segregation; Mississippi—Lott’s home state—was one of just four states that Thurmond carried. “We’re proud of it,” Lott told the partygoers. “And if the rest of the country had followed our lead, we wouldn’t have had all these problems over all these years either.” The implication that Lott was a fan of segregation raised enough of a fury that he was forced to quit his Senate leadership post.

Even if you are a private citizen, you surely wouldn’t want to seem bigoted while appearing in public. Might there be a way to test for discrimination in a public setting?

Unlikely as it may seem, the television game show The Weakest Link provides a unique laboratory to study discrimination. An import from the United Kingdom, The Weakest Link for a short time became wildly popular in the United States. The game includes eight contestants (or, in a later daytime version, six) who each answer trivia questions and compete for a single cash jackpot. But the player who answers the most questions correctly isn’t necessarily the player who advances. After each round, every contestant votes to eliminate one other contestant. A player’s trivia-answering ability is presumably the only worthwhile factor to consider; race, gender, and age wouldn’t seem to matter. But do they? By measuring a contestant’s actual votes against the votes that would truly best serve his self-interest, it’s possible to tell if discrimination is at play.

The voting strategy changes as the game progresses. In the first several rounds, it makes sense to eliminate bad players since the jackpot grows only when correct answers are given. In later rounds, the strategic incentives are flipped. The value of building the jackpot is now outweighed by each contestant’s desire to win the jackpot. It’s easier to do that if you eliminate the other good players. So, roughly speaking, the typical contestant will vote to eliminate the worse players in the early rounds and the better players in the later rounds.

The key to measuring the Weakest Link voting data is to tease out a contestant’s playing ability from his race, gender, and age. If a young black man answers a lot of questions correctly but is voted off early, discrimination would seem to be a factor. Meanwhile, if an elderly white woman doesn’t answer a single question correctly and is still not voted off, some sort of discriminatory favoritism would seem to be at play.

Again, keep in mind that all of this is happening on camera. A contestant knows that his friends, family, and co-workers—along with a few million strangers—are watching. So who, if anyone, is discriminated against on The Weakest Link?

Not, as it turns out, blacks. An analysis of more than 160 episodes reveals that black contestants, in both the early and late rounds of the game, are eliminated at a rate commensurate with their trivia-answering abilities. The same is true for female contestants. In a way, neither of these findings is so surprising. Two of the most potent social campaigns of the past half-century were the civil rights movement and the feminist movement, which demonized discrimination against blacks and women, respectively.

So perhaps, you say hopefully, discrimination was practically eradicated during the twentieth century, like polio.

Or more likely, it has become so unfashionable to discriminate against certain groups that all but the most insensitive people take pains to at least appear fair-minded, at least in public. This hardly means that discrimination itself has ended—only that people are embarrassed to show it. How might you determine whether the lack of discrimination against blacks and women represents a true absence or just a charade? The answer can be found by looking at other groups that society doesn’t protect as well. Indeed, the Weakest Link voting data do indicate two kinds of contestants who are consistently discriminated against: the elderly and Latinos.

Among economists, there are two leading theories of discrimination. Interestingly, elderly Weakest Link contestants seem to suffer from one type, while Latinos suffer the other. The first type is called taste-based discrimination, which means that one person discriminates simply because he prefers to not interact with a particular type of other person. In the second type, known as information-based discrimination, one person believes that another type of person has poor skills, and acts accordingly.

On The Weakest Link, Latinos suffer information-based discrimination. Other contestants seem to view the Latinos as poor players, even when they are not. This perception translates into Latinos’ being eliminated in the early rounds even if they are doing well and not being eliminated in the later rounds, when other contestants want to keep the Latinos around to weaken the field.

Elderly players, meanwhile, are victims of taste-based discrimination: in the early rounds and late rounds, they are eliminated far out of proportion to their skills. It seems as if the other contestants—this is a show on which the average age is thirty-four—simply don’t want the older players around.

It’s quite possible that a typical Weakest Link contestant isn’t even cognizant of his discrimination toward Latinos and the elderly (or, in the case of blacks and women, his lack of discrimination). He is bound to be nervous, after all, and excited, playing a fast-moving game under the glare of television lights. Which naturally suggests another question: how might that same person express his preferences—and reveal information about himself—in the privacy of his home?

In a given year, some forty million Americans swap intimate truths about themselves with complete strangers. It all happens on Internet dating sites. Some of them, like Match.com, eHarmony.com, and Yahoo! Personals, appeal to a broad audience. Others cater to more specific tastes: ChristianSingles.com, JDate.com, LatinMatcher .com, BlackSinglesConnection.com, CountryWesternSingles.com, USMilitarySingles.com, OverweightDate.com, and Gay.com. Dating websites are the most successful subscription-based business on the Internet.

Each site operates a bit differently, but the gist is this: You compose a personal ad about yourself that typically includes a photo, vital statistics, your income range, level of education, likes and dislikes, and so on. If the ad catches someone’s fancy, that someone will e-mail you and perhaps arrange a date. On many sites, you also specify your dating aims: “long-term relationship,” “a casual lover,” or “just looking.”

So there are two massive layers of data to be mined here: the information that people include in their ads and the level of response gleaned by any particular ad. Each layer of the data can be asked its own question. In the case of the ads, how forthright (and honest) are people when it comes to sharing their personal information? And in the case of the responses, what kind of information in personal ads is considered the most (and least) desirable?

Two economists and a psychologist recently banded together to address these questions. Günter J. Hitsch, Ali Hortaçsu, and Dan Ariely analyzed the data from one of the mainstream dating sites, focusing on more than 20,000 active users, half in Boston and half in San Diego. Fifty-six percent of the users were men, and the median age range for all users was twenty-one to thirty-five. Although they represented an adequate racial mix to reach some conclusions about race, they were predominantly white.

They were also a lot richer, taller, skinnier, and better-looking than average. That, at least, is what they wrote about themselves. More than 4 percent of the online daters claimed to earn more than $200,000 a year, whereas fewer than 1 percent of typical Internet users actually earn that much, suggesting that three of the four big earners were exaggerating. Male and female users typically reported that they are about an inch taller than the national average. As for weight, the men were in line with the national average, but the women typically said they weighed about twenty pounds less than the national average.

Most impressively, fully 72 percent of the women claimed “above average” looks, including 24 percent claiming “very good looks.” The online men too were gorgeous: 68 percent called themselves “above average,” including 19 percent with “very good looks.” This leaves only about 30 percent of the users with “average” looks, including a paltry 1 percent with “less than average” looks—which suggests that the typical online dater is either a fabulist, a narcissist, or simply resistant to the meaning of “average.” (Or perhaps they are all just pragmatists: as any real-estate agent knows, the typical house isn’t “charming” or “fantastic,” but unless you say it is, no one will even bother to take a look.) Twenty-eight percent of the women on the site said they were blond, a number far beyond the national average, which indicates a lot of dyeing, or lying, or both.

Some users, meanwhile, were bracingly honest. Seven percent of the men conceded that they were married, with a significant minority of these men reporting that they were “happily married.” But the fact that they were honest doesn’t mean they were rash. Of the 243 “happily married” men in the sample, only 12 chose to post a picture of themselves. The reward of gaining a mistress was evidently outweighed by the risk of having your wife discover your personal ad. (“And what were you doing on that website?” the husband might bluster, undoubtedly to little avail.)

Of the many ways to fail on a dating website, not posting a photo of yourself is perhaps the most certain. (Not that the photo necessarily is a photo of yourself; it may well be some better-looking stranger, but such deception would obviously backfire in time.) A man who does not include his photo gets only 60 percent of the volume of e-mail response of a man who does; a woman who doesn’t include her photo gets only 24 percent as much. A low-income, poorly educated, unhappily employed, not very attractive, slightly overweight, and balding man who posts his photo stands a better chance of gleaning some e-mails than a man who says he makes $200,000 and is deadly handsome but doesn’t post a photo. There are plenty of reasons someone might not post a photo—he’s technically challenged or is ashamed of being spotted by friends or is just plain unattractive—but as in the case of a brand-new car with a For Sale sign, prospective customers will assume he’s got something seriously wrong under the hood.

Getting a date is hard enough as it is. Fifty-six percent of the men who post ads don’t receive even one e-mail; 21 percent of the women don’t get a single response. The traits that do draw a big response, meanwhile, will not be a big surprise to anyone with even a passing knowledge of the sexes. In fact, the preferences expressed by online daters fit snugly with the most common stereotypes about men and women.

For instance, men who say they want a long-term relationship do much better than men looking for an occasional lover. But women looking for an occasional lover do great. For men, a woman’s looks are of paramount importance. For women, a man’s income is terribly important. The richer a man is, the more e-mails he receives. But a woman’s income appeal is a bell-shaped curve: men do not want to date low-earning women, but once a woman starts earning too much, they seem to be scared off. Women are eager to date military men, policemen, and firemen (possibly the result of a 9/11 Effect, like the higher payments to Paul Feldman’s bagel business), along with lawyers and doctors; they generally avoid men with manufacturing jobs. For men, being short is a big disadvantage (which is probably why so many lie about it), but weight doesn’t much matter. For women, being overweight is deadly (which is probably why they lie). For a man, having red hair or curly hair is a downer, as is “bald with a fringe”—but a shaved head is okay. For a woman, salt-and-pepper hair is bad, while blond hair is, not surprisingly, very good.

In addition to all the information about income, education, and looks, men and women on the dating site listed their race. They were also asked to indicate a preference regarding the race of their potential dates. The two preferences were “the same as mine” or “it doesn’t matter.” Like the Weakest Link contestants, the website users were now publicly declaring how they felt about people who didn’t look like them. They would reveal their actual preferences later, in confidential e-mails to the people they wanted to date.

Roughly half of the white women on the site and 80 percent of the white men declared that race didn’t matter to them. But the response data tell a different story. The white men who said that race didn’t matter sent 90 percent of their e-mail queries to white women. The white women who said race didn’t matter sent about 97 percent of their e-mail queries to white men. This means that an Asian man who is good-looking, rich, and well educated will receive fewer than 25 percent as many e-mails from white women as a white man with the same qualifications would receive; similarly, black and Latino men receive about half as many e-mails from white women as they would if they were white.

Is it possible that race really didn’t matter for these white women and men and that they simply never happened to browse a nonwhite date that interested them? Or, more likely, did they say that race didn’t matter because they wanted to come across—especially to potential mates of their own race—as open-minded?

The gulf between the information we publicly proclaim and the information we know to be true is often vast. (Or, put a more familiar way: we say one thing and do another.) This can be seen in personal relationships, in commercial transactions, and of course in politics.

By now we are fully accustomed to the false public proclamations of politicians themselves. But voters lie too. Consider an election between a black candidate and a white candidate. Might white voters lie to pollsters, claiming they will vote for the black candidate in order to appear more color-blind than they actually are? Apparently so. In New York City’s 1989 mayoral race between David Dinkins (a black candidate) and Rudolph Giuliani (who is white), Dinkins won by only a few points. Although Dinkins became the city’s first black mayor, his slender margin of victory came as a surprise, for preelection polls showed Dinkins winning by nearly 15 points. When the white supremacist David Duke ran for the U.S. Senate in 1990, he garnered nearly 20 percent more of the vote than pre-election polls had projected, an indication that thousands of Louisiana voters did not want to admit their preference for a candidate with racist views.

Duke, though he never won the high political office he often sought, proved himself a master of information abuse. As Grand Wizard of the Knights of the Ku Klux Klan, he was able to compile a mailing list of thousands of rank-and-file Klansmen and other supporters who would eventually become his political base. Not content to use the list only for himself, he sold it for $150,000 to the governor of Louisiana. Years later, Duke would once again use the list himself, letting his supporters know that he’d fallen on hard times and needed their donations. In this way Duke was able to raise hundreds of thousands of dollars for his continuing work in the field of white supremacy. He had explained to his supporters in a letter that he was so broke that the bank was trying to repossess his house.

In truth, Duke had already sold his house for a solid profit. (It isn’t known whether he used a real-estate agent.) And most of the money he raised from his supporters was being used not to promote any white supremacist cause but rather to satisfy Duke’s gambling habit. It was a sweet little scam he was running—until he was arrested and sent to federal prison in Big Spring, Texas.

3


Why Do Drug Dealers Still Live with Their Moms?


The two previous chapters were built around a pair of admittedly freakish questions: What do schoolteachers and sumo wrestlers have in common? and How is the Ku Klux Klan like a group of real-estate agents? But if you ask enough questions, strange as they seem at the time, you may eventually learn something worthwhile.

The first trick of asking questions is to determine if your question is a good one. Just because a question has never been asked does not make it good. Smart people have been asking questions for quite a few centuries now, so many of the questions that haven’t been asked are bound to yield uninteresting answers.

But if you can question something that people really care about and find an answer that may surprise them—that is, if you can overturn the conventional wisdom—then you may have some luck.

It was John Kenneth Galbraith, the hyperliterate economic sage, who coined the phrase “conventional wisdom.” He did not consider it a compliment. “We associate truth with convenience,” he wrote, “with what most closely accords with self-interest and personal well-being or promises best to avoid awkward effort or unwelcome dislocation of life. We also find highly acceptable what contributes most to self-esteem.” Economic and social behaviors, Galbraith continued, “are complex, and to comprehend their character is mentally tiring. Therefore we adhere, as though to a raft, to those ideas which represent our understanding.”

So the conventional wisdom in Galbraith’s view must be simple, convenient, comfortable, and comforting—though not necessarily true. It would be silly to argue that the conventional wisdom is never true. But noticing where the conventional wisdom may be false—noticing, perhaps, the contrails of sloppy or self-interested thinking—is a nice place to start asking questions.

Consider the recent history of homelessness in the United States. In the early 1980s, an advocate for the homeless named Mitch Snyder took to saying that there were about 3 million homeless Americans. The public duly sat up and took notice. More than 1 of every 100 people were homeless? That sure seemed high, but…well, the expert said it. A heretofore quiescent problem was suddenly catapulted into the national consciousness. Snyder even testified before Congress about the magnitude of the problem. He also reportedly told a college audience that 45 homeless people die each second—which would mean a whopping 1.4 billion dead homeless every year. (The U.S. population at the time was about 225 million.) Assuming that Snyder misspoke or was misquoted and meant to say that one homeless person died every forty-five seconds, that’s still 701,000 dead homeless people every year—roughly one-third of all deaths in the United States. Hmm. Ultimately, when Snyder was pressed on his figure of 3 million homeless, he admitted that it was a fabrication. Journalists had been hounding him for a specific number, he said, and he hadn’t wanted them to walk away empty-handed.

It may be sad but not surprising to learn that experts like Snyder can be self-interested to the point of deceit. But they cannot deceive on their own. Journalists need experts as badly as experts need journalists. Every day there are newspaper pages and television newscasts to be filled, and an expert who can deliver a jarring piece of wisdom is always welcome. Working together, journalists and experts are the architects of much conventional wisdom.

Advertising too is a brilliant tool for creating conventional wisdom. Listerine, for instance, was invented in the nineteenth century as a powerful surgical antiseptic. It was later sold, in distilled form, as a floor cleaner and a cure for gonorrhea. But it wasn’t a runaway success until the 1920s, when it was pitched as a solution for “chronic halitosis”—a then obscure medical term for bad breath. Listerine’s new ads featured forlorn young women and men, eager for marriage but turned off by their mate’s rotten breath. “Can I be happy with him in spite of that?” one maiden asked herself. Until that time, bad breath was not conventionally considered such a catastrophe. But Listerine changed that. As the advertising scholar James B. Twitchell writes, “Listerine did not make mouthwash as much as it made halitosis.” In just seven years, the company’s revenues rose from $115,000 to more than $8 million.

However created, the conventional wisdom can be hard to budge. The economist Paul Krugman, a New York Times columnist and devout critic of George W. Bush, bemoaned this fact as the President’s reelection campaign got under way in early 2004: “The approved story line about Mr. Bush is that he’s a bluff, honest, plainspoken guy, and anecdotes that fit that story get reported. But if the conventional wisdom were instead that he’s a phony, a silver-spoon baby who pretends to be a cowboy, journalists would have plenty of material to work with.”

In the months leading up to the U.S. invasion of Iraq in 2003, dueling experts floated diametrically opposite forecasts about Iraq’s weapons of mass destruction. But more often, as with Mitch Snyder’s homeless “statistics,” one side wins the war of conventional wisdom. Women’s rights advocates, for instance, have hyped the incidence of sexual assault, claiming that one in three American women will in her lifetime be a victim of rape or attempted rape. (The actual figure is more like one in eight—but the advocates know it would take a callous person to publicly dispute their claims.) Advocates working for the cures of various tragic diseases regularly do the same. Why not? A little creative lying can draw attention, indignation, and—perhaps most important—the money and political capital to address the actual problem.

Of course an expert, whether a women’s health advocate or a political advisor or an advertising executive, tends to have different incentives than the rest of us. And an expert’s incentives may shift 180 degrees, depending on the situation.

Consider the police. A recent audit discovered that the police in Atlanta were radically underreporting crime since the early 1990s. The practice apparently began when Atlanta was working to land the 1996 Olympics. The city needed to shed its violent image, and fast. So each year thousands of crime reports were either downgraded from violent to nonviolent or simply thrown away. (Despite these continuing efforts—there were more than 22,000 missing police reports in 2002 alone—Atlanta regularly ranks among the most violent American cities.)

Police in other cities, meanwhile, were spinning a different story during the 1990s. The sudden, violent appearance of crack cocaine had police departments across the country scrapping for resources. They made it known that it wasn’t a fair fight: the drug dealers were armed with state-of-the-art weapons and a bottomless supply of cash. This emphasis on illicit cash proved to be a winning effort, for nothing infuriated the law-abiding populace more than the image of the millionaire crack dealer. The media eagerly glommed on to this story, portraying crack dealing as one of the most profitable jobs in America.

But if you were to have spent a little time around the housing projects where crack was so often sold, you might have noticed something strange: not only did most of the crack dealers still live in the projects, but most of them still lived at home with their moms. And then you may have scratched your head and said, “Why is that?”

The answer lies in finding the right data, and the secret to finding the right data usually means finding the right person—which is more easily said than done. Drug dealers are rarely trained in economics, and economists rarely hang out with crack dealers. So the answer to this question begins with finding someone who did live among the drug dealers and managed to walk away with the secrets of their trade.

Sudhir Venkatesh—his boyhood friends called him Sid, but he has since reverted to Sudhir—was born in India, raised in the suburbs of upstate New York and southern California, and graduated from the University of California at San Diego with a degree in mathematics. In 1989 he began to pursue his PhD in sociology at the University of Chicago. He was interested in understanding how young people form their identities; to that end, he had just spent three months following the Grateful Dead around the country. What he was not interested in was the grueling fieldwork that typifies sociology.

But his graduate advisor, the eminent poverty scholar William Julius Wilson, promptly sent Venkatesh into the field. His assignment: to visit Chicago’s poorest black neighborhoods with a clipboard and a seventy-question, multiple-choice survey. This was the first question on the survey:

How do you feel about being black and poor?


a. Very bad

b. Bad

c. Neither bad nor good

d. Somewhat good

e. Very good

One day Venkatesh walked twenty blocks from the university to a housing project on the shore of Lake Michigan to administer his survey. The project comprised three sixteen-story buildings made of yellow-gray brick. Venkatesh soon discovered that the names and addresses he had been given were badly outdated. These buildings were condemned, practically abandoned. Some families lived on the lower floors, pirating water and electricity, but the elevators didn’t work. Neither did the lights in the stairwell. It was late afternoon in early winter, nearly dark outside.

Venkatesh, who is a thoughtful, handsome, and well-built but not aberrationally brave person, had made his way up to the sixth floor, trying to find someone willing to take his survey. Suddenly, on the stairwell landing, he startled a group of teenagers shooting dice. They turned out to be a gang of junior-level crack dealers who operated out of the building, and they were not happy to see him.

“I’m a student at the University of Chicago,” Venkatesh sputtered, sticking to his survey script, “and I am administering—”

“Fuck you, nigger, what are you doing in our stairwell?”

There was an ongoing gang war in Chicago. Things had been violent lately, with shootings nearly every day. This gang, a branch of the Black Gangster Disciple Nation, was plainly on edge. They didn’t know what to make of Venkatesh. He didn’t seem to be a member of a rival gang. But maybe he was some kind of spy? He certainly wasn’t a cop. He wasn’t black, wasn’t white. He wasn’t exactly threatening—he was armed only with his clipboard—but he didn’t seem quite harmless either. Thanks to his three months trailing the Grateful Dead, he still looked, as he would later put it, “like a genuine freak, with hair down to my ass.”

The gang members started arguing over what should be done with Venkatesh. Let him go? But if he did tell the rival gang about this stairwell hangout, they’d be susceptible to a surprise attack. One jittery kid kept wagging something back and forth in his hands—in the dimming light, Venkatesh eventually realized it was a gun—and muttering, “Let me have him, let me have him.” Venkatesh was very, very scared.

The crowd grew bigger and louder. Then an older gang member appeared. He snatched the clipboard from Venkatesh’s hands and, when he saw that it was a written questionnaire, looked puzzled.

“I can’t read any of this shit,” he said.

“That’s because you can’t read,” said one of the teenagers, and everyone laughed at the older gangster.

He told Venkatesh to go ahead and ask him a question from the survey. Venkatesh led with the how-does-it-feel-to-be-black-and-poor question. It was met with a round of guffaws, some angrier than others. As Venkatesh would later tell his university colleagues, he realized that the multiple-choice answers A through E were insufficient. In reality, he now knew, the answers should have looked like this:


a. Very bad

b. Bad

c. Neither bad nor good

d. Somewhat good

e. Very good

f. Fuck you

Just as things were looking their bleakest for Venkatesh, another man appeared. This was J. T., the gang’s leader. J. T. wanted to know what was going on. Then he told Venkatesh to read him the survey question. He listened but then said he couldn’t answer the question because he wasn’t black.

“Well then,” Venkatesh said, “how does it feel to be African American and poor?”

“I ain’t no African American either, you idiot. I’m a nigger.” J. T. then administered a lively though not unfriendly taxonomical lesson in “nigger” versus “African American” versus “black.” When he was through, there was an awkward silence. Still nobody seemed to know what to do with Venkatesh. J. T., who was in his late twenties, had cooled down his subordinates, but he didn’t seem to want to interfere directly with their catch. Darkness fell and J. T. left. “People don’t come out of here alive,” the jittery teenager with the gun told Venkatesh. “You know that, don’t you?”

As night deepened, his captors eased up. They gave Venkatesh one of their beers, and then another and another. When he had to pee, he went where they went—on the stairwell landing one floor up. J. T. stopped by a few times during the night but didn’t have much to say. Daybreak came and then noon. Venkatesh would occasionally try to discuss his survey, but the young crack dealers just laughed and told him how stupid his questions were. Finally, nearly twenty-four hours after Venkatesh stumbled upon them, they set him free.

He went home and took a shower. He was relieved but he was also curious. It struck Venkatesh that most people, including himself, had never given much thought to the daily life of ghetto criminals. He was now eager to learn how the Black Disciples worked, from top to bottom.

After a few hours, he decided to walk back to the housing project. By now he had thought of some better questions to ask.

Having seen firsthand that the conventional method of data gathering was in this case absurd, Venkatesh vowed to scrap his questionnaire and embed himself with the gang. He tracked down J. T. and sketched out his proposal. J. T. thought Venkatesh was crazy, literally—a university student wanting to cozy up to a crack gang? But he also admired what Venkatesh was after. As it happened, J. T. was a college graduate himself, a business major. After college, he had taken a job in the Loop, working in the marketing department of a company that sold office equipment. But he felt so out of place there—like a white man working at Afro Sheen headquarters, he liked to say—that he quit. Still, he never forgot what he learned. He knew the importance of collecting data and finding new markets; he was always on the lookout for better management strategies. It was no coincidence, in other words, that J. T. was the leader of this crack gang. He was bred to be a boss.

After some wrangling, J. T. promised Venkatesh unfettered access to the gang’s operations as long as J. T. retained veto power over any information that, if published, might prove harmful.

When the yellow-gray buildings on the lakefront were demolished, shortly after Venkatesh’s first visit, the gang relocated to another housing project even deeper in Chicago’s south side. For the next six years, Venkatesh practically lived there. Under J. T.’s protection he watched the gang members up close, at work and at home. He asked endless questions. Sometimes the gangsters were annoyed by his curiosity; more often they took advantage of his willingness to listen. “It’s a war out here, man,” one dealer told him. “I mean, every day people struggling to survive, so you know, we just do what we can. We ain’t got no choice, and if that means getting killed, well, shit, it’s what niggers do around here to feed their family.”

Venkatesh would move from one family to the next, washing their dinner dishes and sleeping on the floor. He bought toys for their children; he once watched a woman use her baby’s bib to sop up the blood of a teenaged drug dealer who was shot to death in front of Venkatesh. William Julius Wilson, back at the U. of C., was having regular nightmares on Venkatesh’s behalf.

Over the years the gang endured bloody turf wars and, eventually, a federal indictment. A member named Booty, who was one rank beneath J. T., came to Venkatesh with a story. Booty was being blamed by the rest of the gang for bringing about the indictment, he told Venkatesh, and therefore suspected that he would soon be killed. (He was right.) But first Booty wanted to do a little atoning. For all the gang’s talk about how crack dealing didn’t do any harm—they even liked to brag that it kept black money in the black community—Booty was feeling guilty. He wanted to leave behind something that might somehow benefit the next generation. He handed Venkatesh a stack of well-worn spiral notebooks—blue and black, the gang’s colors. They represented a complete record of four years’ worth of the gang’s financial transactions. At J. T.’s direction, the ledgers had been rigorously compiled: sales, wages, dues, even the death benefits paid out to the families of murdered members.

At first Venkatesh didn’t even want the notebooks. What if the Feds found out he had them—perhaps he’d be indicted too? Besides, what was he supposed to do with the data? Despite his math background, he had long ago stopped thinking in numbers.

Upon completing his graduate work at the University of Chicago, Venkatesh was awarded a three-year stay at Harvard’s Society of Fellows. Its environment of sharp thinking and bonhomie—the walnut paneling, the sherry cart once owned by Oliver Wendell Holmes—delighted Venkatesh. He went so far as to become the society’s wine steward. And yet he regularly left Cambridge, returning again and again to the crack gang in Chicago. This street-level research made Venkatesh something of an anomaly. Most of the other young Fellows were dyed-in-the-tweed intellectuals who liked to pun in Greek.

One of the society’s aims was to bring together scholars from various fields who might not otherwise have occasion to meet. Venkatesh soon encountered another anomalous young Fellow, one who also failed the society stereotype. This one happened to be an economist who, instead of thinking grand macro thoughts, favored his own list of offbeat micro curiosities. At the very top of his list was crime. And so, within ten minutes of their meeting, Sudhir Venkatesh told Steven Levitt about the spiral notebooks from Chicago and they decided to collaborate on a paper. It would be the first time that such priceless financial data had fallen into an economist’s hands, affording an analysis of a heretofore uncharted criminal enterprise.

So how did the gang work? An awful lot like most American businesses, actually, though perhaps none more so than McDonald’s. In fact, if you were to hold a McDonald’s organizational chart and a Black Disciples org chart side by side, you could hardly tell the difference.

The gang that Venkatesh had fallen in with was one of about a hundred branches—franchises, really—of a larger Black Disciples organization. J. T., the college-educated leader of his franchise, reported to a central leadership of about twenty men that was called, without irony, the board of directors. (At the same time that white suburbanites were studiously mimicking black rappers’ ghetto culture, black ghetto criminals were studiously mimicking the suburbanites’ dads’ corp-think.) J. T. paid the board of directors nearly 20 percent of his revenues for the right to sell crack in a designated twelve-square-block area. The rest of the money was his to distribute as he saw fit.

Three officers reported directly to J. T.: an enforcer (who ensured the gang members’ safety), a treasurer (who watched over the gang’s liquid assets), and a runner (who transported large quantities of drugs and money to and from the supplier). Beneath the officers were the street-level salesmen known as foot soldiers. The goal of a foot soldier was to someday become an officer. J. T. had anywhere from twenty-five to seventy-five foot soldiers on his payroll at any given time, depending on the time of year (autumn was the best crack-selling season; summer and Christmastime were slow) and the size of the gang’s territory (which doubled at one point when the Black Disciples engineered a hostile takeover of a rival gang’s turf). At the very bottom of J. T.’s organization were as many as two hundred members known as the rank and file. They were not employees at all. They did, however, pay dues to the gang—some for protection from rival gangs, others for the chance to eventually earn a job as a foot soldier.

The four years recorded in the gang’s notebooks coincided with the peak years of the crack boom, and business was excellent. J. T.’s franchise quadrupled its revenues during this period. In the first year, it took in an average of $18,500 each month; by the final year, it was collecting $68,400 a month. Here’s a look at the monthly revenues in the third year:

“Drug sales” represents only the money from dealing crack cocaine. The gang did allow some rank-and-file members to sell heroin on its turf but accepted a fixed licensing fee in lieu of a share of profits. (This was off-the-books money and went straight into J. T.’s pocket; he probably skimmed from other sources as well.) The $5,100 in dues came from rank-and-file members only, since full gang members didn’t pay dues. The extortionary taxes were paid by other businesses that operated on the gang’s turf, including grocery stores, gypsy cabs, pimps, and people selling stolen goods or repairing cars on the street.

Now, here’s what it cost J. T., excluding wages, to bring in that $32,000 per month:

Mercenary fighters were nonmembers hired on short-term contracts to help the gang fight turf wars. The cost of weapons is small here because the Black Disciples had a side deal with local gunrunners, helping them navigate the neighborhood in exchange for free or steeply discounted guns. The miscellaneous expenses include legal fees, parties, bribes, and gang-sponsored “community events.” (The Black Disciples worked hard to be seen as a pillar rather than a scourge of the housing-project community.) The miscellaneous expenses also include the costs associated with a gang member’s murder. The gang not only paid for the funeral but often gave a stipend of up to three years’ wages to the victim’s family. Venkatesh had once asked why the gang was so generous in this regard. “That’s a fucking stupid question,” he was told, “’cause as long as you been with us, you still don’t understand that their families is our families. We can’t just leave ’em out. We been knowing these folks our whole lives, man, so we grieve when they grieve. You got to respect the family.” There was another reason for the death benefits: the gang feared community backlash (its enterprise was plainly a destructive one) and figured it could buy some goodwill for a few hundred dollars here and there.

The rest of the money the gang took in went to its members, starting with J. T. Here is the single line item in the gang’s budget that made J. T. the happiest:

Net monthly profit accruing to leader $8,500

At $8,500 per month, J. T.’s annual salary was about $100,000—tax-free, of course, and not including the various off-the-books money he pocketed. This was a lot more than he earned at his short-lived office job in the Loop. And J. T. was just one of roughly 100 leaders at this level within the Black Disciples network. So there were indeed some drug dealers who could afford to live large—or, in the case of the gang’s board of directors, extremely large. Each of those top 20 bosses stood to earn about $500,000 a year. (A third of them, however, were typically imprisoned at any time, a significant downside of an up position in an illicit industry.)

So the top 120 men on the Black Disciples’ pyramid were paid very well. But the pyramid they sat atop was gigantic. Using J. T.’s franchise as a yardstick—3 officers and roughly 50 foot soldiers—there were some 5,300 other men working for those 120 bosses. Then there were another 20,000 unpaid rank-and-file members, many of whom wanted nothing more than an opportunity to become a foot soldier. They were even willing to pay gang dues to have their chance.

And how well did that dream job pay? Here are the monthly totals for the wages that J. T. paid his gang members:

So J. T. paid his employees $9,500, a combined monthly salary that was only $1,000 more than his own official salary. J. T.’s hourly wage was $66. His three officers, meanwhile, each took home $700 a month, which works out to about $7 an hour. And the foot soldiers earned just $3.30 an hour, less than the minimum wage. So the answer to the original question—if drug dealers make so much money, why are they still living with their mothers?—is that, except for the top cats, they don’t make much money. They had no choice but to live with their mothers. For every big earner, there were hundreds more just scraping along. The top 120 men in the Black Disciples gang represented just 2.2 percent of the full-fledged gang membership but took home well more than half the money.

In other words, a crack gang works pretty much like the standard capitalist enterprise: you have to be near the top of the pyramid to make a big wage. Notwithstanding the leadership’s rhetoric about the family nature of the business, the gang’s wages are about as skewed as wages in corporate America. A foot soldier had plenty in common with a McDonald’s burger flipper or a Wal-Mart shelf stocker. In fact, most of J. T.’s foot soldiers also held minimum-wage jobs in the legitimate sector to supplement their skimpy illicit earnings. The leader of another crack gang once told Venkatesh that he could easily afford to pay his foot soldiers more, but it wouldn’t be prudent. “You got all these niggers below you who want your job, you dig?” he said. “So, you know, you try to take care of them, but you know, you also have to show them you the boss. You always have to get yours first, or else you really ain’t no leader. If you start taking losses, they see you as weak and shit.”

Along with the bad pay, the foot soldiers faced terrible job conditions. For starters, they had to stand on a street corner all day and do business with crackheads. (The gang members were strongly advised against using the product themselves, advice that was enforced by beatings if necessary.) Foot soldiers also risked arrest and, more worrisome, violence. Using the gang’s financial documents and the rest of Venkatesh’s research, it is possible to construct an adverse-events index of J. T.’s gang during the four years in question. The results are astonishingly bleak. If you were a member of J. T.’s gang for all four years, here is the typical fate you would have faced during that period:

Number of times arrested 5.9

Number of nonfatal wounds or injuries (not including injuries meted out by the gang itself for rules violations) 2.4

Chance of being killed 1 in 4

A 1-in-4 chance of being killed! Compare these odds with those for a timber cutter, which the Bureau of Labor Statistics calls the most dangerous job in the United States. Over four years’ time, a timber cutter would stand only a 1-in-200 chance of being killed. Or compare the crack dealer’s odds to those of a death-row inmate in Texas, which executes more prisoners than any other state. In 2003, Texas put to death twenty-four inmates—or just 5 percent of the nearly 500 inmates on its death row during that time. Which means that you stand a greater chance of dying while dealing crack in a Chicago housing project than you do while sitting on death row in Texas.

So if crack dealing is the most dangerous job in America, and if the salary was only $3.30 an hour, why on earth would anyone take such a job?

Well, for the same reason that a pretty Wisconsin farm girl moves to Hollywood. For the same reason that a high-school quarterback wakes up at 5 a.m. to lift weights. They all want to succeed in an extremely competitive field in which, if you reach the top, you are paid a fortune (to say nothing of the attendant glory and power).

To the kids growing up in a housing project on Chicago’s south side, crack dealing seemed like a glamour profession. For many of them, the job of gang boss—highly visible and highly lucrative—was easily the best job they thought they had access to. Had they grown up under different circumstances, they might have thought about becoming economists or writers. But in the neighborhood where J. T.’s gang operated, the path to a decent legitimate job was practically invisible. Fifty-six percent of the neighborhood’s children lived below the poverty line (compared to a national average of 18 percent). Seventy-eight percent came from single-parent homes. Fewer than 5 percent of the neighborhood’s adults had a college degree; barely one in three adult men worked at all. The neighborhood’s median income was about $15,000 a year, well less than half the U.S. average. During the years that Venkatesh lived with J. T.’s gang, foot soldiers often asked his help in landing what they called “a good job”: working as a janitor at the University of Chicago.

The problem with crack dealing is the same as in every other glamour profession: a lot of people are competing for a very few prizes. Earning big money in the crack gang wasn’t much more likely than the Wisconsin farm girl becoming a movie star or the high-school quarterback playing in the NFL. But criminals, like everyone else, respond to incentives. So if the prize is big enough, they will form a line down the block just hoping for a chance. On the south side of Chicago, people wanting to sell crack vastly outnumbered the available street corners.

These budding drug lords bumped up against an immutable law of labor: when there are a lot of people willing and able to do a job, that job generally doesn’t pay well. This is one of four meaningful factors that determine a wage. The others are the specialized skills a job requires, the unpleasantness of a job, and the demand for services that the job fulfills.

The delicate balance between these factors helps explain why, for instance, the typical prostitute earns more than the typical architect. It may not seem as though she should. The architect would appear to be more skilled (as the word is usually defined) and better educated (again, as usually defined). But little girls don’t grow up dreaming of becoming prostitutes, so the supply of potential prostitutes is relatively small. Their skills, while not necessarily “specialized,” are practiced in a very specialized context. The job is unpleasant and forbidding in at least two significant ways: the likelihood of violence and the lost opportunity of having a stable family life. As for demand? Let’s just say that an architect is more likely to hire a prostitute than vice versa.

In the glamour professions—movies, sports, music, fashion—there is a different dynamic at play. Even in second-tier glamour industries like publishing, advertising, and media, swarms of bright young people throw themselves at grunt jobs that pay poorly and demand unstinting devotion. An editorial assistant earning $22,000 at a Manhattan publishing house, an unpaid high-school quarterback, and a teenage crack dealer earning $3.30 an hour are all playing the same game, a game that is best viewed as a tournament.

The rules of a tournament are straightforward. You must start at the bottom to have a shot at the top. (Just as a Major League shortstop probably played Little League and just as a Grand Dragon of the Ku Klux Klan probably started out as a lowly spear-carrier, a drug lord typically began by selling drugs on a street corner.) You must be willing to work long and hard at substandard wages. In order to advance in the tournament, you must prove yourself not merely above average but spectacular. (The way to distinguish yourself differs from profession to profession, of course; while J. T. certainly monitored his foot soldiers’ sales performance, it was their force of personality that really counted—more than it would for, say, a shortstop.) And finally, once you come to the sad realization that you will never make it to the top, you will quit the tournament. (Some people hang on longer than others—witness the graying “actors” who wait tables in New York—but people generally get the message quite early.)

Most of J. T.’s foot soldiers were unwilling to stay foot soldiers for long after they realized they weren’t advancing. Especially once the shooting started. After several relatively peaceful years, J. T.’s gang got involved in a turf war with a neighboring gang. Drive-by shootings became a daily event. For a foot soldier—the gang’s man on the street—this development was particularly dangerous. The nature of the business demanded that customers be able to find him easily and quickly; if he hid from the other gang, he couldn’t sell his crack.

Until the gang war, J. T.’s foot soldiers had been willing to balance the risky, low-paying job with the reward of advancement. But as one foot soldier told Venkatesh, he now wanted to be compensated for the added risk: “Would you stand around here when all this shit is going on? No, right? So if I gonna be asked to put my life on the line, then front me the cash, man. Pay me more ’cause it ain’t worth my time to be here when they’re warring.”

J. T. hadn’t wanted this war. For one thing, he was forced to pay his foot soldiers higher wages because of the added risk. Far worse, gang warfare was bad for business. If Burger King and McDonald’s launch a price war to gain market share, they partly make up in volume what they lose in price. (Nor is anyone getting shot.) But with a gang war, sales plummet because customers are so scared of the violence that they won’t come out in the open to buy their crack. In every way, war was expensive for J. T.

So why did he start the war? As a matter of fact, he didn’t. It was his foot soldiers who started it. It turns out that a crack boss didn’t have as much control over his subordinates as he would have liked. That’s because they had different incentives.

For J. T., violence was a distraction from the business at hand; he would have preferred that his members never fired a single gunshot. For a foot soldier, however, violence served a purpose. One of the few ways that a foot soldier could distinguish himself—and advance in the tournament—was by proving his mettle for violence. A killer was respected, feared, talked about. A foot soldier’s incentive was to make a name for himself; J. T.’s incentive was, in effect, to keep the foot soldiers from doing so. “We try to tell these shorties that they belong to a serious organization,” he once told Venkatesh. “It ain’t all about killing. They see these movies and shit, they think it’s all about running around tearing shit up. But it’s not. You’ve got to learn to be part of an organization; you can’t be fighting all the time. It’s bad for business.”

In the end, J. T. prevailed. He oversaw the gang’s expansion and ushered in a new era of prosperity and relative peace. J. T. was a winner. He was paid well because so few people could do what he did. He was a tall, good-looking, smart, tough man who knew how to motivate people. He was shrewd too, never tempting arrest by carrying guns or cash. While the rest of his gang lived in poverty with their mothers, J. T. had several homes, several women, several cars. He also had his business education, of course. He constantly worked to extend this advantage. That was why he ordered the corporate-style bookkeeping that eventually found its way into Sudhir Venkatesh’s hands. No other franchise leader had ever done such a thing. J. T. once showed his ledgers to the board of directors to prove, as if proof were needed, the extent of his business acumen.

And it worked. After six years running his local gang, J. T. was promoted to the board of directors. He was now thirty-four years old. He had won the tournament. But this tournament had a catch that publishing and pro sports and even Hollywood don’t have. Selling drugs, after all, is illegal. Not long after he made the board of directors, the Black Disciples were essentially shut down by a federal indictment—the same indictment that led the gangster named Booty to turn over his notebooks to Venkatesh—and J. T. was sent to prison.

Now for another unlikely question: what did crack cocaine have in common with nylon stockings?

In 1939, when DuPont introduced nylons, countless American women felt as if a miracle had been performed in their honor. Until then, stockings were made of silk, and silk was delicate, expensive, and in ever shorter supply. By 1941, some sixty-four million pairs of nylon stockings had been sold—more stockings than there were adult women in the United States. They were easily affordable, immensely appealing, practically addictive.

DuPont had pulled off the feat that every marketer dreams of: it brought class to the masses. In this regard, the invention of nylon stockings was markedly similar to the invention of crack cocaine.

In the 1970s, if you were the sort of person who did drugs, there was no classier drug than cocaine. Beloved by rock stars and movie stars, ballplayers and even the occasional politician, cocaine was a drug of power and panache. It was clean, it was white, it was pretty. Heroin was droopy and pot was foggy but cocaine provided a beautiful high.

Alas, it was also very expensive. Nor did the high last long. This led cocaine users to try jacking up the drug’s potency. They did this primarily by freebasing—adding ammonia and ethyl ether to cocaine hydrochloride, or powdered cocaine, and burning it to free up the “base” cocaine. But this could be dangerous. As more than a few flame-scarred drug users could attest, chemistry is best left to chemists.

Meanwhile, cocaine dealers and aficionados across the country, and perhaps also in the Caribbean and South America, were working on a safer version of distilled cocaine. They found that mixing powdered cocaine in a saucepan with baking soda and water, and then cooking off the liquid, produced tiny rocks of smokeable cocaine. It came to be called crack for the crackling sound the baking soda made when it was burned. More affectionate nicknames would soon follow: Rock, Kryptonite, Kibbles ’n Bits, Scrabble, and Love. By the early 1980s, the class drug was ready for the masses. Now only two things were needed to turn crack into a phenomenon: an abundant supply of raw cocaine and a way to get the new product to a mass market.

The cocaine was easy to come by, for the invention of crack coincided with a Colombian cocaine glut. During the late 1970s, the wholesale price of cocaine in the United States fell dramatically, even as its purity was rising. One man, a Nicaraguan émigré named Oscar Danilo Blandon, was suspected of importing far more Colombian cocaine than anyone else. Blandon did so much business with the budding crack dealers of South Central Los Angeles that he came to be known as the Johnny Appleseed of Crack. Blandon would later claim that he was selling the cocaine to raise money for the CIA-sponsored Contras back home in Nicaragua. He liked to say that the CIA was in turn watching his back in the United States, allowing him to sell cocaine with impunity. This claim would spark a belief that still seethes to this day, especially among urban blacks, that the CIA itself was the chief sponsor of the American crack trade.

Verifying that claim is beyond the purview of this book. What is demonstrably true is that Oscar Danilo Blandon helped establish a link—between Colombian cocaine cartels and inner-city crack merchants—that would alter American history. By putting massive amounts of cocaine into the hands of street gangs, Blandon and others like him gave rise to a devastating crack boom. And gangs like the Black Gangster Disciple Nation were given new reason to exist.

As long as there have been cities, there have been gangs of one sort or another. In the United States, gangs have traditionally been a sort of halfway house for recent immigrants. In the 1920s, Chicago alone had more than 1,300 street gangs, catering to every ethnic, political, and criminal leaning imaginable. As a rule, gangs would prove much better at making mayhem than money. Some fancied themselves commercial enterprises, and a few—the Mafia, most notably—actually did make money (at least for the higher-ups). But most gangsters were, as the cliché assures us, two-bit gangsters.

Black street gangs in particular flourished in Chicago, with membership in the tens of thousands by the 1970s. They constituted the sort of criminals, petty and otherwise, who sucked the life out of urban areas. Part of the problem was that these criminals never seemed to get locked up. The 1960s and 1970s were, in retrospect, a great time to be a street criminal in most American cities. The likelihood of punishment was so low—this was the heyday of a liberal justice system and the criminals’ rights movement—that it simply didn’t cost very much to commit a crime.

By the 1980s, however, the courts had begun to radically reverse that trend. Criminals’ rights were curtailed and stricter sentencing guidelines put in place. More and more of Chicago’s black gangsters were getting sent to federal prisons. By happy coincidence, some of their fellow inmates were Mexican gang members with close ties to Colombian drug dealers. In the past, the black gangsters had bought their drugs from a middleman, the Mafia—which, as it happened, was then being pummeled by the federal government’s new anti-racketeering laws. But by the time crack came to Chicago, the black gangsters had made the connections to buy their cocaine directly from Colombian dealers.

Cocaine had never been a big seller in the ghetto: it was too expensive. But that was before the invention of crack. This new product was ideal for a low-income, street-level customer. Because it required such a tiny amount of pure cocaine, one hit of crack cost only a few dollars. Its powerful high reached the brain in just a few seconds—and then faded fast, sending the user back for more. From the outset, crack was bound to be a huge success.

And who better to sell it than the thousands of junior members of all those street gangs like the Black Gangster Disciple Nation? The gangs already owned the territory—real estate was, in essence, their core business—and they were suitably menacing to keep customers from even thinking about ripping them off. Suddenly the urban street gang evolved from a club for wayward teenagers into a true commercial enterprise.

The gang also presented an opportunity for longtime employment. Before crack, it was just about impossible to earn a living in a street gang. When it was time for a gangster to start supporting a family, he would have to quit. There was no such thing as a thirty-year-old gangster: he was either working a legitimate job, dead, or in prison. But with crack, there was real money to be made. Instead of moving on and making way for the younger gangsters to ascend, the veterans stayed put. This was happening just as the old-fashioned sort of lifetime jobs—factory jobs especially—were disappearing. In the past, a semi-skilled black man in Chicago could earn a decent wage working in a factory. With that option narrowing, crack dealing looked even better. How hard could it be? The stuff was so addictive that a fool could sell it.

Who cared if the crack game was a tournament that only a few of them could possibly win? Who cared if it was so dangerous—standing out there on a corner, selling it as fast and anonymously as McDonald’s sells hamburgers, not knowing any of your customers, wondering who might be coming to arrest or rob or kill you? Who cared if your product got twelve-year-olds and grandmothers and preachers so addicted that they stopped thinking about anything except their next hit? Who cared if crack killed the neighborhood?

For black Americans, the four decades between World War II and the crack boom had been marked by steady and often dramatic improvement. Particularly since the civil rights legislation of the mid-1960s, the telltale signs of societal progress had finally taken root among black Americans. The black-white income gap was shrinking. So was the gap between black children’s test scores and those of white children. Perhaps the most heartening gain had been in infant mortality. As late as 1964, a black infant was twice as likely to die as a white infant, often of a cause as basic as diarrhea or pneumonia. With segregated hospitals, many black patients received what amounted to Third World care. But that changed when the federal government ordered the hospitals to be desegregated: within just seven years, the black infant mortality rate had been cut in half. By the 1980s, virtually every facet of life was improving for black Americans, and the progress showed no sign of stopping.

Then came crack cocaine.

While crack use was hardly a black-only phenomenon, it hit black neighborhoods much harder than most. The evidence can be seen by measuring the same indicators of societal progress cited above. After decades of decline, black infant mortality began to soar in the 1980s, as did the rate of low-birthweight babies and parent abandonment. The gap between black and white schoolchildren widened. The number of blacks sent to prison tripled. Crack was so dramatically destructive that if its effect is averaged for all black Americans, not just crack users and their families, you will see that the group’s postwar progress was not only stopped cold but was often knocked as much as ten years backward. Black Americans were hurt more by crack cocaine than by any other single cause since Jim Crow.

And then there was the crime. Within a five-year period, the homicide rate among young urban blacks quadrupled. Suddenly it was just as dangerous to live in parts of Chicago or St. Louis or Los Angeles as it was to live in Bogotá.

The violence associated with the crack boom was various and relentless. It coincided with an even broader American crime wave that had been building for two decades. Although the rise of this crime wave long predated crack, the trend was so exacerbated by crack that criminologists got downright apocalyptic in their predictions. James Alan Fox, perhaps the most widely quoted crime expert in the popular press, warned of a coming “bloodbath” of youth violence.

But Fox and the other purveyors of conventional wisdom turned out to be wrong. The bloodbath did not materialize. The crime rate in fact began to fall—so unexpectedly and dramatically and thoroughly that now, from the distance of several years, it is almost hard to recall the crushing grip of that crime wave.

Why did it fall?

For a few reasons, but one of them more surprising than the rest. Oscar Danilo Blandon, the so-called Johnny Appleseed of Crack, may have been the instigator of one ripple effect, in which by his actions a single person inadvertently causes an ocean of despair. But unbeknownst to just about everybody, another remarkably powerful ripple effect—this one moving in the opposite direction—had just come into play.

4


Where Have All the Criminals Gone?


In 1966, one year after Nicolae Ceauşescu became the Communist dictator of Romania, he made abortion illegal. “The fetus is the property of the entire society,” he proclaimed. “Anyone who avoids having children is a deserter who abandons the laws of national continuity.”

Such grandiose declarations were commonplace during Ceauşescu’s reign, for his master plan—to create a nation worthy of the New Socialist Man—was an exercise in grandiosity. He built palaces for himself while alternately brutalizing and neglecting his citizens. Abandoning agriculture in favor of manufacturing, he forced many of the nation’s rural dwellers into unheated apartment buildings. He gave government positions to forty family members including his wife, Elena, who required forty homes and a commensurate supply of fur and jewels. Madame Ceauşescu, known officially as the Best Mother Romania Could Have, was not particularly maternal. “The worms never get satisfied, regardless of how much food you give them,” she said when Romanians complained about the food shortages brought on by her husband’s mismanagement. She had her own children bugged to ensure their loyalty.

Ceauşescu’s ban on abortion was designed to achieve one of his major aims: to rapidly strengthen Romania by boosting its population. Until 1966, Romania had had one of the most liberal abortion policies in the world. Abortion was in fact the main form of birth control, with four abortions for every live birth. Now, virtually overnight, abortion was forbidden. The only exemptions were mothers who already had four children or women with significant standing in the Communist Party. At the same time, all contraception and sex education were banned. Government agents sardonically known as the Menstrual Police regularly rounded up women in their workplaces to administer pregnancy tests. If a woman repeatedly failed to conceive, she was forced to pay a steep “celibacy tax.”

Ceauşescu’s incentives produced the desired effect. Within one year of the abortion ban, the Romanian birth rate had doubled. These babies were born into a country where, unless you belonged to the Ceauşescu clan or the Communist elite, life was miserable. But these children would turn out to have particularly miserable lives. Compared to Romanian children born just a year earlier, the cohort of children born after the abortion ban would do worse in every measurable way: they would test lower in school, they would have less success in the labor market, and they would also prove much more likely to become criminals.

The abortion ban stayed in effect until Ceauşescu finally lost his grip on Romania. On December 16, 1989, thousands of people took to the streets of Timisoara to protest his corrosive regime. Many of the protestors were teenagers and college students. The police killed dozens of them. One of the opposition leaders, a forty-one-year-old professor, later said it was his thirteen-year-old daughter who insisted he attend the protest, despite his fear. “What is most interesting is that we learned not to be afraid from our children,” he said. “Most were aged thirteen to twenty.” A few days after the massacre in Timisoara, Ceauşescu gave a speech in Bucharest before one hundred thousand people. Again the young people were out in force. They shouted down Ceauşescu with cries of “Timisoara!” and “Down with the murderers!” His time had come. He and Elena tried to escape the country with $1 billion, but they were captured, given a crude trial, and, on Christmas Day, executed by firing squad.

Of all the Communist leaders deposed in the years bracketing the collapse of the Soviet Union, only Nicolae Ceauşescu met a violent death. It should not be overlooked that his demise was precipitated in large measure by the youth of Romania—a great number of whom, were it not for his abortion ban, would never have been born at all.

The story of abortion in Romania might seem an odd way to begin telling the story of American crime in the 1990s. But it’s not. In one important way, the Romanian abortion story is a reverse image of the American crime story. The point of overlap was on that Christmas Day of 1989, when Nicolae Ceauşescu learned the hard way—with a bullet to the head—that his abortion ban had much deeper implications than he knew.

On that day, crime was just about at its peak in the United States. In the previous fifteen years, violent crime had risen 80 percent. It was crime that led the nightly news and the national conversation.

When the crime rate began falling in the early 1990s, it did so with such speed and suddenness that it surprised everyone. It took some experts many years to even recognize that crime was falling, so confident had they been of its continuing rise. Long after crime had peaked, in fact, some of them continued to predict ever darker scenarios. But the evidence was irrefutable: the long and brutal spike in crime was moving in the opposite direction, and it wouldn’t stop until the crime rate had fallen back to the levels of forty years earlier.

Now the experts hustled to explain their faulty forecasting. The criminologist James Alan Fox explained that his warning of a “bloodbath” was in fact an intentional overstatement. “I never said there would be blood flowing in the streets,” he said, “but I used strong terms like ‘bloodbath’ to get people’s attention. And it did. I don’t apologize for using alarmist terms.” (If Fox seems to be offering a distinction without a difference—“bloodbath” versus “blood flowing in the streets”—we should remember that even in retreat mode, experts can be self-serving.)

After the relief had settled in, after people remembered how to go about their lives without the pressing fear of crime, there arose a natural question: just where did all those criminals go?

At one level, the answer seemed puzzling. After all, if none of the criminologists, police officials, economists, politicians, or others who traffic in such matters had foreseen the crime decline, how could they suddenly identify its causes?

But this diverse army of experts now marched out a phalanx of hypotheses to explain the drop in crime. A great many newspaper articles would be written on the subject. Their conclusions often hinged on which expert had most recently spoken to which reporter. Here, ranked by frequency of mention, are the crime-drop explanations cited in articles published from 1991 to 2001 in the ten largest-circulation papers in the LexisNexis database:

If you are the sort of person who likes guessing games, you may wish to spend the next few moments pondering which of the preceding explanations seem to have merit and which don’t. Hint: of the seven major explanations on the list, only three can be shown to have contributed to the drop in crime. The others are, for the most part, figments of someone’s imagination, self-interest, or wishful thinking. Further hint: one of the greatest measurable causes of the crime drop does not appear on the list at all, for it didn’t receive a single newspaper mention.

Let’s begin with a fairly uncontroversial one: the strong economy. The decline in crime that began in the early 1990s was accompanied by a blistering national economy and a significant drop in unemployment. It might seem to follow that the economy was a hammer that helped beat down crime. But a closer look at the data destroys this theory. It is true that a stronger job market may make certain crimes relatively less attractive. But that is only the case for crimes with a direct financial motivation—burglary, robbery, and auto theft—as opposed to violent crimes like homicide, assault, and rape. Moreover, studies have shown that an unemployment decline of 1 percentage point accounts for a 1 percent drop in nonviolent crime. During the 1990s, the unemployment rate fell by 2 percentage points; nonviolent crime, meanwhile, fell by roughly 40 percent. But an even bigger flaw in the strong-economy theory concerns violent crime. Homicide fell at a greater rate during the 1990s than any other sort of crime, and a number of reliable studies have shown virtually no link between the economy and violent crime. This weak link is made even weaker by glancing back to a recent decade, the 1960s, when the economy went on a wild growth spurt—as did violent crime. So while a strong 1990s economy might have seemed, on the surface, a likely explanation for the drop in crime, it almost certainly didn’t affect criminal behavior in any significant way.

Unless, that is, “the economy” is construed in a broader sense—as a means to build and maintain hundreds of prisons. Let’s now consider another crime-drop explanation: increased reliance on prisons. It might help to start by flipping the crime question around. Instead of wondering what made crime fall, think about this: why had it risen so dramatically in the first place?

During the first half of the twentieth century, the incidence of violent crime in the United States was, for the most part, fairly steady. But in the early 1960s, it began to climb. In retrospect, it is clear that one of the major factors pushing this trend was a more lenient justice system. Conviction rates declined during the 1960s, and criminals who were convicted served shorter sentences. This trend was driven in part by an expansion in the rights of people accused of crimes—a long overdue expansion, some would argue. (Others would argue that the expansion went too far.) At the same time, politicians were growing increasingly softer on crime—“for fear of sounding racist,” as the economist Gary Becker has written, “since African-Americans and Hispanics commit a disproportionate share of felonies.” So if you were the kind of person who might want to commit a crime, the incentives were lining up in your favor: a slimmer likelihood of being convicted and, if convicted, a shorter prison term. Because criminals respond to incentives as readily as anyone, the result was a surge in crime.

It took some time, and a great deal of political turmoil, but these incentives were eventually curtailed. Criminals who would have previously been set free—for drug-related offenses and parole revocation in particular—were instead locked up. Between 1980 and 2000, there was a fifteenfold increase in the number of people sent to prison on drug charges. Many other sentences, especially for violent crime, were lengthened. The total effect was dramatic. By 2000, more than two million people were in prison, roughly four times the number as of1972. Fully half of that increase took place during the 1990s.

The evidence linking increased punishment with lower crime rates is very strong. Harsh prison terms have been shown to act as both deterrent (for the would-be criminal on the street) and prophylactic (for the would-be criminal who is already locked up). Logical as this may sound, some criminologists have fought the logic. A 1977 academic study called “On Behalf of a Moratorium on Prison Construction” noted that crime rates tend to be high when imprisonment rates are high, and concluded that crime would fall if imprisonment rates could only be lowered. (Fortunately, jailers did not suddenly turn loose their wards and sit back waiting for crime to fall. As the political scientist John J. DiIulio Jr. later commented, “Apparently, it takes a Ph.D. in criminology to doubt that keeping dangerous criminals incarcerated cuts crime.”)

The “Moratorium” argument rests on a fundamental confusion of correlation and causality. Consider a parallel argument. The mayor of a city sees that his citizens celebrate wildly when their team wins the World Series. He is intrigued by this correlation but, like the “Moratorium” author, fails to see the direction in which the correlation runs. So the following year, the mayor decrees that his citizens start celebrating the World Series before the first pitch is thrown—an act that, in his confused mind, will ensure a victory.

There are certainly plenty of reasons to dislike the huge surge in the prison population. Not everyone is pleased that such a significant fraction of Americans, especially black Americans, live behind bars. Nor does prison even begin to address the root causes of crime, which are diverse and complex. Lastly, prison is hardly a cheap solution: it costs about $25,000 a year to keep someone incarcerated. But if the goal here is to explain the drop in crime in the 1990s, imprisonment is certainly one of the key answers. It accounts for roughly one-third of the drop in crime.

Another crime-drop explanation is often cited in tandem with imprisonment: the increased use of capital punishment. The number of executions in the United States quadrupled between the 1980s and the 1990s, leading many people to conclude—in the context of a debate that has been going on for decades—that capital punishment helped drive down crime. Lost in the debate, however, are two important facts.

First, given the rarity with which executions are carried out in this country and the long delays in doing so, no reasonable criminal should be deterred by the threat of execution. Even though capital punishment quadrupled within a decade, there were still only 478 executions in the entire United States during the 1990s. Any parent who has ever said to a recalcitrant child, “Okay, I’m going to count to ten and this time I’m really going to punish you,” knows the difference between deterrent and empty threat. New York State, for instance, has not as of this writing executed a single criminal since reinstituting its death penalty in 1995. Even among prisoners on death row, the annual execution rate is only 2 percent—compared with the 7 percent annual chance of dying faced by a member of the Black Gangster Disciple Nation crack gang. If life on death row is safer than life on the streets, it’s hard to believe that the fear of execution is a driving force in a criminal’s calculus. Like the $3 fine for late-arriving parents at the Israeli day-care centers, the negative incentive of capital punishment simply isn’t serious enough for a criminal to change his behavior.

The second flaw in the capital punishment argument is even more obvious. Assume for a moment that the death penalty is a deterrent. How much crime does it actually deter? The economist Isaac Ehrlich, in an oft-cited 1975 paper, put forth an estimate that is generally considered optimistic: executing 1 criminal translates into 7 fewer homicides that the criminal might have committed. Now do the math. In 1991, there were 14 executions in the United States; in 2001, there were 66. According to Ehrlich’s calculation, those 52 additional executions would have accounted for 364 fewer homicides in 2001—not a small drop, to be sure, but less than 4 percent of the actual decrease in homicides that year. So even in a death penalty advocate’s best-case scenario, capital punishment could explain only one twenty-fifth of the drop in homicides in the 1990s. And because the death penalty is rarely given for crimes other than homicide, its deterrent effect cannot account for a speck of decline in other violent crimes.

It is extremely unlikely, therefore, that the death penalty, as currently practiced in the United States, exerts any real influence on crime rates. Even many of its onetime supporters have come to this conclusion. “I feel morally and intellectually obligated simply to concede that the death penalty experiment has failed,” said U.S. Supreme Court Justice Harry A. Blackmun in 1994, nearly twenty years after he had voted for its reinstatement. “I no longer shall tinker with the machinery of death.”

So it wasn’t capital punishment that drove crime down, nor was it the booming economy. But higher rates of imprisonment did have a lot to do with it. All those criminals didn’t march into jail by themselves, of course. Someone had to investigate the crime, catch the bad guy, and put together the case that would get him convicted. Which naturally leads to a related pair of crime-drop explanations:


Innovative policing strategies

Increased number of police

Let’s address the second one first. The number of police officers per capita in the United States rose about 14 percent during the 1990s. Does merely increasing the number of police, however, reduce crime? The answer would seem obvious—yes—but proving that answer isn’t so easy. That’s because when crime is rising, people clamor for protection, and invariably more money is found for cops. So if you just look at raw correlations between police and crime, you will find that when there are more police, there tends to be more crime. That doesn’t mean, of course, that the police are causing the crime, just as it doesn’t mean, as some criminologists have argued, that crime will fall if criminals are released from prison.

To show causality, we need a scenario in which more police are hired for reasons completely unrelated to rising crime. If, for instance, police were randomly sprinkled in some cities and not in others, we could look to see whether crime declines in the cities where the police happen to land.

As it turns out, that exact scenario is often created by vote-hungry politicians. In the months leading up to Election Day, incumbent mayors routinely try to lock up the law-and-order vote by hiring more police—even when the crime rate is standing still. So by comparing the crime rate in one set of cities that have recently had an election (and which therefore hired extra police) with another set of cities that had no election (and therefore no extra police), it’s possible to tease out the effect of the extra police on crime. The answer: yes indeed, additional police substantially lower the crime rate.

Again, it may help to look backward and see why crime had risen so much in the first place. From 1960 to 1985, the number of police officers fell more than 50 percent relative to the number of crimes. In some cases, hiring additional police was considered a violation of the era’s liberal aesthetic; in others, it was simply deemed too expensive. This 50 percent decline in police translated into a roughly equal decline in the probability that a given criminal would be caught. Coupled with the above-cited leniency in the other half of the criminal justice system, the courtrooms, this decrease in policing created a strong positive incentive for criminals.

By the 1990s, philosophies—and necessities—had changed. The policing trend was put in reverse, with wide-scale hiring in cities across the country. Not only did all those police act as a deterrent, but they also provided the manpower to imprison criminals who might have otherwise gone uncaught. The hiring of additional police accounted for roughly 10 percent of the 1990s crime drop.

But it wasn’t only the number of police that changed in the 1990s; consider the most commonly cited crime-drop explanation of all: innovative policing strategies.

There was perhaps no more attractive theory than the belief that smart policing stops crime. It offered a set of bona fide heroes rather than simply a dearth of villains. This theory rapidly became an article of faith because it appealed to the factors that, according to John Kenneth Galbraith, most contribute to the formation of conventional wisdom: the ease with which an idea may be understood and the degree to which it affects our personal well-being.

The story played out most dramatically in New York City, where newly elected mayor Rudolph Giuliani and his handpicked police commissioner, William Bratton, vowed to fix the city’s desperate crime situation. Bratton took a novel approach to policing. He ushered the NYPD into what one senior police official later called “our Athenian period,” in which new ideas were given weight over calcified practices. Instead of coddling his precinct commanders, Bratton demanded accountability. Instead of relying solely on old-fashioned cop know-how, he introduced technological solutions like CompStat, a computerized method of addressing crime hot spots.

The most compelling new idea that Bratton brought to life stemmed from the broken window theory, which was conceived by the criminologists James Q. Wilson and George Kelling. The broken window theory argues that minor nuisances, if left unchecked, turn into major nuisances: that is, if someone breaks a window and sees it isn’t fixed immediately, he gets the signal that it’s all right to break the rest of the windows and maybe set the building afire too.

So with murder raging all around, Bill Bratton’s cops began to police the sort of deeds that used to go unpoliced: jumping a subway turnstile, panhandling too aggressively, urinating in the streets, swabbing a filthy squeegee across a car’s windshield unless the driver made an appropriate “donation.”

Most New Yorkers loved this crackdown on its own merit. But they particularly loved the idea, as stoutly preached by Bratton and Giuliani, that choking off these small crimes was like choking off the criminal element’s oxygen supply. Today’s turnstile jumper might easily be wanted for yesterday’s murder. That junkie peeing in an alley might have been on his way to a robbery.

As violent crime began to fall dramatically, New Yorkers were more than happy to heap laurels on their operatic, Brooklyn-bred mayor and his hatchet-faced police chief with the big Boston accent. But the two strong-willed men weren’t very good at sharing the glory. Soon after the city’s crime turnaround landed Bratton—and not Giuliani—on the cover of Time, Bratton was pushed to resign. He had been police commissioner for just twenty-seven months.

New York City was a clear innovator in police strategies during the 1990s crime drop, and it also enjoyed the greatest decline in crime of any large American city. Homicide rates fell from 30.7 per 100,000 people in 1990 to 8.4 per 100,000 people in 2000, a change of 73.6 percent. But a careful analysis of the facts shows that the innovative policing strategies probably had little effect on this huge decline.

First, the drop in crime in New York began in 1990. By the end of 1993, the rate of property crime and violent crime, including homicides, had already fallen nearly 20 percent. Rudolph Giuliani, however, did not become mayor—and install Bratton—until early 1994. Crime was well on its way down before either man arrived. And it would continue to fall long after Bratton was bumped from office.

Second, the new police strategies were accompanied by a much more significant change within the police force: a hiring binge. Between 1991 and 2001, the NYPD grew by 45 percent, more than three times the national average. As argued above, an increase in the number of police, regardless of new strategies, has been proven to reduce crime. By a conservative calculation, this huge expansion of New York’s police force would be expected to reduce crime in New York by 18 percent relative to the national average. If you subtract that 18 percent from New York’s homicide reduction, thereby discounting the effect of the police-hiring surge, New York no longer leads the nation with its 73.6 percent drop; it goes straight to the middle of the pack. Many of those new police were in fact hired by David Dinkins, the mayor whom Giuliani defeated. Dinkins had been desperate to secure the law-and-order vote, having known all along that his opponent would be Giuliani, a former federal prosecutor. (The two men had run against each other four years earlier as well.) So those who wish to credit Giuliani with the crime drop may still do so, for it was his own law-and-order reputation that made Dinkins hire all those police. In the end, of course, the police increase helped everyone—but it helped Giuliani a lot more than Dinkins.

Most damaging to the claim that New York’s police innovations radically lowered crime is one simple and often overlooked fact: crime went down everywhere during the 1990s, not only in New York. Few other cities tried the kind of strategies that New York did, and certainly none with the same zeal. But even in Los Angeles, a city notorious for bad policing, crime fell at about the same rate as it did in New York once the growth in New York’s police force is accounted for.

It would be churlish to argue that smart policing isn’t a good thing. Bill Bratton certainly deserves credit for invigorating New York’s police force. But there is frighteningly little evidence that his strategy was the crime panacea that he and the media deemed it. The next step will be to continue measuring the impact of police innovations—in Los Angeles, for instance, where Bratton himself became police chief in late 2002. While he duly instituted some of the innovations that were his hallmark in New York, Bratton announced that his highest priority was a more basic one: finding the money to hire thousands of new police officers.

Now to explore another pair of common crime-drop explanations:


Tougher gun laws

Changes in crack and other drug markets

First, the guns. Debates on this subject are rarely coolheaded. Gun advocates believe that gun laws are too strict; opponents believe exactly the opposite. How can intelligent people view the world so differently? Because a gun raises a complex set of issues that change according to one factor: whose hand happens to be holding the gun.

It might be worthwhile to take a step back and ask a rudimentary question: what is a gun? It’s a tool that can be used to kill someone, of course, but more significantly, a gun is a great disrupter of the natural order.

A gun scrambles the outcome of any dispute. Let’s say that a tough guy and a not-so-tough guy exchange words in a bar, which leads to a fight. It’s pretty obvious to the not-so-tough guy that he’ll be beaten, so why bother fighting? The pecking order remains intact. But if the not-so-tough guy happens to have a gun, he stands a good chance of winning. In this scenario, the introduction of a gun may well lead to more violence.

Now instead of the tough guy and the not-so-tough guy, picture a high-school girl out for a nighttime stroll when she is suddenly set upon by a mugger. What if only the mugger is armed? What if only the girl is armed? What if both are armed? A gun opponent might argue that the gun has to be kept out of the mugger’s hands in the first place. A gun advocate might argue that the high-school girl needs to have a gun to disrupt what has become the natural order: it’s the bad guys that have the guns. (If the girl scares off the mugger, then the introduction of a gun in this case may lead to less violence.) Any mugger with even a little initiative is bound to be armed, for in a country like the United States, with a thriving black market in guns, anyone can get hold of one.

There are enough guns in the United States that if you gave one to every adult, you would run out of adults before you ran out of guns. Nearly two-thirds of U.S. homicides involve a gun, a far greater fraction than in other industrialized countries. Our homicide rate is also much higher than in those countries. It would therefore seem likely that our homicide rate is so high in part because guns are so easily available. Research indeed shows this to be true.

But guns are not the whole story. In Switzerland, every adult male is issued an assault rifle for militia duty and is allowed to keep the gun at home. On a per capita basis, Switzerland has more firearms than just about any other country, and yet it is one of the safest places in the world. In other words, guns do not cause crime. That said, the established U.S. methods of keeping guns away from the people who do cause crime are, at best, feeble. And since a gun—unlike a bag of cocaine or a car or a pair of pants—lasts pretty much forever, even turning off the spigot of new guns still leaves an ocean of available ones.

So bearing all this in mind, let’s consider a variety of recent gun initiatives to see the impact they may have had on crime in the 1990s.

The most famous gun-control law is the Brady Act, passed in 1993, which requires a criminal check and a waiting period before a person can purchase a handgun. This solution may have seemed appealing to politicians, but to an economist it doesn’t make much sense. Why? Because regulation of a legal market is bound to fail when a healthy black market exists for the same product. With guns so cheap and so easy to get, the standard criminal has no incentive to fill out a firearms application at his local gun shop and then wait a week. The Brady Act, accordingly, has proven to be practically impotent in lowering crime. (A study of imprisoned felons showed that even before the Brady Act, only about one-fifth of the criminals had bought their guns through a licensed dealer.) Various local gun-control laws have also failed. Washington, D.C., and Chicago both instituted handgun bans well before crime began to fall across the country in the 1990s, and yet those two cities were laggards, not leaders, in the national reduction in crime. One deterrent that has proven moderately effective is a stiff increase in prison time for anyone caught in possession of an illegal gun. But there is plenty of room for improvement. Not that this is likely, but if the death penalty were assessed to anyone carrying an illegal gun, and if the penalty were actually enforced, gun crimes would surely plunge.

Another staple of 1990s crime fighting—and of the evening news—was the gun buyback. You remember the image: a menacing, glistening heap of firearms surrounded by the mayor, the police chief, the neighborhood activists. It made for a nice photo op, but that’s about as meaningful as a gun buyback gets. The guns that are turned in tend to be heirlooms or junk. The payoff to the gun seller—usually $50 or $100, but in one California buyback, three free hours of psychotherapy—isn’t an adequate incentive for anyone who actually plans to use his gun. And the number of surrendered guns is no match for even the number of new guns simultaneously coming to market. Given the number of handguns in the United States and the number of homicides each year, the likelihood that a particular gun was used to kill someone that year is 1 in 10,000. The typical gun buyback program yields fewer than 1,000 guns—which translates into an expectation of less than one-tenth of one homicide per buyback. Not enough, that is, to make even a sliver of impact on the fall of crime.

Then there is an opposite argument—that we need more guns on the street, but in the hands of the right people (like the high-school girl above, instead of her mugger). The economist John R. Lott Jr. is the main champion of this idea. His calling card is the book More Guns, Less Crime, in which he argues that violent crime has decreased in areas where law-abiding citizens are allowed to carry concealed weapons. His theory might be surprising, but it is sensible. If a criminal thinks his potential victim may be armed, he may be deterred from committing the crime. Handgun opponents call Lott a pro-gun ideologue, and Lott let himself become a lightning rod for gun controversy. He exacerbated his trouble by creating a pseudonym, “Mary Rosh,” to defend his theory in online debates. Rosh, identifying herself as a former student of Lott’s, praised her teacher’s intellect, his evenhandedness, his charisma. “I have to say that he was the best professor that I ever had,” s/he wrote. “You wouldn’t know that he was a ‘right-wing’ ideologue from the class….There were a group of us students who would try to take any class that he taught. Lott finally had to tell us that it was best for us to try and take classes from other professors more to be exposed to other ways of teaching graduate material.” Then there was the troubling allegation that Lott actually invented some of the survey data that support his more-guns/less-crime theory. Regardless of whether the data were faked, Lott’s admittedly intriguing hypothesis doesn’t seem to be true. When other scholars have tried to replicate his results, they found that right-to-carry laws simply don’t bring down crime.

Consider the next crime-drop explanation: the bursting of the crack bubble. Crack cocaine was such a potent, addictive drug that a hugely profitable market had been created practically overnight. True, it was only the leaders of the crack gangs who were getting rich. But that only made the street-level dealers all the more desperate to advance. Many of them were willing to kill their rivals to do so, whether the rival belonged to the same gang or a different one. There were also gun battles over valuable drug-selling corners. The typical crack murder involved one crack dealer shooting another (or two of them, or three) and not, contrary to conventional wisdom, some bug-eyed crackhead shooting a shopkeeper over a few dollars. The result was a huge increase in violent crime. One study found that more than 25 percent of the homicides in New York City in 1988 were crack-related.

The violence associated with crack began to ebb in about 1991. This has led many people to think that crack itself went away. It didn’t. Smoking crack remains much more popular today than most people realize. Nearly 5 percent of all arrests in the United States are still related to cocaine (as against 6 percent at crack’s peak); nor have emergency room visits for crack users diminished all that much.

What did go away were the huge profits for selling crack. The price of cocaine had been falling for years, and it got only cheaper as crack grew more popular. Dealers began to underprice one another; profits vanished. The crack bubble burst as dramatically as the Nasdaq bubble would eventually burst. (Think of the first generation of crack dealers as the Microsoft millionaires; think of the second generation as Pets.com.) As veteran crack dealers were killed or sent to prison, younger dealers decided that the smaller profits didn’t justify the risk. The tournament had lost its allure. It was no longer worth killing someone to steal their crack turf, and certainly not worth being killed.

So the violence abated. From 1991 to 2001, the homicide rate among young black men—who were disproportionately represented among crack dealers—fell 48 percent, compared to 30 percent for older black men and older white men. (Another minor contributor to the falling homicide rate is the fact that some crack dealers took to shooting their enemies in the buttocks rather than murdering them; this method of violent insult was considered more degrading—and was obviously less severely punished—than murder.) All told, the crash of the crack market accounted for roughly 15 percent of the crime drop of the 1990s—a substantial factor, to be sure, though it should be noted that crack was responsible for far more than 15 percent of the crime increase of the 1980s. In other words, the net effect of crack is still being felt in the form of violent crime, to say nothing of the miseries the drug itself continues to cause.

The final pair of crime-drop explanations concern two demographic trends. The first one received many media citations: aging of the population.

Until crime fell so drastically, no one talked about this theory at all. In fact, the “bloodbath” school of criminology was touting exactly the opposite theory—that an increase in the teenage share of the population would produce a crop of superpredators who would lay the nation low. “Just beyond the horizon, there lurks a cloud that the winds will soon bring over us,” James Q. Wilson wrote in 1995. “The population will start getting younger again….Get ready.”

But overall, the teenage share of the population wasn’t getting much bigger. Criminologists like Wilson and James Alan Fox had badly misread the demographic data. The real population growth in the 1990s was in fact among the elderly. While this may have been scary news in terms of Medicare and Social Security, the average American had little to fear from the growing horde of oldsters. It shouldn’t be surprising to learn that elderly people are not very criminally intent; the average sixty-five-year-old is about one-fiftieth as likely to be arrested as the average teenager. That is what makes this aging-of-the-population theory of crime reduction so appealingly tidy: since people mellow out as they get older, more older people must lead to less crime. But a thorough look at the data reveals that the graying of America did nothing to bring down crime in the 1990s. Demographic change is too slow and subtle a process—you don’t graduate from teenage hoodlum to senior citizen in just a few years—to even begin to explain the suddenness of the crime decline.

There was another demographic change, however, unforeseen and long-gestating, that did drastically reduce crime in the 1990s.

Think back for a moment to Romania in 1966. Suddenly and without warning, Nicolae Ceauşescu declared abortion illegal. The children born in the wake of the abortion ban were much more likely to become criminals than children born earlier. Why was that? Studies in other parts of Eastern Europe and in Scandinavia from the 1930s through the 1960s reveal a similar trend. In most of these cases, abortion was not forbidden outright, but a woman had to receive permission from a judge in order to obtain one. Researchers found that in the instances where the woman was denied an abortion, she often resented her baby and failed to provide it with a good home. Even when controlling for the income, age, education, and health of the mother, the researchers found that these children too were more likely to become criminals.

The United States, meanwhile, has had a different abortion history than Europe. In the early days of the nation, it was permissible to have an abortion prior to “quickening”—that is, when the first movements of the fetus could be felt, usually around the sixteenth to eighteenth week of pregnancy. In 1828, New York became the first state to restrict abortion; by 1900 it had been made illegal throughout the country. Abortion in the twentieth century was often dangerous and usually expensive. Fewer poor women, therefore, had abortions. They also had less access to birth control. What they did have, accordingly, was a lot more babies.

In the late 1960s, several states began to allow abortion under extreme circumstances: rape, incest, or danger to the mother. By 1970 five states had made abortion entirely legal and broadly available: New York, California, Washington, Alaska, and Hawaii. On January 22, 1973, legalized abortion was suddenly extended to the entire country with the U.S. Supreme Court’s ruling in Roe v. Wade. The majority opinion, written by Justice Harry Blackmun, spoke specifically to the would-be mother’s predicament:


The detriment that the State would impose upon the pregnant woman by denying this choice altogether is apparent…. Maternity, or additional offspring, may force upon the woman a distressful life and future. Psychological harm may be imminent. Mental and physical health may be taxed by child care. There is also the distress, for all concerned, associated with the unwanted child, and there is the problem of bringing a child into a family already unable, psychologically and otherwise, to care for it.

The Supreme Court gave voice to what the mothers in Romania and Scandinavia—and elsewhere—had long known: when a woman does not want to have a child, she usually has good reason. She may be unmarried or in a bad marriage. She may consider herself too poor to raise a child. She may think her life is too unstable or unhappy, or she may think that her drinking or drug use will damage the baby’s health. She may believe that she is too young or hasn’t yet received enough education. She may want a child badly but in a few years, not now. For any of a hundred reasons, she may feel that she cannot provide a home environment that is conducive to raising a healthy and productive child.

In the first year after Roe v. Wade, some 750,000 women had abortions in the United States (representing one abortion for every 4 live births). By 1980 the number of abortions reached 1.6 million (one for every 2.25 live births), where it leveled off. In a country of 225 million people, 1.6 million abortions per year—one for every 140 Americans—may not have seemed so dramatic. In the first year after Nicolae Ceauşescu’s death, when abortion was reinstated in Romania, there was one abortion for every twenty-two Romanians. But still: 1.6 million American women a year who got pregnant were suddenly not having those babies.

Before Roe v. Wade, it was predominantly the daughters of middle-or upper-class families who could arrange and afford a safe illegal abortion. Now, instead of an illegal procedure that might cost $500, any woman could easily obtain an abortion, often for less than $100.

What sort of woman was most likely to take advantage of Roe v. Wade? Very often she was unmarried or in her teens or poor, and sometimes all three. What sort of future might her child have had? One study has shown that the typical child who went unborn in the earliest years of legalized abortion would have been 50 percent more likely than average to live in poverty; he would have also been 60 percent more likely to grow up with just one parent. These two factors—childhood poverty and a single-parent household—are among the strongest predictors that a child will have a criminal future. Growing up in a single-parent home roughly doubles a child’s propensity to commit crime. So does having a teenage mother. Another study has shown that low maternal education is the single most powerful factor leading to criminality.

In other words, the very factors that drove millions of American women to have an abortion also seemed to predict that their children, had they been born, would have led unhappy and possibly criminal lives.

To be sure, the legalization of abortion in the United States had myriad consequences. Infanticide fell dramatically. So did shotgun marriages, as well as the number of babies put up for adoption (which has led to the boom in the adoption of foreign babies). Conceptions rose by nearly 30 percent, but births actually fell by 6 percent, indicating that many women were using abortion as a method of birth control, a crude and drastic sort of insurance policy.

Perhaps the most dramatic effect of legalized abortion, however, and one that would take years to reveal itself, was its impact on crime. In the early 1990s, just as the first cohort of children born after Roe v. Wade was hitting its late teen years—the years during which young men enter their criminal prime—the rate of crime began to fall. What this cohort was missing, of course, were the children who stood the greatest chance of becoming criminals. And the crime rate continued to fall as an entire generation came of age minus the children whose mothers had not wanted to bring a child into the world. Legalized abortion led to less unwantedness; unwantedness leads to high crime; legalized abortion, therefore, led to less crime.

This theory is bound to provoke a variety of reactions, ranging from disbelief to revulsion, and a variety of objections, ranging from the quotidian to the moral. The likeliest first objection is the most straightforward one: is the theory true? Perhaps abortion and crime are merely correlated and not causal.

It may be more comforting to believe what the newspapers say, that the drop in crime was due to brilliant policing and clever gun control and a surging economy. We have evolved with a tendency to link causality to things we can touch or feel, not to some distant or difficult phenomenon. We believe especially in near-term causes: a snake bites your friend, he screams with pain, and he dies. The snakebite, you conclude, must have killed him. Most of the time, such a reckoning is correct. But when it comes to cause and effect, there is often a trap in such open-and-shut thinking. We smirk now when we think of ancient cultures that embraced faulty causes—the warriors who believed, for instance, that it was their raping of a virgin that brought them victory on the battlefield. But we too embrace faulty causes, usually at the urging of an expert proclaiming a truth in which he has a vested interest.

How, then, can we tell if the abortion-crime link is a case of causality rather than simply correlation?

One way to test the effect of abortion on crime would be to measure crime data in the five states where abortion was made legal before the Supreme Court extended abortion rights to the rest of the country. In New York, California, Washington, Alaska, and Hawaii, a woman had been able to obtain a legal abortion for at least two years before Roe v. Wade. And indeed, those early-legalizing states saw crime begin to fall earlier than the other forty-five states and the District of Columbia. Between 1988 and 1994, violent crime in the early-legalizing states fell 13 percent compared to the other states; between 1994 and 1997, their murder rates fell 23 percent more than those of the other states.

But what if those early legalizers simply got lucky? What else might we look for in the data to establish an abortion-crime link?

One factor to look for would be a correlation between each state’s abortion rate and its crime rate. Sure enough, the states with the highest abortion rates in the 1970s experienced the greatest crime drops in the 1990s, while states with low abortion rates experienced smaller crime drops. (This correlation exists even when controlling for a variety of factors that influence crime: a state’s level of incarceration, number of police, and its economic situation.) Since 1985, states with high abortion rates have experienced a roughly 30 percent drop in crime relative to low-abortion states. (New York City had high abortion rates and lay within an early-legalizing state, a pair of facts that further dampen the claim that innovative policing caused the crime drop.) Moreover, there was no link between a given state’s abortion rate and its crime rate before the late 1980s—when the first cohort affected by legalized abortion was reaching its criminal prime—which is yet another indication that Roe v. Wade was indeed the event that tipped the crime scale.

There are even more correlations, positive and negative, that shore up the abortion-crime link. In states with high abortion rates, the entire decline in crime was among the post-Roe cohort as opposed to older criminals. Also, studies of Australia and Canada have since established a similar link between legalized abortion and crime. And the post-Roe cohort was not only missing thousands of young male criminals but also thousands of single, teenage mothers—for many of the aborted baby girls would have been the children most likely to replicate their own mothers’ tendencies.

To discover that abortion was one of the greatest crime-lowering factors in American history is, needless to say, jarring. It feels less Darwinian than Swiftian; it calls to mind a long-ago dart attributed to G. K. Chesterton: when there aren’t enough hats to go around, the problem isn’t solved by lopping off some heads. The crime drop was, in the language of economists, an “unintended benefit” of legalized abortion. But one need not oppose abortion on moral or religious grounds to feel shaken by the notion of a private sadness being converted into a public good.

Indeed, there are plenty of people who consider abortion itself to be a violent crime. One legal scholar called legalized abortion worse than either slavery (since it routinely involves death) or the Holocaust (since the number of post-Roe abortions in the United States, roughly thirty-seven million as of 2004, outnumber the six million Jews killed in Europe). Whether or not one feels so strongly about abortion, it remains a singularly charged issue. Anthony V. Bouza, a former top police official in both the Bronx and Minneapolis, discovered this when he ran for Minnesota governor in 1994. A few years earlier, Bouza had written a book in which he called abortion “arguably the only effective crime-prevention device adopted in this nation since the late 1960s.” When Bouza’s opinion was publicized just before the election, he fell sharply in the polls. And then he lost.

However a person feels about abortion, a question is likely to come to mind: what are we to make of the trade-off of more abortion for less crime? Is it even possible to put a number on such a complicated transaction?

As it happens, economists have a curious habit of affixing numbers to complicated transactions. Consider the effort to save the northern spotted owl from extinction. One economic study found that in order to protect roughly five thousand owls, the opportunity costs—that is, the income surrendered by the logging industry and others—would be $46 billion, or just over $9 million per owl. After the Exxon Valdez oil spill in 1989, another study estimated the amount that the typical American household would be willing to pay to avoid another such disaster: $31. An economist can affix a value even to a particular body part. Consider the schedule that the state of Connecticut uses to compensate for work-related injuries.

Now, for the sake of argument, let’s ask an outrageous question: what is the relative value of a fetus and a newborn? If faced with the Solomonic task of sacrificing the life of one newborn for an indeterminate number of fetuses, what number might you choose? This is nothing but a thought exercise—obviously there is no right answer—but it may help clarify the impact of abortion on crime.

For a person who is either resolutely pro-life or resolutely pro-choice, this is a simple calculation. The first, believing that life begins at conception, would likely consider the value of a fetus versus the value of a newborn to be 1:1. The second person, believing that a woman’s right to an abortion trumps any other factor, would likely argue that no number of fetuses can equal even one newborn.

But let’s consider a third person. (If you identify strongly with either person number one or person number two, the following exercise might strike you as offensive, and you may want to skip this paragraph and the next.) This third person does not believe that a fetus is the 1:1 equivalent of a newborn, yet neither does he believe that a fetus has no relative value. Let’s say that he is forced, for the sake of argument, to affix a relative value, and he decides that 1 newborn is worth 100 fetuses.

There are roughly 1.5 million abortions in the United States every year. For a person who believes that 1 newborn is worth 100 fetuses, those 1.5 million abortions would translate—dividing 1.5 million by 100—into the equivalent of a loss of 15,000 human lives. Fifteen thousand lives: that happens to be about the same number of people who die in homicides in the United States every year. And it is far more than the number of homicides eliminated each year due to legalized abortion. So even for someone who considers a fetus to be worth only one one-hundredth of a human being, the trade-off between higher abortion and lower crime is, by an economist’s reckoning, terribly inefficient.

What the link between abortion and crime does say is this: when the government gives a woman the opportunity to make her own decision about abortion, she generally does a good job of figuring out if she is in a position to raise the baby well. If she decides she can’t, she often chooses the abortion.

But once a woman decides she will have her baby, a pressing question arises: what are parents supposed to do once a child is born?

5


What Makes a Perfect Parent?


Has there ever been another art so devoutly converted into a science as the art of parenting?

Over the recent decades, a vast and diverse flock of parenting experts has arisen. Anyone who tries even casually to follow their advice may be stymied, for the conventional wisdom on parenting seems to shift by the hour. Sometimes it is a case of one expert differing from another. At other times the most vocal experts suddenly agree en masse that the old wisdom was wrong and that the new wisdom is, for a little while at least, irrefutably right. Breast feeding, for example, is the only way to guarantee a healthy and intellectually advanced child—unless bottle feeding is the answer. A baby should always be put to sleep on her back—until it is decreed that she should only be put to sleep on her stomach. Eating liver is either a) toxic or b) imperative for brain development. Spare the rod and spoil the child; spank the child and go to jail.

In her book Raising America: Experts, Parents, and a Century of Advice About Children, Ann Hulbert documented how parenting experts contradict one another and even themselves. Their banter might be hilarious were it not so confounding and, often, scary. Gary Ezzo, who in the Babywise book series endorses an “infant-management strategy” for moms and dads trying to “achieve excellence in parenting,” stresses how important it is to train a baby, early on, to sleep alone through the night. Otherwise, Ezzo warns, sleep deprivation might “negatively impact an infant’s developing central nervous system” and lead to learning disabilities. Advocates of “co-sleeping,” meanwhile, warn that sleeping alone is harmful to a baby’s psyche and that he should be brought into the “family bed.” What about stimulation? In 1983 T. Berry Brazelton wrote that a baby arrives in the world “beautifully prepared for the role of learning about him-or herself and the world all around.” Brazelton favored early, ardent stimulation—an “interactive” child. One hundred years earlier, however, L. Emmett Holt cautioned that a baby is not a “plaything.” There should be “no forcing, no pressure, no undue stimulation” during the first two years of a child’s life, Holt believed; the brain is growing so much during that time that overstimulation might cause “a great deal of harm.” He also believed that a crying baby should never be picked up unless it is in pain. As Holt explained, a baby should be left to cry for fifteen to thirty minutes a day: “It is the baby’s exercise.”

The typical parenting expert, like experts in other fields, is prone to sound exceedingly sure of himself. An expert doesn’t so much argue the various sides of an issue as plant his flag firmly on one side. That’s because an expert whose argument reeks of restraint or nuance often doesn’t get much attention. An expert must be bold if he hopes to alchemize his homespun theory into conventional wisdom. His best chance of doing so is to engage the public’s emotions, for emotion is the enemy of rational argument. And as emotions go, one of them—fear—is more potent than the rest. The superpredator, Iraqi weapons of mass destruction, mad-cow disease, crib death: how can we fail to heed the expert’s advice on these horrors when, like that mean uncle telling too-scary stories to too-young children, he has reduced us to quivers?

No one is more susceptible to an expert’s fearmongering than a parent. Fear is in fact a major component of the act of parenting. A parent, after all, is the steward of another creature’s life, a creature who in the beginning is more helpless than the newborn of nearly any other species. This leads a lot of parents to spend a lot of their parenting energy simply being scared.

The problem is that they are often scared of the wrong things. It’s not their fault, really. Separating facts from rumors is always hard work, especially for a busy parent. And the white noise generated by the experts—to say nothing of the pressure exerted by fellow parents—is so overwhelming that they can barely think for themselves. The facts they do manage to glean have usually been varnished or exaggerated or otherwise taken out of context to serve an agenda that isn’t their own.

Consider the parents of an eight-year-old girl named, say, Molly. Her two best friends, Amy and Imani, each live nearby. Molly’s parents know that Amy’s parents keep a gun in their house, so they have forbidden Molly to play there. Instead, Molly spends a lot of time at Imani’s house, which has a swimming pool in the backyard. Molly’s parents feel good about having made such a smart choice to protect their daughter.

But according to the data, their choice isn’t smart at all. In a given year, there is one drowning of a child for every 11,000 residential pools in the United States. (In a country with 6 million pools, this means that roughly 550 children under the age of ten drown each year.) Meanwhile, there is 1 child killed by a gun for every 1 million-plus guns. (In a country with an estimated 200 million guns, this means that roughly 175 children under ten die each year from guns.) The likelihood of death by pool (1 in 11,000) versus death by gun (1 in 1 million-plus) isn’t even close: Molly is far more likely to die in a swimming accident at Imani’s house than in gunplay at Amy’s.

But most of us are, like Molly’s parents, terrible risk assessors. Peter Sandman, a self-described “risk communications consultant” in Princeton, New Jersey, made this point in early 2004 after a single case of mad-cow disease in the United States prompted an antibeef frenzy. “The basic reality,” Sandman told the New York Times, “is that the risks that scare people and the risks that kill people are very different.”

Sandman offered a comparison between mad-cow disease (a superscary but exceedingly rare threat) and the spread of food-borne pathogens in the average home kitchen (exceedingly common but somehow not very scary). “Risks that you control are much less a source of outrage than risks that are out of your control,” Sandman said. “In the case of mad-cow, it feels like it’s beyond my control. I can’t tell if my meat has prions in it or not. I can’t see it, I can’t smell it. Whereas dirt in my own kitchen is very much in my own control. I can clean my sponges. I can clean the floor.”

Sandman’s “control” principle might also explain why most people are more scared of flying in an airplane than driving a car. Their thinking goes like this: since I control the car, I am the one keeping myself safe; since I have no control of the airplane, I am at the mercy of myriad external factors.

So which should we actually fear more, flying or driving?

It might first help to ask a more basic question: what, exactly, are we afraid of? Death, presumably. But the fear of death needs to be narrowed down. Of course we all know that we are bound to die, and we might worry about it casually. But if you are told that you have a 10 percent chance of dying within the next year, you might worry a lot more, perhaps even choosing to live your life differently. And if you are told that you have 10 percent chance of dying within the next minute, you’ll probably panic. So it’s the imminent possibility of death that drives the fear—which means that the most sensible way to calculate fear of death would be to think about it on a per-hour basis.

If you are taking a trip and have the choice of driving or flying, you might wish to consider the per-hour death rate of driving versus flying. It is true that many more people die in the United States each year in motor vehicle accidents (roughly forty thousand) than in airplane crashes (fewer than one thousand). But it’s also true that most people spend a lot more time in cars than in airplanes. (More people die even in boating accidents each year than in airplane crashes; as we saw with swimming pools versus guns, water is a lot more dangerous than most people think.) The per-hour death rate of driving versus flying, however, is about equal. The two contraptions are equally likely (or, in truth, unlikely) to lead to death.

But fear best thrives in the present tense. That is why experts rely on it; in a world that is increasingly impatient with long-term processes, fear is a potent short-term play. Imagine that you are a government official charged with procuring the funds to fight one of two proven killers: terrorist attacks and heart disease. Which cause do you think the members of Congress will open up the coffers for? The likelihood of any given person being killed in a terrorist attack is far smaller than the likelihood that the same person will clog up his arteries with fatty food and die of heart disease. But a terrorist attack happens now; death by heart disease is some distant, quiet catastrophe. Terrorist acts lie beyond our control; french fries do not. Just as important as the control factor is what Peter Sandman calls the dread factor. Death by terrorist attack (or mad-cow disease) is considered wholly dreadful; death by heart disease is, for some reason, not.

Sandman is an expert who works both sides of the aisle. One day he might help a group of environmentalists expose a public health hazard. His client the next day could be a fast-food CEO trying to deal with an E. coli outbreak. Sandman has reduced his expertise to a tidy equation: Risk = hazard + outrage. For the CEO with the bad hamburger meat, Sandman engages in “outrage reduction” for the environmentalists, it’s “outrage increase.”

Note that Sandman addresses the outrage but not the hazard itself. He concedes that outrage and hazard do not carry equal weight in his risk equation. “When hazard is high and outrage is low, people underreact,” he says. “And when hazard is low and outrage is high, they overreact.”

So why is a swimming pool less frightening than a gun? The thought of a child being shot through the chest with a neighbor’s gun is gruesome, dramatic, horrifying—in a word, outrageous. Swimming pools do not inspire outrage. This is due in part to the familiarity factor. Just as most people spend more time in cars than in airplanes, most of us have a lot more experience swimming in pools than shooting guns. But it takes only about thirty seconds for a child to drown, and it often happens noiselessly. An infant can drown in water as shallow as a few inches. The steps to prevent drowning, meanwhile, are pretty straightforward: a watchful adult, a fence around the pool, a locked back door so a toddler doesn’t slip outside unnoticed.

If every parent followed these precautions, the lives of perhaps four hundred young children could be saved each year. That would outnumber the lives saved by two of the most widely promoted inventions in recent memory: safer cribs and child car seats. The data show that car seats are, at best, nominally helpful. It is certainly safer to keep a child in the rear seat than sitting on a lap in the front seat, where in the event of an accident he essentially becomes a projectile. But the safety to be gained here is from preventing the kids from riding shotgun, not from strapping them into a $200 car seat. Nevertheless, many parents so magnify the benefit of a car seat that they trek to the local police station or firehouse to have it installed just right. Theirs is a gesture of love, surely, but also a gesture of what might be called obsessive parenting. (Obsessive parents know who they are and are generally proud of the fact; non-obsessive parents also know who the obsessives are and tend to snicker at them.)

Most innovations in the field of child safety are affiliated with—shock of shocks—a new product to be marketed. (Nearly five million car seats are sold each year.) These products are often a response to some growing scare in which, as Peter Sandman might put it, the outrage outweighs the hazard. Compare the four hundred lives that a few swimming pool precautions might save to the number of lives saved by far noisier crusades: child-resistant packaging (an estimated fifty lives a year), flame-retardant pajamas (ten lives), keeping children away from airbags in cars (fewer than five young children a year have been killed by airbags since their introduction), and safety drawstrings on children’s clothing (two lives).

Hold on a minute, you say. What does it matter if parents are manipulated by experts and marketers? Shouldn’t we applaud any effort, regardless of how minor or manipulative, that makes even one child safer? Don’t parents already have enough to worry about? After all, parents are responsible for one of the most awesomely important feats we know: the very shaping of a child’s character. Aren’t they?

The most radical shift of late in the conventional wisdom on parenting has been provoked by one simple question: how much do parents really matter?

Clearly, bad parenting matters a great deal. As the link between abortion and crime makes clear, unwanted children—who are disproportionately subject to neglect and abuse—have worse outcomes than children who were eagerly welcomed by their parents. But how much can those eager parents actually accomplish for their children’s sake?

This question represents a crescendo of decades’ worth of research. A long line of studies, including research into twins who were separated at birth, had already concluded that genes alone are responsible for perhaps 50 percent of a child’s personality and abilities.

So if nature accounts for half of a child’s destiny, what accounts for the other half? Surely it must be the nurturing—the Baby Mozart tapes, the church sermons, the museum trips, the French lessons, the bargaining and hugging and quarreling and punishing that, in toto, constitute the act of parenting. But how then to explain another famous study, the Colorado Adoption Project, which followed the lives of 245 babies put up for adoption and found virtually no correlation between the child’s personality traits and those of his adopted parents? Or the other studies showing that a child’s character wasn’t much affected whether or not he was sent to day care, whether he had one parent or two, whether his mother worked or didn’t, whether he had two mommies or two daddies or one of each?

These nature-nurture discrepancies were addressed in a 1998 book by a little-known textbook author named Judith Rich Harris. The Nurture Assumption was in effect an attack on obsessive parenting, a book so provocative that it required two subtitles: Why Children Turn Out the Way They Do and Parents Matter Less than You Think and Peers Matter More. Harris argued, albeit gently, that parents are wrong to think they contribute so mightily to their child’s personality. This belief, she wrote, was a “cultural myth.” Harris argued that the top-down influence of parents is overwhelmed by the grassroots effect of peer pressure, the blunt force applied each day by friends and schoolmates.

The unlikeliness of Harris’s bombshell—she was a grandmother, no less, without PhD or academic affiliation—prompted both wonder and chagrin. “The public may be forgiven for saying, ‘Here we go again,’” wrote one reviewer. “One year we’re told bonding is the key, the next that it’s birth order. Wait, what really matters is stimulation. The first five years of life are the most important; no, the first three years; no, it’s all over by the first year. Forget that: It’s all genetics!”

But Harris’s theory was duly endorsed by a slate of heavyweights. Among them was Steven Pinker, the cognitive psychologist and bestselling author, who in his own book Blank Slate called Harris’s views “mind-boggling” (in a good way). “Patients in traditional forms of psychotherapy while away their fifty minutes reliving childhood conflicts and learning to blame their unhappiness on how their parents treated them,” Pinker wrote. “Many biographies scavenge through the subject’s childhood for the roots of the grown-up’s tragedies and triumphs. ‘Parenting experts’ make women feel like ogres if they slip out of the house to work or skip a reading of Goodnight Moon. All these deeply held beliefs will have to be rethought.”

Or will they? Parents must matter, you tell yourself. Besides, even if peers exert so much influence on a child, isn’t it the parents who essentially choose a child’s peers? Isn’t that why parents agonize over the right neighborhood, the right school, the right circle of friends?

Still, the question of how much parents matter is a good one. It is also terribly complicated. In determining a parent’s influence, which dimension of the child are we measuring: his personality? his school grades? his moral behavior? his creative abilities? his salary as an adult? And what weight should we assign each of the many inputs that affect a child’s outcome: genes, family environment, socioeconomic level, schooling, discrimination, luck, illness, and so on?

For the sake of argument, let’s consider the story of two boys, one white and one black.

The white boy is raised in a Chicago suburb by parents who read widely and involve themselves in school reform. His father, who has a decent manufacturing job, often takes the boy on nature hikes. His mother is a housewife who will eventually go back to college and earn a bachelor’s degree in education. The boy is happy and performs very well in school. His teachers think he may be a bona fide math genius. His parents encourage him and are terribly proud when he skips a grade. He has an adoring younger brother who is also very bright. The family even holds literary salons in their home.

The black boy is born in Daytona Beach, Florida, and his mother abandons him at the age of two. His father has a good job in sales but is a heavy drinker. He often beats the little boy with the metal end of a garden hose. One night when the boy is eleven, he is decorating a tabletop Christmas tree—the first one he has ever had—when his father starts beating up a lady friend in the kitchen. He hits her so hard that some teeth fly out of her mouth and land at the base of the boy’s Christmas tree, but the boy knows better than to speak up. At school he makes no effort whatsoever. Before long he is selling drugs, mugging suburbanites, carrying a gun. He makes sure to be asleep by the time his father comes home from drinking, and to be out of the house before his father awakes. The father eventually goes to jail for sexual assault. By the age of twelve, the boy is essentially fending for himself.

You don’t have to believe in obsessive parenting to think that the second boy doesn’t stand a chance and that the first boy has it made. What are the odds that the second boy, with the added handicap of racial discrimination, will turn out to lead a productive life? What are the odds that the first boy, so deftly primed for success, will somehow fail? And how much of his fate should each boy attribute to his parents?

One could theorize forever about what makes the perfect parent. For two reasons, the authors of this book will not do so. The first is that neither of us professes to be a parenting expert (although between us we do have six children under the age of five). The second is that we are less persuaded by parenting theory than by what the data have to say.

Certain facets of a child’s outcome—personality, for instance, or creativity—are not easily measured by data. But school performance is. And since most parents would agree that education lies at the core of a child’s formation, it would make sense to begin by examining a telling set of school data.

These data concern school choice, an issue that most people feel strongly about in one direction or another. True believers of school choice argue that their tax dollars buy them the right to send their children to the best school possible. Critics worry that school choice will leave behind the worst students in the worst schools. Still, just about every parent seems to believe that her child will thrive if only he can attend the right school, the one with an appropriate blend of academics, extracurriculars, friendliness, and safety.

School choice came early to the Chicago Public School system. That’s because the CPS, like most urban school districts, had a disproportionate number of minority students. Despite the U.S. Supreme Court’s 1954 ruling in Brown v. Board of Education of Topeka, which dictated that schools be desegregated, many black CPS students continued to attend schools that were nearly all-black. So in 1980 the U.S. Department of Justice and the Chicago Board of Education teamed up to try to better integrate the city’s schools. It was decreed that incoming freshmen could apply to virtually any high school in the district.

Aside from its longevity, there are several reasons the CPS school-choice program is a good one to study. It offers a huge data set—Chicago has the third-largest school system in the country, after New York and Los Angeles—as well as an enormous amount of choice (more than sixty high schools) and flexibility. Its take-up rates are accordingly very high, with roughly half of the CPS students opting out of their neighborhood school. But the most serendipitous aspect of the CPS program—for the sake of a study, at least—is how the school-choice game was played.

As might be expected, throwing open the doors of any school to every freshman in Chicago threatened to create bedlam. The schools with good test scores and high graduation rates would be rabidly oversubscribed, making it impossible to satisfy every student’s request.

In the interest of fairness, the CPS resorted to a lottery. For a researcher, this is a remarkable boon. A behavioral scientist could hardly design a better experiment in his laboratory. Just as the scientist might randomly assign one mouse to a treatment group and another to a control group, the Chicago school board effectively did the same. Imagine two students, statistically identical, each of whom wants to attend a new, better school. Thanks to how the ball bounces in the hopper, one student goes to the new school and the other stays behind. Now imagine multiplying those students by the thousands. The result is a natural experiment on a grand scale. This was hardly the goal in the mind of the Chicago school officials who conceived the lottery. But when viewed in this way, the lottery offers a wonderful means of measuring just how much school choice—or, really, a better school—truly matters.

So what do the data reveal?

The answer will not be heartening to obsessive parents: in this case, school choice barely mattered at all. It is true that the Chicago students who entered the school-choice lottery were more likely to graduate than the students who didn’t—which seems to suggest that school choice does make a difference. But that’s an illusion. The proof is in this comparison: the students who won the lottery and went to a “better” school did no better than equivalent students who lost the lottery and were left behind. That is, a student who opted out of his neighborhood school was more likely to graduate whether or not he actually won the opportunity to go to a new school. What appears to be an advantage gained by going to a new school isn’t connected to the new school at all. What this means is that the students—and parents—who choose to opt out tend to be smarter and more academically motivated to begin with. But statistically, they gained no academic benefit by changing schools.

And is it true that the students left behind in neighborhood schools suffered? No: they continued to test at about the same levels as before the supposed brain drain.

There was, however, one group of students in Chicago who did see a dramatic change: those who entered a technical school or career academy. These students performed substantially better than they did in their old academic settings and graduated at a much higher rate than their past performance would have predicted. So the CPS school-choice program did help prepare a small segment of otherwise struggling students for solid careers by giving them practical skills. But it doesn’t appear that it made anyone much smarter.

Could it really be that school choice doesn’t much matter? No self-respecting parent, obsessive or otherwise, is ready to believe that. But wait: maybe it’s because the CPS study measures high-school students; maybe by then the die has already been cast. “There are too many students who arrive at high school not prepared to do high school work,” Richard P. Mills, the education commissioner of New York State, noted recently, “too many students who arrive at high school reading, writing, and doing math at the elementary level. We have to correct the problem in the earlier grades.”

Indeed, academic studies have substantiated Mills’s anxiety. In examining the income gap between black and white adults—it is well established that blacks earn significantly less—scholars have found that the gap is virtually eradicated if the blacks’ lower eighth-grade test scores are taken into account. In other words, the black-white income gap is largely a product of a black-white education gap that could have been observed many years earlier. “Reducing the black-white test score gap,” wrote the authors of one study, “would do more to promote racial equality than any other strategy that commands broad political support.”

So where does that black-white test gap come from? Many theories have been put forth over the years: poverty, genetic makeup, the “summer setback” phenomenon (blacks are thought to lose more ground than whites when school is out of session), racial bias in testing or in teachers’ perceptions, and a black backlash against “acting white.”

In a paper called “The Economics of ‘Acting White,’” the young black Harvard economist Roland G. Fryer Jr. argues that some black students “have tremendous disincentives to invest in particular behaviors (i.e., education, ballet, etc.) due to the fact that they may be deemed a person who is trying to act like a white person (a.k.a. ‘selling-out’). Such a label, in some neighborhoods, can carry penalties that range from being deemed a social outcast, to being beaten or killed.” Fryer cites the recollections of a young Kareem Abdul-Jabbar, known then as Lew Alcindor, who had just entered the fourth grade in a new school and discovered that he was a better reader than even the seventh graders: “When the kids found this out, I became a target….It was my first time away from home, my first experiencein an all-black situation, and I found myself being punished for everything I’d ever been taught was right. I got all A’s and was hated for it; I spoke correctly and was called a punk. I had to learn a new language simply to be able to deal with the threats. I had good manners and was a good little boy and paid for it with my hide.”

Fryer is also one of the authors of “Understanding the Black-White Test Score Gap in the First Two Years of School.” This paper takes advantage of a new trove of government data that helps reliably address the black-white gap. Perhaps more interestingly, the data do a nice job of answering the question that every parent—black, white, and otherwise—wants to ask: what are the factors that do and do not affect a child’s performance in the early school years?

In the late 1990s, the U.S. Department of Education undertook a monumental project called the Early Childhood Longitudinal Study. The ECLS sought to measure the academic progress of more than twenty thousand children from kindergarten through the fifth grade. The subjects were chosen from across the country to represent an accurate cross section of American schoolchildren.

The ECLS measured the students’ academic performance and gathered typical survey information about each child: his or her race, gender, family structure, socioeconomic status, the level of his or her parents’ education, and so on. But the study went well beyond these basics. It also included interviews with the students’ parents (and teachers and school administrators), posing a long list of questions more intimate than those in the typical government interview: whether the parents spanked their children, and how often; whether they took them to libraries or museums; how much television the children watched.

The result is an incredibly rich set of data—which, if the right questions are asked of it, tells some surprising stories.

How can this type of data be made to tell a reliable story? By subjecting it to the economist’s favorite trick: regression analysis. No, regression analysis is not some forgotten form of psychiatric treatment. It is a powerful—if limited—tool that uses statistical techniques to identify otherwise elusive correlations.

Correlation is nothing more than a statistical term that indicates whether two variables move together. It tends to be cold outside when it snows; those two factors are positively correlated. Sunshine and rain, meanwhile, are negatively correlated. Easy enough—as long as there are only a couple of variables. But with a couple of hundred variables, things get harder. Regression analysis is the tool that enables an economist to sort out these huge piles of data. It does so by artificially holding constant every variable except the two he wishes to focus on, and then showing how those two co-vary.

In a perfect world, an economist could run a controlled experiment just as a physicist or a biologist does: setting up two samples, randomly manipulating one of them, and measuring the effect. But an economist rarely has the luxury of such pure experimentation. (That’s why the school-choice lottery in Chicago was such a happy accident.) What an economist typically has is a data set with a great many variables, none of them randomly generated, some related and others not. From this jumble, he must determine which factors are correlated and which are not.

In the case of the ECLS data, it might help to think of regression analysis as performing the following task: converting each of those twenty thousand schoolchildren into a sort of circuit board with an identical number of switches. Each switch represents a single category of the child’s data: his first-grade math score, his third-grade math score, his first-grade reading score, his third-grade reading score, his mother’s education level, his father’s income, the number of books in his home, the relative affluence of his neighborhood, and so on.

Now a researcher is able to tease some insights from this very complicated set of data. He can line up all the children who share many characteristics—all the circuit boards that have their switches flipped the same direction—and then pinpoint the single characteristic they don’t share. This is how he isolates the true impact of that single switch on the sprawling circuit board. This is how the effect of that switch—and, eventually, of every switch—becomes manifest.

Let’s say that we want to ask the ECLS data a fundamental question about parenting and education: does having a lot of books in your home lead your child to do well in school? Regression analysis can’t quite answer that question, but it can answer a subtly different one: does a child with a lot of books in his home tend to do better than a child with no books? The difference between the first and second questions is the difference between causality (question 1) and correlation (question 2). A regression analysis can demonstrate correlation, but it doesn’t prove cause. After all, there are several ways in which two variables can be correlated. X can cause Y; Y can cause X; or it may be that some other factor is causing both X and Y. A regression alone can’t tell you whether it snows because it’s cold, whether it’s cold because it snows, or if the two just happen to go together.

The ECLS data do show, for instance, that a child with a lot of books in his home tends to test higher than a child with no books. So those factors are correlated, and that’s nice to know. But higher test scores are correlated with many other factors as well. If you simply measure children with a lot of books against children with no books, the answer may not be very meaningful. Perhaps the number of books in a child’s home merely indicates how much money his parents make. What we really want to do is measure two children who are alike in every way except one—in this case, the number of books in their homes—and see if that one factor makes a difference in their school performance.

It should be said that regression analysis is more art than science. (In this regard, it has a great deal in common with parenting itself.) But a skilled practitioner can use it to tell how meaningful a correlation is—and maybe even tell whether that correlation does indicate a causal relationship.

So what does an analysis of the ECLS data tell us about schoolchildren’s performance? A number of things. The first one concerns the black-white test score gap.

It has long been observed that black children, even before they set foot in a classroom, underperform their white counterparts. Moreover, black children didn’t measure up even when controlling for a wide array of variables. (To control for a variable is essentially to eliminate its influence, much as one golfer uses a handicap against another. In the case of an academic study such as the ECLS, a researcher might control for any number of disadvantages that one student might carry when measured against the average student.) But this new data set tells a different story. After controlling for just a few variables—including the income and education level of the child’s parents and the mother’s age at the birth of her first child—the gap between black and white children is virtually eliminated at the time the children enter school.

This is an encouraging finding on two fronts. It means that young black children have continued to make gains relative to their white counterparts. It also means that whatever gap remains can be linked to a handful of readily identifiable factors. The data reveal that black children who perform poorly in school do so not because they are black but because a black child is more likely to come from a low-income, low-education household. A typical black child and white child from the same socioeconomic background, however, have the same abilities in math and reading upon entering kindergarten.

Great news, right? Well, not so fast. First of all, because the average black child is more likely to come from a low-income, low-education household, the gap is very real: on average, black children still are scoring worse. Worse yet, even when the parents’ income and education are controlled for, the black-white gap reappears within just two years of a child’s entering school. By the end of first grade, a black child is underperforming a statistically equivalent white child. And the gap steadily grows over the second and third grades.

Why does this happen? That’s a hard, complicated question. But one answer may lie in the fact that the school attended by the typical black child is not the same school attended by the typical white child, and the typical black child goes to a school that is simply…bad. Even fifty years after Brown v. Board, many American schools are virtually segregated. The ECLS project surveyed roughly one thousand schools, taking samples of twenty children from each. In 35 percent of those schools, not a single black child was included in the sample. The typical white child in the ECLS study attends a school that is only 6 percent black; the typical black child, meanwhile, attends a school that is about 60 percent black.

Just how are the black schools bad? Not, interestingly, in the ways that schools are traditionally measured. In terms of class size, teachers’ education, and computer-to-student ratio, the schools attended by blacks and whites are similar. But the typical black student’s school has a far higher rate of troublesome indicators, such as gang problems, nonstudents loitering in front of the school, and lack of PTA funding. These schools offer an environment that is simply not conducive to learning.

Black students are hardly the only ones who suffer in bad schools. White children in these schools also perform poorly. In fact, there is essentially no black-white test score gap within a bad school in the early years once you control for students’ backgrounds. But all students in a bad school, black and white, do lose ground to students in good schools. Perhaps educators and researchers are wrong to be so hung up on the black-white test score gap; the bad-school/good-school gap may be the more salient issue. Consider this fact: the ECLS data reveal that black students in good schools don’t lose ground to their white counterparts, and black students in good schools outperform whites in poor schools.

So according to these data, a child’s school does seem to have a clear impact on his academic progress, at least in the early years. Can the same be said for parenting? Did all those Baby Mozart tapes pay off? What about those marathon readings of Goodnight Moon? Was the move to the suburbs worthwhile? Do the kids with PTA parents do better than the kids whose parents have never heard of the PTA?

The wide-ranging ECLS data offer a number of compelling correlations between a child’s personal circumstances and his school performance. For instance, once all other factors are controlled for, it is clear that students from rural areas tend to do worse than average. Suburban children, meanwhile, are in the middle of the curve, while urban children tend to score higher than average. (It may be that cities attract a more educated workforce and, therefore, parents with smarter children.) On average, girls test higher than boys, and Asians test higher than whites—although blacks, as we have already established, test similarly to whites from comparable backgrounds and in comparable schools.

Knowing what you now know about regression analysis, conventional wisdom, and the art of parenting, consider the following list of sixteen factors. According to the ECLS data, eight of the factors show a strong correlation—positive or negative—with test scores. The other eight don’t seem to matter. Feel free to guess which are which. Keep in mind that these results reflect only a child’s early test scores, a useful but fairly narrow measurement; poor testing in early childhood isn’t necessarily a great harbinger of future earnings, creativity, or happiness.

The child has highly educated parents.

The child’s family is intact.

The child’s parents have high socioeconomic status.

The child’s parents recently moved into a better neighborhood.

The child’s mother was thirty or older at the time of her first child’s birth.

The child’s mother didn’t work between birth and kindergarten.

The child had low birthweight.

The child attended Head Start.

The child’s parents speak English in the home.

The child’s parents regularly take him to museums.

The child is adopted.

The child is regularly spanked.

The child’s parents are involved in the PTA.

The child frequently watches television.

The child has many books in his home.

The child’s parents read to him nearly every day.

Here now are the eight factors that are strongly correlated with test scores:

The child has highly educated parents.

The child’s parents have high socioeconomic status.

The child’s mother was thirty or older at the time of her first child’s birth.

The child had low birthweight.

The child’s parents speak English in the home.

The child is adopted.

The child’s parents are involved in the PTA.

The child has many books in his home.

And the eight that aren’t:

The child’s family is intact.

The child’s parents recently moved into a better neighborhood.

The child’s mother didn’t work between birth and kindergarten.

The child attended Head Start.

The child’s parents regularly take him to museums.

The child is regularly spanked.

The child frequently watches television.

The child’s parents read to him nearly every day.

Now, two by two:


Matters: The child has highly educated parents.

Doesn’t: The child’s family is intact.

A child whose parents are highly educated typically does well in school; not much surprise there. A family with a lot of schooling tends to value schooling. Perhaps more important, parents with higher IQs tend to get more education, and IQ is strongly hereditary. But whether a child’s family is intact doesn’t seem to matter. Just as the earlier-cited studies show that family structure has little impact on a child’s personality, it does not seem to affect his academic abilities either, at least in the early years. This is not to say that families ought to go around splitting up willy-nilly. It should, however, offer encouragement to the roughly twenty million American schoolchildren being raised by a single parent.


Matters: The child’s parents have high socioeconomic status.

Doesn’t: The child’s parents recently moved into a better neighborhood.

A high socioeconomic status is strongly correlated to higher test scores, which seems sensible. Socioeconomic status is a strong indicator of success in general—it suggests a higher IQ and more education—and successful parents are more likely to have successful children. But moving to a better neighborhood doesn’t improve a child’s chances in school. It may be that moving itself is a disruptive force; more likely, it’s because a nicer house doesn’t improve math or reading scores any more than nicer sneakers make you jump higher.


Matters: The child’s mother was thirty or older at the time of her first child’s birth.

Doesn’t: The child’s mother didn’t work between birth and kindergarten.

A woman who doesn’t have her first child until she is at least thirty is likely to see that child do well in school. This mother tends to be a woman who wanted to get some advanced education or develop traction in her career. She is also likely to want a child more than a teenage mother wants a child. This doesn’t mean that an older first-time mother is necessarily a better mother, but she has put herself—and her children—in a more advantageous position. (It is worth noting that this advantage is nonexistent for a teenage mother who waits until she is thirty to have her second child. The ECLS data show that her second child will perform no better than her first.) At the same time, a mother who stays home from work until her child goes to kindergarten does not seem to provide any advantage. Obsessive parents might find this lack of correlation bothersome—what was the point of all those Mommy and Me classes?—but that is what the data tell us.


Matters: The child had low birthweight.

Doesn’t: The child attended Head Start.

A child who had a low birthweight tends to do poorly in school. It may be that being born prematurely is simply hurtful to a child’s overall well-being. It may also be that low birthweight is a strong forecaster of poor parenting, since a mother who smokes or drinks or otherwise mistreats her baby in utero isn’t likely to turn things around just because the baby is born. A low-birthweight child, in turn, is more likely to be a poor child—and, therefore, more likely to attend Head Start, the federal preschool program. But according to the ECLS data, Head Start does nothing for a child’s future test scores. Despite a deep reservoir of appreciation for Head Start (one of this book’s authors was a charter student), we must acknowledge that it has repeatedly been proven ineffectual in the long term. Here’s a likely reason: instead of spending the day with his own undereducated, overworked mother, the typical Head Start child spends the day with someone else’s undereducated, overworked mother. (And a whole roomful of similarly needy children.) As it happens, fewer than 30 percent of Head Start teachers have even a bachelor’s degree. And the job pays so poorly—about $21,000 for a Head Start teacher versus $40,000 for the average public-school kindergarten teacher—that it is unlikely to attract better teachers any time soon.


Matters: The child’s parents speak English in the home.

Doesn’t: The child’s parents regularly take him to museums.

A child with English-speaking parents does better in school than one whose parents don’t speak English. Again, not much of a surprise. This correlation is further supported by the performance of Hispanic students in the ECLS study. As a group, Hispanic students test poorly; they are also disproportionately likely to have non-English-speaking parents. (They do, however, tend to catch up with their peers in later grades.) So how about the opposite case: what if a mother and father are not only proficient in English but spend their weekends broadening their child’s cultural horizons by taking him to museums? Sorry. Culture cramming may be a foundational belief of obsessive parenting, but the ECLS data show no correlation between museum visits and test scores.


Matters: The child is adopted.

Doesn’t: The child is regularly spanked.

There is a strong correlation—a negative one—between adoption and school test scores. Why? Studies have shown that a child’s academic abilities are far more influenced by the IQs of his biological parents than the IQs of his adoptive parents, and mothers who offer up their children for adoption tend to have significantly lower IQs than the people who are doing the adopting. There is another explanation for low-achieving adoptees which, though it may seem distasteful, jibes with the basic economic theory of self-interest: a woman who knows she will offer her baby for adoption may not take the same prenatal care as a woman who is keeping her baby. (Consider—at the risk of furthering the distasteful thinking—how you treat a car you own versus a car you are renting for the weekend.)

But if an adopted child is prone to lower test scores, a spanked child is not. This may seem surprising—not because spanking itself is necessarily detrimental but because, conventionally speaking, spanking is considered an unenlightened practice. We might therefore assume that parents who spank are unenlightened in other ways. Perhaps that isn’t the case at all. Or perhaps there is a different spanking story to be told. Remember, the ECLS survey included direct interviews with the children’s parents. So a parent would have to sit knee to knee with a government researcher and admit to spanking his child. This would suggest that a parent who does so is either unenlightened or—more interestingly—congenitally honest. It may be that honesty is more important to good parenting than spanking is to bad parenting.


Matters: The child’s parents are involved in the PTA.

Doesn’t: The child frequently watches television.

A child whose parents are involved in the PTA tends to do well in school—which probably indicates that parents with a strong relationship to education get involved in the PTA, not that their PTA involvement somehow makes their children smarter. The ECLS data show no correlation, meanwhile, between a child’s test scores and the amount of television he watches. Despite the conventional wisdom, watching television apparently does not turn a child’s brain to mush. (In Finland, whose education system has been ranked the world’s best, most children do not begin school until age seven but have often learned to read on their own by watching American television with Finnish subtitles.) Nor, however, does using a computer at home turn a child into Einstein: the ECLS data show no correlation between computer use and school test scores.

Now for the final pair of factors:


Matters: The child has many books in his home.

Doesn’t: The child’s parents read to him nearly every day.

As noted earlier, a child with many books in his home has indeed been found to do well on school tests. But regularly reading to a child doesn’t affect early childhood test scores.

This would seem to present a riddle. It bounces us back to our original question: just how much, and in what ways, do parents really matter?

Let’s start with the positive correlation: books in the home equal higher test scores. Most people would look at this correlation and infer an obvious cause-and-effect relationship. To wit: a little boy named Isaiah has a lot of books at home; Isaiah does beautifully on his reading test at school; this must be because his mother or father regularly reads to him. But Isaiah’s friend Emily, who also has a lot of books in her home, practically never touches them. She would rather dress up her Bratz or watch cartoons. And Emily tests just as well as Isaiah. Meanwhile, Isaiah and Emily’s friend Ricky doesn’t have any books at home. But Ricky goes to the library every day with his mother. And yet he does worse on his school tests than either Emily or Isaiah.

What are we to make of this? If reading books doesn’t have an impact on early childhood test scores, could it be that the books’ mere physical presence in the house makes the children smarter? Do books perform some kind of magical osmosis on a child’s brain? If so, one might be tempted to simply deliver a truckload of books to every home that contains a preschooler.

That, in fact, is what the governor of Illinois tried to do. In early 2004, Governor Rod Blagojevich announced a plan to mail one book a month to every child in Illinois from the time they were born until they entered kindergarten. The plan would cost $26 million a year. But, Blagojevich argued, this was a vital intervention in a state where 40 percent of third graders read below their grade level. “When you own [books] and they’re yours,” he said, “and they just come as part of your life, all of that will contribute to a sense…that books should be part of your life.”

So all children born in Illinois would end up with a sixty-volume library by the time they entered school. Does this mean they would all perform better on their reading tests?

Probably not. (Although we may never know for sure: in the end, the Illinois legislature rejected the book plan.) After all, the ECLS data don’t say that books in the house cause high test scores; it says only that the two are correlated.

How should this correlation be interpreted? Here’s a likely theory: most parents who buy a lot of children’s books tend to be smart and well educated to begin with. (And they pass on their smarts and work ethic to their kids.) Or perhaps they care a great deal about education, and about their children in general. (Which means they create an environment that encourages and rewards learning.) Such parents may believe—as fervently as the governor of Illinois believed—that every children’s book is a talisman that leads to unfettered intelligence. But they are probably wrong. A book is in fact less a cause of intelligence than an indicator.

So what does all this have to say about the importance of parents in general? Consider again the eight ECLS factors that are correlated with school test scores:

The child has highly educated parents.

The child’s parents have high socioeconomic status.

The child’s mother was thirty or older at the time of her first child’s birth.

The child had low birthweight.

The child’s parents speak English in the home.

The child is adopted.

The child’s parents are involved in the PTA.

The child has many books in his home.

And the eight factors that are not:

The child’s family is intact.

The child’s parents recently moved into a better neighborhood.

The child’s mother didn’t work between birth and kindergarten.

The child attended Head Start.

The child’s parents regularly take him to museums.

The child is regularly spanked.

The child frequently watches television.

The child’s parents read to him nearly every day.

To overgeneralize a bit, the first list describes things that parents are; the second list describes things that parents do. Parents who are well educated, successful, and healthy tend to have children who test well in school; but it doesn’t seem to much matter whether a child is trotted off to museums or spanked or sent to Head Start or frequently read to or plopped in front of the television.

For parents—and parenting experts—who are obsessed with child-rearing technique, this may be sobering news. The reality is that technique looks to be highly overrated.

But this is not to say that parents don’t matter. Plainly they matter a great deal. Here is the conundrum: by the time most people pick up a parenting book, it is far too late. Most of the things that matter were decided long ago—who you are, whom you married, what kind of life you lead. If you are smart, hardworking, well educated, well paid, and married to someone equally fortunate, then your children are more likely to succeed. (Nor does it hurt, in all likelihood, to be honest, thoughtful, loving, and curious about the world.) But it isn’t so much a matter of what you do as a parent; it’s who you are. In this regard, an overbearing parent is a lot like a political candidate who believes that money wins elections—whereas in truth, all the money in the world can’t get a candidate elected if the voters don’t like him to start with.

In a paper titled “The Nature and Nurture of Economic Outcomes,” the economist Bruce Sacerdote addressed the nature-nurture debate by taking a long-term quantitative look at the effects of parenting. He used three adoption studies, two American and one British, each of them containing in-depth data about the adopted children, their adoptive parents, and their biological parents. Sacerdote found that parents who adopt children are typically smarter, better educated, and more highly paid than the baby’s biological parents. But the adoptive parents’ advantages had little bearing on the child’s school performance. As also seen in the ECLS data, adopted children test relatively poorly in school; any influence the adoptive parents might exert is seemingly outweighed by the force of genetics. But, Sacerdote found, the parents were not powerless forever. By the time the adopted children became adults, they had veered sharply from the destiny that IQ alone might have predicted. Compared to similar children who were not put up for adoption, the adoptees were far more likely to attend college, to have a well-paid job, and to wait until they were out of their teens before getting married. It was the influence of the adoptive parents, Sacerdote concluded, that made the difference.

6

Perfect Parenting, Part II; or: Would a Roshanda by Any Other Name Smell as Sweet?


Obsessive or not, any parent wants to believe that she is making a big difference in the kind of person her child turns out to be. Otherwise, why bother?

The belief in parental power is manifest in the first official act a parent commits: giving the baby a name. As any modern parent knows, the baby-naming industry is booming, as evidenced by a proliferation of books, websites, and baby-name consultants. Many parents seem to believe that a child cannot prosper unless it is hitched to the right name; names are seen to carry great aesthetic or even predictive powers.

This might explain why, in 1958, a New York City man named Robert Lane decided to call his baby son Winner. The Lanes, who lived in a housing project in Harlem, already had several children, each with a fairly typical name. But this boy—well, Robert Lane apparently had a special feeling about this one. Winner Lane: how could he fail with a name like that?

Three years later, the Lanes had another baby boy, their seventh and last child. For reasons that no one can quite pin down today, Robert decided to name this boy Loser. It doesn’t appear that Robert was unhappy about the new baby; he just seemed to get a kick out of the name’s bookend effect. First a Winner, now a Loser. But if Winner Lane could hardly be expected to fail, could Loser Lane possibly succeed?

Loser Lane did in fact succeed. He went to prep school on a scholarship, graduated from Lafayette College in Pennsylvania, and joined the New York Police Department (this was his mother’s longtime wish), where he made detective and, eventually, sergeant. Although he never hid his name, many people were uncomfortable using it. “So I have a bunch of names,” he says today, “from Jimmy to James to whatever they want to call you. Timmy. But they rarely call you Loser.” Once in a while, he said, “they throw a French twist on it: ‘Losier.’” To his police colleagues, he is known as Lou.

And what of his brother with the can’t-miss name? The most noteworthy achievement of Winner Lane, now in his midforties, is the sheer length of his criminal record: nearly three dozen arrests for burglary, domestic violence, trespassing, resisting arrest, and other mayhem.

These days, Loser and Winner barely speak. The father who named them is no longer alive. Clearly he had the right idea—that naming is destiny—but he must have gotten the boys mixed up.

Then there is the recent case of Temptress, a fifteen-year-old girl whose misdeeds landed her in Albany County Family Court in New York. The judge, W. Dennis Duggan, had long taken note of the strange names borne by some offenders. One teenage boy, Amcher, had been named for the first thing his parents saw upon reaching the hospital: the sign for Albany Medical Center Hospital Emergency Room. But Duggan considered Temptress the most outrageous name he had come across.

“I sent her out of the courtroom so I could talk to her mother about why she named her daughter Temptress,” the judge later recalled. “She said she was watching The Cosby Show and liked the young actress. I told her the actress’s name was actually Tempestt Bledsoe. She said she found that out later, that they had misspelled the name. I asked her if she knew what ‘temptress’ meant, and she said she also found that out at some later point. Her daughter was charged with ungovernable behavior, which included bringing men into the home while the mother was at work. I asked the mother if she had ever thought the daughter was living out her name. Most all of this went completely over her head.”

Was Temptress actually “living out her name,” as Judge Duggan saw it? Or would she have wound up in trouble even if her mother had called her Chastity?*

It isn’t much of a stretch to assume that Temptress didn’t have ideal parents. Not only was her mother willing to name her Temptress in the first place, but she wasn’t smart enough to know what that word even meant. Nor is it so surprising, on some level, that a boy named Amcher would end up in family court. People who can’t be bothered to come up with a name for their child aren’t likely to be the best parents either.

So does the name you give your child affect his life? Or is it your life reflected in his name? In either case, what kind of signal does a child’s name send to the world—and most important, does it really matter?

As it happens, Loser and Winner, Temptress and Amcher were all black. Is this fact merely a curiosity or does it have something larger to say about names and culture?

Every generation seems to produce a few marquee academics who advance the thinking on black culture. Roland G. Fryer Jr., the young black economist who analyzed the “acting white” phenomenon and the black-white test score gap, may be among the next. His ascension has been unlikely. An indifferent high-school student from an unstable family, he went to the University of Texas at Arlington on an athletic scholarship. Two things happened to him during college: he quickly realized he would never make the NFL or the NBA; and, taking his studies seriously for the first time in his life, he found he liked them. After graduate work at Penn State and the University of Chicago, he was hired as a Harvard professor at age twenty-five. His reputation for candid thinking on race was already well established.

Fryer’s mission is the study of black underachievement. “One could rattle off all the statistics about blacks not doing so well,” he says. “You can look at the black-white differential in out-of-wedlock births or infant mortality or life expectancy. Blacks are the worst-performing ethnic group on SATs. Blacks earn less than whites. They are still just not doing well, period. I basically want to figure out where blacks went wrong, and I want to devote my life to this.”

In addition to economic and social disparity between blacks and whites, Fryer had become intrigued by the virtual segregation of culture. Blacks and whites watch different television shows. (Monday Night Football is the only show that typically appears on each group’s top ten list; Seinfeld, one of the most popular sitcoms in history, never ranked in the top fifty among blacks.) They smoke different cigarettes. (Newports enjoy a 75 percent market share among black teenagers versus 12 percent among whites; the white teenagers are mainly smoking Marlboros.) And black parents give their children names that are starkly different from white children’s.

Fryer came to wonder: is distinctive black culture a cause of the economic disparity between blacks and whites or merely a reflection of it?

As with the ECLS study, Fryer went looking for the answer in a mountain of data: birth-certificate information for every child born in California since 1961. The data, covering more than sixteen million births, included standard items such as name, gender, race, birth-weight, and the parents’ marital status, as well as more telling factors about the parents: their zip code (which indicates socioeconomic status and a neighborhood’s racial composition), their means of paying the hospital bill (again, an economic indicator), and their level of education.

The California data prove just how dissimilarly black and white parents name their children. White and Asian-American parents, meanwhile, give their children remarkably similar names; there is some disparity between white and Hispanic-American parents, but it is slim compared to the black-white naming gap.

The data also show the black-white gap to be a recent phenomenon. Until the early 1970s, there was a great overlap between black and white names. The typical baby girl born in a black neighborhood in 1970 was given a name that was twice as common among blacks than whites. By 1980 she received a name that was twenty times more common among blacks. (Boys’ names moved in the same direction but less aggressively—probably because parents of all races are less adventurous with boys’ names than with girls’.) Given the location and timing of this change—dense urban areas where Afro-American activism was gathering strength—the most likely cause of the explosion in distinctively black names was the Black Power movement, which sought to accentuate African culture and fight claims of black inferiority. If this naming revolution was indeed inspired by Black Power, it would be one of the movement’s most enduring remnants. Afros today are rare, dashikis even rarer; Black Panther founder Bobby Seale is best known today for peddling a line of barbecue products.

A great many black names today are unique to blacks. More than 40 percent of the black girls born in California in a given year receive a name that not one of the roughly 100,000 baby white girls received that year. Even more remarkably, nearly 30 percent of the black girls are given a name that is unique among the names of every baby, white and black, born that year in California. (There were also 228 babies named Unique during the 1990s alone, and 1 each of Uneek, Uneque, and Uneqqee.) Even among very popular black names, there is little overlap with whites. Of the 626 baby girls named Deja in the 1990s, 591 were black. Of the 454 girls named Precious, 431 were black. Of the 318 Shanices, 310 were black.

What kind of parent is most likely to give a child such a distinctively black name? The data offer a clear answer: an unmarried, low-income, undereducated teenage mother from a black neighborhood who has a distinctively black name herself. In Fryer’s view, giving a child a superblack name is a black parent’s signal of solidarity with the community. “If I start naming my kid Madison,” he says, “you might think, ‘Oh, you want to go live across the railroad tracks, don’t you?’” If black kids who study calculus and ballet are thought to be “acting white,” Fryer says, then mothers who call their babies Shanice are simply “acting black.”

The California study shows that many white parents send as strong a signal in the opposite direction. More than 40 percent of the white babies are given names that are at least four times more common among whites. Consider Connor and Cody, Emily and Abigail. In one recent ten-year stretch, each of these names was given to at least two thousand babies in California—fewer than 2 percent of them black.

So what are the “whitest” names and the “blackest” names?

The Twenty “Whitest” Girl Names


1. Molly

2. Amy

3. Claire

4. Emily

5. Katie

6. Madeline

7. Katelyn

8. Emma

9. Abigail

10. Carly

11. Jenna

12. Heather

13. Katherine

14. Caitlin

15. Kaitlin

16. Holly

17. Allison

18. Kaitlyn

19. Hannah

20. Kathryn

The Twenty “Blackest” Girl Names


1. Imani

2. Ebony

3. Shanice

4. Aaliyah

5. Precious

6. Nia

7. Deja

8. Diamond

9. Asia

10. Aliyah

11. Jada

12. Tierra

13. Tiara

14. Kiara

15. Jazmine

16. Jasmin

17. Jazmin

18. Jasmine

19. Alexus

20. Raven

The Twenty “Whitest” Boy Names


1. Jake

2. Connor

3. Tanner

4. Wyatt

5. Cody

6. Dustin

7. Luke

8. Jack

9. Scott

10. Logan

11. Cole

12. Lucas

13. Bradley

14. Jacob

15. Garrett

16. Dylan

17. Maxwell

18. Hunter

19. Brett

20. Colin

The Twenty “Blackest” Boy Names


1. DeShawn

2. DeAndre

3. Marquis

4. Darnell

5. Terrell

6. Malik

7. Trevon

8. Tyrone

9. Willie

10. Dominique

11. Demetrius

12. Reginald

13. Jamal

14. Maurice

15. Jalen

16. Darius

17. Xavier

18. Terrance

19. Andre

20. Darryl

So how does it matter if you have a very white name or a very black name? Over the years, a series of “audit studies” have tried to measure how people perceive different names. In a typical audit study, a researcher would send two identical (and fake) résumés, one with a traditionally white name and the other with an immigrant or minority-sounding name, to potential employers. The “white” résumés have always gleaned more job interviews.

According to such a study, if DeShawn Williams and Jake Williams sent identical résumés to the same employer, Jake Williams would be more likely to get a callback. The implication is that black-sounding names carry an economic penalty. Such studies are tantalizing but severely limited, for they can’t explain why DeShawn didn’t get the call. Was he rejected because the employer is a racist and is convinced that DeShawn Williams is black? Or did he reject him because “DeShawn” sounds like someone from a low-income, low-education family? A résumé is a fairly undependable set of clues—a recent study showed that more than 50 percent of them contain lies—so “DeShawn” may simply signal a disadvantaged background to an employer who believes that workers from such backgrounds are undependable.

Nor do the black-white audit studies predict what might have happened in a job interview. What if the employer is racist, and if he unwittingly agreed to interview a black person who happened to have a white-sounding name—would he be any more likely to hire the black applicant after meeting face-to-face? Or is the interview a painful and discouraging waste of time for the black applicant—that is, an economic penalty for having a white-sounding name? Along those same lines, perhaps a black person with a white name pays an economic penalty in the black community; and what of the potential advantage to be gained in the black community by having a distinctively black name? But because the audit studies can’t measure the actual life outcomes of the fictitious DeShawn Williams versus Jake Williams, they can’t assess the broader impact of a distinctively black name.

Maybe DeShawn should just change his name.

People do this all the time, of course. The clerks in New York City’s civil court recently reported that name changes are at an all-time high. Some of the changes are purely, if bizarrely, aesthetic. A young couple named Natalie Jeremijenko and Dalton Conley recently renamed their four-year-old son Yo Xing Heyno Augustus Eisner Alexander Weiser Knuckles Jeremijenko-Conley. Some people change names for economic purposes: after a New York livery-cab driver named Michael Goldberg was shot in early 2004, it was reported that Mr. Goldberg was in fact an Indian-born Sikh who thought it advantageous to take a Jewish name upon immigrating to New York. Goldberg’s decision might have puzzled some people in show business circles, where it is a time-honored tradition to change Jewish names. Thus did Issur Danielovitch become Kirk Douglas; thus did the William Morris Agency rise to prominence under its namesake, the former Zelman Moses.

The question is, would Zelman Moses have done as well had he not become William Morris? And would DeShawn Williams do any better if he called himself Jake Williams or Connor Williams? It is tempting to think so—just as it is tempting to think that a truckload of children’s books will make a child smarter.

Though the audit studies can’t be used to truly measure how much a name matters, the California names data can.

How? The California data included not only each baby’s vital statistics but information about the mother’s level of education, income, and, most significantly, her own date of birth. This last fact made it possible to identify the hundreds of thousands of California mothers who had themselves been born in California and then to link them to their own birth records. Now a new and extremely potent story emerged from the data: it was possible to track the life outcome of any individual woman. This is the sort of data chain that researchers dream about, making it possible to identify a set of children who were born under similar circumstances, then locate them again twenty or thirty years later to see how they turned out. Among the hundreds of thousands of such women in the California data, many bore distinctively black names and many others did not. Using regression analysis to control for other factors that might influence life trajectories, it was then possible to measure the impact of a single factor—in this case, a woman’s first name—on her educational, income, and health outcomes.

So does a name matter?

The data show that, on average, a person with a distinctively black name—whether it is a woman named Imani or a man named DeShawn—does have a worse life outcome than a woman named Molly or a man named Jake. But it isn’t the fault of their names. If two black boys, Jake Williams and DeShawn Williams, are born in the same neighborhood and into the same familial and economic circumstances, they would likely have similar life outcomes. But the kind of parents who name their son Jake don’t tend to live in the same neighborhoods or share economic circumstances with the kind of parents who name their son DeShawn. And that’s why, on average, a boy named Jake will tend to earn more money and get more education than a boy named DeShawn. A DeShawn is more likely to have been handicapped by a low-income, low-education, single-parent background. His name is an indicator—not a cause—of his outcome. Just as a child with no books in his home isn’t likely to test well in school, a boy named DeShawn isn’t likely to do as well in life.

And what if DeShawn had changed his name to Jake or Connor: would his situation improve? Here’s a guess: anybody who bothers to change his name in the name of economic success is—like the high-school freshmen in Chicago who entered the school-choice lottery—at least highly motivated, and motivation is probably a stronger indicator of success than, well, a name.

Just as the ECLS data answered questions about parenting that went well beyond the black-white test gap, the California names data tell a lot of stories in addition to the one about distinctively black names. Broadly speaking, the data tell us how parents see themselves—and, more significantly, what kind of expectations they have for their children.

Here’s a question to begin with: where does a name come from, anyway? Not, that is, the actual source of the name—that much is usually obvious: there’s the Bible, there’s the huge cluster of traditional English and Germanic and Italian and French names, there are princess names and hippie names, nostalgic names and place names. Increasingly, there are brand names (Lexus, Armani, Bacardi, Timberland) and what might be called aspirational names. The California data show eight Harvards born during the 1990s (all of them black), fifteen Yales (all white), and eighteen Princetons (all black). There were no Doctors but three Lawyers (all black), nine Judges (eight of them white), three Senators (all white), and two Presidents (both black). Then there are the invented names. Roland G. Fryer Jr., while discussing his names research on a radio show, took a call from a black woman who was upset with the name just given to her baby niece. It was pronounced shuh-TEED but was in fact spelled “Shithead.”*

Shithead has yet to catch on among the masses, but other names do. How does a name migrate through the population, and why? Is it purely a matter of zeitgeist, or is there some sensible explanation? We all know that names rise and fall and rise—witness the return of Sophie and Max from near extinction—but is there a discernible pattern to these movements?

The answer lies in the California data, and the answer is yes.

Among the most interesting revelations in the data is the correlation between a baby’s name and the parents’ socioeconomic status. Consider the most common female names found in middle-income white households versus low-income white households. (These and other lists to follow include data from the 1990s alone, to ensure a large sample that is also current.)

Most Common Middle-Income White Girl Names


1. Sarah

2. Emily

3. Jessica

4. Lauren

5. Ashley

6. Amanda

7. Megan

8. Samantha

9. Hannah

10. Rachel

11. Nicole

12. Taylor

13. Elizabeth

14. Katherine

15. Madison

16. Jennifer

17. Alexandra

18. Brittany

19. Danielle

20. Rebecca

Most Common Low-Income White Girl Names


1. Ashley

2. Jessica

3. Amanda

4. Samantha

5. Brittany

6. Sarah

7. Kayla

8. Amber

9. Megan

10. Taylor

11. Emily

12. Nicole

13. Elizabeth

14. Heather

15. Alyssa

16. Stephanie

17. Jennifer

18. Hannah

19. Courtney

20. Rebecca

There is considerable overlap, to be sure. But keep in mind that these are the most common names of all, and consider the size of the data set. The difference between consecutive positions on these lists may represent several hundred or even several thousand children. So if Brittany is number five on the low-income list and number eighteen on the middle-income list, you can be assured that Brittany is a decidedly low-end name. Other examples are even more pronounced. Five names in each category don’t appear at all in the other category’s top twenty. Here are the top five names among high-end and low-end families, in order of their relative disparity with the other category:

Most Common High-End White Girl Names


1. Alexandra

2. Lauren

3. Katherine

4. Madison

5. Rachel

Most Common Low-End White Girl Names


1. Amber

2. Heather

3. Kayla

4. Stephanie

5. Alyssa

And for the boys:

Most Common High-End White Boy Names


1. Benjamin

2. Samuel

3. Jonathan

4. Alexander

5. Andrew

Most Common Low-End White Boy Names


1. Cody

2. Brandon

3. Anthony

4. Justin

5. Robert

Considering the relationship between income and names, and given the fact that income and education are strongly correlated, it is not surprising to find a similarly strong link between the parents’ level of education and the name they give their baby. Once again drawing from the pool of most common names among white children, here are the top picks of highly educated parents versus those with the least education:

Most Common White Girl Names Among High-Education Parents


1. Katherine

2. Emma

3. Alexandra

4. Julia

5. Rachel

Most Common White Girl Names Among Low-Education Parents


1. Kayla

2. Amber

3. Heather

4. Brittany

5. Brianna

Most Common White Boy Names Among High-Education Parents


1. Benjamin

2. Samuel

3. Alexander

4. John

5. William

Most Common White Boy Names Among Low-Education Parents


1. Cody

2. Travis

3. Brandon

4. Justin

5. Tyler

The effect is even more pronounced when the sample is widened beyond the most common names. Drawing from the entire California database, here are the names that signify the most poorly educated white parents.

The Twenty White Girl Names

That Best Signify Low-Education Parents*

(Average number of years of mother’s education in parentheses)


1. Angel (11.38)

2. Heaven (11.46)

3. Misty (11.61)

4. Destiny (11.66)

5. Brenda (11.71)

6. Tabatha (11.81)

7. Bobbie (11.87)

8. Brandy (11.89)

9. Destinee (11.91)

10. Cindy (11.92)

11. Jazmine (11.94)

12. Shyanne (11.96)

13. Britany (12.05)

14. Mercedes (12.06)

15. Tiffanie (12.08)

16. Ashly (12.11)

17. Tonya (12.13)

18. Crystal (12.15)

19. Brandie (12.16)

20. Brandi (12.17)

If you or someone you love is named Cindy or Brenda and is over, say, forty, and feels that those names did not formerly connote a low-education family, you are right. These names, like many others, have shifted hard and fast of late. Some of the other low-education names are obviously misspellings, whether intentional or not, of more standard names. In most cases the standard spellings of the names—Tabitha, Cheyenne, Tiffany, Brittany, and Jasmine—also signify low education. But the various spellings of even one name can reveal a strong disparity:

Ten “Jasmines” in Ascending Order of Maternal Education

(Years of mother’s education in parentheses)


1. Jazmine (11.94)

2. Jazmyne (12.08)

3. Jazzmin (12.14)

4. Jazzmine (12.16)

5. Jasmyne (12.18)

6. Jasmina (12.50)

7. Jazmyn (12.77)

8. Jasmine (12.88)

9. Jasmin (13.12)

10. Jasmyn (13.23)

Here is the list of low-education white boy names. It includes the occasional misspelling (Micheal and Tylor), but more common is the nickname-as-proper-name trend.

The Twenty White Boy Names

That Best Signify Low-Education Parents*

(Years of mother’s education in parentheses)


1. Ricky (11.55)

2. Joey (11.65)

3. Jessie (11.66)

4. Jimmy (11.66)

5. Billy (11.69)

6. Bobby (11.74)

7. Johnny (11.75)

8. Larry (11.80)

9. Edgar (11.81)

10. Steve (11.84)

11. Tommy (11.89)

12. Tony (11.96)

13. Micheal (11.98)

14. Ronnie (12.03)

15. Randy (12.07)

16. Jerry (12.08)

17. Tylor (12.14)

18. Terry (12.15)

19. Danny (12.17)

20. Harley (12.22)

Now for the names that signify the highest level of parental education. These names don’t have much in common, phonetically or aesthetically, with the low-education names. The girls’ names are in most regards diverse, though with a fair share of literary and otherwise artful touches. A caution to prospective parents who are shopping for a “smart” name: remember that such a name won’t make your child smart; it will, however, give her the same name as other smart kids—at least for a while. (For a much longer and more varied list of girls’ and boys’ names)

The Twenty White Girl Names

That Best Signify High-Education Parents*

(Years of mother’s education in parentheses)


1. Lucienne (16.60)

2. Marie-Claire (16.50)

3. Glynnis (16.40)

4. Adair (16.36)

5. Meira (16.27)

6. Beatrix (16.26)

7. Clementine (16.23)

8. Philippa (16.21)

9. Aviva (16.18)

10. Flannery (16.10)

11. Rotem (16.08)

12. Oona (16.00)

13. Atara (16.00)

14. Linden (15.94)

15. Waverly (15.93)

16. Zofia (15.88)

17. Pascale (15.82)

18. Eleanora (15.80)

19. Elika (15.80)

20. Neeka (15.77)

Now for the boys’ names that are turning up these days in high-education households. This list is particularly heavy on the Hebrew, with a noticeable trend toward Irish traditionalism.

The Twenty White Boy Names

That Best Signify High-Education Parents*

(Years of mother’s education in parentheses)


1. Dov (16.50)

2. Akiva (16.42)

3. Sander (16.29)

4. Yannick (16.20)

5. Sacha (16.18)

6. Guillaume (16.17)

7. Elon (16.16)

8. Ansel (16.14)

9. Yonah (16.14)

10. Tor (16.13)

11. Finnegan (16.13)

12. MacGregor (16.10)

13. Florian (15.94)

14. Zev (15.92)

15. Beckett (15.91)

16. Kia (15.90)

17. Ashkon (15.84)

18. Harper (15.83)

19. Sumner (15.77)

20. Calder (15.75)

If many names on the above lists were unfamiliar to you, don’t feel bad. Even boys’ names—which have always been scarcer than girls’—have been proliferating wildly. This means that even the most popular names today are less popular than they used to be. Consider the ten most popular names given to black baby boys in California in 1990 and then in 2000. The top ten in 1990 includes 3,375 babies (18.7 percent of those born that year), while the top ten in 2000 includes only 2,115 (14.6 percent of those born that year).

Most Popular Black Boy Names

(Number of occurrences in parentheses)


1990

1. Michael (532)

2. Christopher (531)

3. Anthony (395)

4. Brandon (323)

5. James (303)

6. Joshua (301)

7. Robert (276)

8. David (243)

9. Kevin (240)

10. Justin (231)

2000

1. Isaiah (308)

2. Jordan (267)

3. Elijah (262)

4. Michael (235)

5. Joshua (218)

6. Anthony (208)

7. Christopher (169)

8. Jalen (159)

9. Brandon (148)

10. Justin (141)

In the space of ten years, even the most popular name among black baby boys (532 occurrences for Michael) became far less popular (308 occurrences for Isaiah). So parents are plainly getting more diverse with names. But there’s another noteworthy shift in these lists: a very quick rate of turnover. Note that four of the 1990 names (James, Robert, David, and Kevin) fell out of the top ten by 2000. Granted, they made up the bottom half of the 1990 list. But the names that replaced them in 2000 weren’t bottom dwellers. Three of the new names—Isaiah, Jordan, and Elijah—were in fact numbers one, two, and three in 2000. For an even more drastic example of how quickly and thoroughly a name can cycle in and out of use, consider the ten most popular names given to white girls in California in 1960 and then in 2000.

Most Popular White Girl Names


1960

1. Susan

2. Lisa

3. Karen

4. Mary

5. Cynthia

6. Deborah

7. Linda

8. Patricia

9. Debra

10. Sandra

2000

1. Emily

2. Hannah

3. Madison

4. Sarah

5. Samantha

6. Lauren

7. Ashley

8. Emma

9. Taylor

10. Megan

Not a single name from 1960 remains in the top ten. But, you say, it’s hard to stay popular for forty years. So how about comparing today’s most popular names with the top ten from only twenty years earlier?

Most Popular White Girl Names


1980

1. Jennifer

2. Sarah

3. Melissa

4. Jessica

5. Christina

6. Amanda

7. Nicole

8. Michelle

9. Heather

10. Amber

2000

1. Emily

2. Hannah

3. Madison

4. Sarah

5. Samantha

6. Lauren

7. Ashley

8. Emma

9. Taylor

10. Megan

A single holdover: Sarah. So where do these Emilys and Emmas and Laurens all come from? Where on earth did Madison come from?* It’s easy enough to see that new names become very popular very fast—but why?

Let’s take another look at a pair of earlier lists. Here are the most popular names given to baby girls in the 1990s among low-income families and among families of middle income or higher.

Most Common “High-End” White Girl Names in the 1990s


1. Alexandra

2. Lauren

3. Katherine

4. Madison

5. Rachel

Most Common “Low-End” White Girl Names in the 1990s


1. Amber

2. Heather

3. Kayla

4. Stephanie

5. Alyssa

Notice anything? You might want to compare these names with the “Most Popular White Girl Names” list, which includes the top ten overall names from 1980 and 2000. Lauren and Madison, two of the most popular “high-end” names from the 1990s, made the 2000 top ten list. Amber and Heather, meanwhile, two of the overall most popular names from 1980, are now among the “low-end” names.

There is a clear pattern at play: once a name catches on among high-income, highly educated parents, it starts working its way down the socioeconomic ladder. Amber and Heather started out as high-end names, as did Stephanie and Brittany. For every high-end baby named Stephanie or Brittany, another five lower-income girls received those names within ten years.

So where do lower-end families go name-shopping? Many people assume that naming trends are driven by celebrities. But celebrities actually have a weak effect on baby names. As of 2000, the pop star Madonna had sold 130 million records worldwide but hadn’t generated even the ten copycat namings—in California, no less—required to make the master index of four thousand names from which the sprawling list of girls’ names was drawn. Or considering all the Brittanys, Britneys, Brittanis, Brittanies, Brittneys, and Brittnis you encounter these days, you might think of Britney Spears. But she is in fact a symptom, not a cause, of the Brittany/Britney/Brittani/ Brittanie/Brittney/Brittni explosion. With the most common spelling of the name, Brittany, at number eighteen among high-end families and number five among low-end families, it is surely approaching its pull date. Decades earlier, Shirley Temple was similarly a symptom of the Shirley boom, though she is often now remembered as its cause. (It should also be noted that many girls’ names, including Shirley, Carol, Leslie, Hilary, Renee, Stacy, and Tracy began life as boys’ names, but girls’ names almost never cross over to boys.)

So it isn’t famous people who drive the name game. It is the family just a few blocks over, the one with the bigger house and newer car. The kind of families that were the first to call their daughters Amber or Heather and are now calling them Lauren or Madison. The kind of families that used to name their sons Justin or Brandon and are now calling them Alexander or Benjamin. Parents are reluctant to poach a name from someone too near—family members or close friends—but many parents, whether they realize it or not, like the sound of names that sound “successful.”

But as a high-end name is adopted en masse, high-end parents begin to abandon it. Eventually, it is considered so common that even lower-end parents may not want it, whereby it falls out of the rotation entirely. The lower-end parents, meanwhile, go looking for the next name that the upper-end parents have broken in.

So the implication is clear: the parents of all those Alexandras, Laurens, Katherines, Madisons, and Rachels should not expect the cachet to last much longer. Those names are already on their way to overexposure. Where, then, will the new high-end names come from?

It wouldn’t be surprising to find them among the “smartest” girls’ and boys’ names in California, listed on pages 181–82, that are still fairly obscure. Granted, some of them—Oona and Glynnis, Florian and Kia—are bound to remain obscure. The same could be surmised of most of the Hebrew names (Rotem and Zofia, Akiva and Zev), even though many of today’s most mainstream names (David, Jonathan, Samuel, Benjamin, Rachel, Hannah, Sarah, Rebecca) are of course Hebrew biblical names. Aviva may be the one modern Hebrew name that is ready to break out: it’s easy to pronounce, pretty, peppy, and suitably flexible.

Drawn from a pair of “smart” databases, here is a sampling of today’s high-end names. Some of them, as unlikely as it seems, are bound to become tomorrow’s mainstream names. Before you scoff, ask yourself this: do any of them seem more ridiculous than “Madison” might have seemed ten years ago?

Most Popular Girls’ Names of 2015?


Annika

Ansley

Ava

Avery

Aviva

Clementine

Eleanora

Ella

Emma

Fiona

Flannery

Grace

Isabel

Kate

Lara

Linden

Maeve

Marie-Claire

Maya

Philippa

Phoebe

Quinn

Sophie

Waverly

Most Popular Boys’ Names of 2015?


Aidan

Aldo

Anderson

Ansel

Asher

Beckett

Bennett

Carter

Cooper

Finnegan

Harper

Jackson

Johan

Keyon

Liam

Maximilian

McGregor

Oliver

Reagan

Sander

Sumner

Will

Obviously, a variety of motives are at work when parents consider a name for their child. They may want something traditional or something bohemian, something unique or something perfectly trendy. It would be an overstatement to suggest that all parents are looking—whether consciously or not—for a “smart” name or a “high-end” name. But they are all trying to signal something with a name, whether the name is Winner or Loser, Madison or Amber, Shithead or Sander, DeShawn or Jake. What the California names data suggest is that an overwhelming number of parents use a name to signal their own expectations of how successful their children will be. The name isn’t likely to make a shard of difference. But the parents can at least feel better knowing that, from the very outset, they tried their best.

EPILOGUE:

Two Paths to Harvard


And now, with all these pages behind us, an early promise has been confirmed: this book indeed has no “unifying theme.”

But if there is no unifying theme to Freakonomics, there is at least a common thread running through the everyday application of Freakonomics. It has to do with thinking sensibly about how people behave in the real world. All it requires is a novel way of looking, of discerning, of measuring. This isn’t necessarily a difficult task, nor does it require supersophisticated thinking. We have essentially tried to figure out what the typical gang member or sumo wrestler figured out on his own (although we had to do so in reverse).

Will the ability to think such thoughts improve your life materially? Probably not. Perhaps you’ll put up a sturdy gate around your swimming pool or push your real-estate agent to work a little harder. But the net effect is likely to be more subtle than that. You might become more skeptical of the conventional wisdom; you may begin looking for hints as to how things aren’t quite what they seem; perhaps you will seek out some trove of data and sift through it, balancing your intelligence and your intuition to arrive at a glimmering new idea. Some of these ideas might make you uncomfortable, even unpopular. To claim that legalized abortion resulted in a massive drop in crime will inevitably lead to explosive moral reactions. But the fact of the matter is that Freakonomics-style thinking simply doesn’t traffic in morality. As we suggested near the beginning of this book, if morality represents an ideal world, then economics represents the actual world.

The most likely result of having read this book is a simple one: you may find yourself asking a lot of questions. Many of them will lead to nothing. But some will produce answers that are interesting, even surprising. Consider the question posed at the beginning of this book’s penultimate chapter: how much do parents really matter?

The data have by now made it clear that parents matter a great deal in some regards (most of which have been long determined by the time a child is born) and not at all in others (the ones we obsess about). You can’t blame parents for trying to do something—anything—to help their child succeed, even if it’s something as irrelevant as giving him a high-end first name.

But there is also a huge random effect that rains down on even the best parenting efforts. If you are in any way typical, you have known some intelligent and devoted parents whose child went badly off the rails. You may have also known of the opposite instance, where a child succeeds despite his parents’ worst intentions and habits.

Recall for a moment the two boys, one white and one black, who were described in chapter 5. The white boy who grew up outside Chicago had smart, solid, encouraging, loving parents who stressed education and family. The black boy from Daytona Beach was abandoned by his mother, was beaten by his father, and had become a full-fledged gangster by his teens. So what became of the two boys?

The second child, now twenty-eight years old, is Roland G. Fryer Jr., the Harvard economist studying black underachievement.

The white child also made it to Harvard. But soon after, things went badly for him. His name is Ted Kaczynski.

BONUS MATERIAL ADDED TO THE REVISED AND EXPANDED 2006 EDITION

The original New York Times Magazine article about Steven D. Levitt by Stephen J. Dubner, which led to the creation of this book.


Seven “Freakonomics” columns written for the New York Times Magazine, published between August 2005 and April 2006.


Selected entries from the Freakonomics blog, posted between April 2005 and May 2006 at http://www.freakonomics.com/blog/.

THE PROBABILITY THAT A REAL-ESTATE AGENT IS CHEATING YOU (AND OTHER RIDDLES OF MODERN LIFE)

Inside the curious mind of the heralded young economist Steven Levitt

by Stephen J. Dubner

New York Times Magazine, August 3, 2003

The most brilliant young economist in America—the one so deemed, at least, by a jury of his elders—brakes to a stop at a traffic light on Chicago’s south side. It is a sunny day in mid-June. He drives an aging green Chevy Cavalier with a dusty dashboard and a window that doesn’t quite shut, producing a dull roar at highway speeds. But the car is quiet for now, as are the noontime streets: gas stations, boundless concrete, brick buildings with plywood windows.

An elderly homeless man approaches. It says he is homeless right on his sign, which also asks for money. He wears a torn jacket, too heavy for the warm day, and a grimy red baseball cap.

The economist doesn’t lock his doors or inch the car forward. Nor does he go scrounging for spare change. He just watches, as if through one-way glass. After a while, the homeless man moves along.

“He had nice headphones,” says the economist, still watching in the rearview mirror. “Well, nicer than the ones I have. Otherwise, it doesn’t look like he has many assets.”

Steven Levitt tends to see things differently than the average person. Differently, too, than the average economist. This is either a wonderful trait or a troubling one, depending on how you feel about economists. The average economist is known to wax oracularly about any and all monetary issues. But if you were to ask Levitt his opinion of some standard economic matter, he would probably swipe the hair from his eyes and plead ignorance. “I gave up a long time ago pretending that I knew stuff I didn’t know,” he says. “I mean, I just—I just don’t know very much about the field of economics. I’m not good at math, I don’t know a lot of econometrics, and I also don’t know how to do theory. If you ask me about whether the stock market’s going to go up or down, if you ask me whether the economy’s going to grow or shrink, if you ask me whether deflation’s good or bad, if you ask me about taxes—I mean, it would be total fakery if I said I knew anything about any of those things.”

In Levitt’s view, economics is a science with excellent tools for gaining answers but a serious shortage of interesting questions. His particular gift is the ability to ask such questions. For instance: If drug dealers make so much money, why do they still live with their mothers? Which is more dangerous, a gun or a swimming pool? What really caused crime rates to plunge during the past decade? Do real-estate agents have their clients’ best interests at heart? Why do black parents give their children names that may hurt their career prospects? Do schoolteachers cheat to meet high-stakes testing standards? Is sumo wrestling corrupt?

And how does a homeless man afford $50 headphones?

Many people—including a fair number of his peers—might not recognize Levitt’s work as economics at all. But he has merely distilled the so-called dismal science down to its most primal aim: explaining how people get what they want, or need. Unlike most academics, he is unafraid of using personal observations and curiosities (though he does fear calculus). He is an intuitionist. He sifts through a pile of data to find a story that no one else had found. He devises a way to measure an effect that veteran economists had declared unmeasurable. His abiding interests—though he says he has never trafficked in them himself—are cheating, corruption and crime.

His interest in the homeless man’s headphones, meanwhile, didn’t last long. “Maybe,” he said later, “it was just testimony to the fact I’m too disorganized to buy a set of headphones that I myself covet.”

Levitt is the first to say that some of his topics border on the trivial. But he has proved to be such an ingenious researcher and clear-eyed thinker that instead of being consigned to the fringe of his field, the opposite has happened: he has shown other economists just how well their tools can make sense of the real world.

“Levitt is considered a demigod, one of the most creative people in economics and maybe in all social science,” says Colin Camerer, an economist at the California Institute of Technology. “He represents something that everyone thinks they will be when they go to grad school in econ, but usually they have the creative spark bored out of them by endless math—namely, a kind of intellectual detective trying to figure stuff out.”

Levitt is a populist in a field that is undergoing a bout of popularization. Undergraduates are swarming the economics departments of elite universities. Economics is seen as the ideal blend of intellectual prestige (it does offer a Nobel, after all) and practical training for a high-flying finance career (unless, like Levitt, you choose to stay in academia). At the same time, economics is ever more visible in the real world, thanks to the continuing fetishization of the stock market and the continuing fixation with Alan Greenspan.

The greatest change, however, is within the scholarly ranks. Microeconomists are gaining on the macro crowd, empiricists gaining on the theorists. Behavioral economists have called into doubt the very notion of “homo economicus,” the supposedly rational decision-maker in each of us. Young economists of every stripe are more inclined to work on real-world subjects and dip into bordering disciplines—psychology, criminology, sociology, even neurology—with the intent of rescuing their science from its slavish dependence upon mathematical models.

Levitt fits everywhere and nowhere. He is a noetic butterfly that no one has pinned down—he was once offered a job on the Clinton economic team, and the Bush campaign approached him about being a crime adviser—but who is widely appreciated.

“Steve isn’t really a behavioral economist, but they’d be happy to have him,” says Austan Goolsbee, who teaches economics at the University of Chicago’s Graduate School of Business. “He’s not really an old price-theory guy, but these Chicago guys are happy to claim him. He’s not really a Cambridge guy”—although Levitt went to Harvard and then M.I.T.—“but they’d love him to come back.”

He has critics, to be sure. Daniel Hamermesh, a prominent labor economist at the University of Texas, has taught Levitt’s paper “The Impact of Legalized Abortion on Crime” to his undergraduates. “I’ve gone over this paper in draft, in its printed version, at great length, and for the life of me I can’t see anything wrong with it,” Hamermesh says. “On the other hand, I don’t believe a word of it. And his stuff on sumo wrestlers—well, this is not exactly fundamental, unless you’re Japanese and weigh 500 pounds.”

But at thirty-six, Levitt is a full professor in the University of Chicago’s economics department, the most legendary program in the country. (He received tenure after only two years.) He is an editor of the Journal of Political Economy, a leading journal in the field. And the American Economic Association recently awarded him its John Bates Clark Medal, given biennially to the country’s best economist under 40.

He is a prolific and diverse writer. But his paper linking a rise in abortion to a drop in crime has made more noise than the rest combined. Levitt and his co-author, John Donohue of Stanford Law School, argued that as much as 50 percent of the huge drop in crime since the early 1990s can be traced to Roe v. Wade. Their thinking goes like this: the women most likely to seek an abortion—poor, single, black or teenage mothers—were the very women whose children, if born, have been shown most likely to become criminals. But since those children weren’t born, crime began to decrease during the years they would have entered their criminal prime. In conversation, Levitt reduces the theory to a tidy syllogism: “Unwantedness leads to high crime; abortion leads to less unwantedness; abortion leads to less crime.”

Levitt had already published widely about crime and punishment. One paper he wrote as a graduate student is still regularly cited. His question was disarmingly simple: Do more police translate into less crime? The answer would seem obvious—yes—but had never been proved: since the number of police officers tends to rise along with the number of crimes, the effectiveness of the police was tricky to measure.

Levitt needed a mechanism that would unlink the crime rate from police hiring. He found it within politics. He noticed that mayors and governors running for re-election often hire more police officers. By measuring those police increases against crime rates, he was able to determine that additional officers do indeed bring down violent crime.

That paper was later disputed—another graduate student found a serious mathematical mistake in it—but Levitt’s ingenuity was obvious. He began to be acknowledged as a master of the simple, clever solution. He was the guy who, in the slapstick scene, sees all the engineers futzing with a broken machine—and then realizes that no one has thought to plug it in.

Arguing that the police help deter crime didn’t make Levitt any enemies. Arguing that abortion deterred crime was another matter.

In the abortion paper, published in 2001, he and Donohue warned that their findings should not be seen “as either an endorsement of abortion or a call for intervention by the state in the fertility decisions of women.” They suggested that crime might just as easily be curbed by “providing better environments for those children at greatest risk for future crime.”

Still, the very topic managed to offend nearly everyone. Conservatives were enraged that abortion could be construed as a crime-fighting tool. Liberals were aghast that poor and black women were singled out. Economists grumbled that Levitt’s methodology was not sound. A syllogism, after all, can be a magic trick: All cats die; Socrates died; therefore Socrates was a cat.

“I think he’s enormously clever in so many areas, focusing very much on the issue of reverse causality,” says Ted Joyce, an economist at Baruch College who has written a critical response to the abortion paper. “But in this case I think he ignored it, or didn’t tend to it well enough.”

As the news media gorged on the abortion-crime story, Levitt came under direct assault. He was called an ideologue (by conservatives and liberals alike), a eugenicist, a racist and downright evil.

In reality, he seems to be very much none of those. He has little taste for politics and less for moralizing. He is genial, low-key and unflappable, confident but not cocky. He is a respected teacher and colleague; he is a sought-after collaborator who, because of the breadth of his curiosities, often works with scholars outside his field—another rarity for an economist.

“I hesitate to use these words, but Steve is a con man, in the best sense,” says Sudhir Venkatesh, a sociologist at Columbia University. “He’s the Shakespearean jester. He’ll make you believe his ideas were yours.” Venkatesh was Levitt’s co-author on “An Economic Analysis of a Drug-Selling Gang’s Finances,” which found that the average street dealer lives with his mother because the take-home pay is, frankly, terrible. The paper analyzed one crack gang’s financial activities as if it were any corporation. (It was Venkatesh who procured the data, from a former gang member.) Such a thing had never been tried. “This lack of focus,” Levitt deadpanned in one version of the paper, “is perhaps partly attributable to the fact that few economists have been involved in the study of gangs.”

Levitt speaks with a boyish lisp. His appearance is High Nerd: a plaid button-down shirt, nondescript khakis and a braided belt, sensible shoes. His pocket calendar is branded with the National Bureau of Economic Research logo. “I wish he would get more than three haircuts a year,” his wife, Jeannette, says, “and that he wasn’t still wearing the same glasses he got fifteen years ago, which weren’t even in fashion then.” He was a good golfer in high school but has so physically atrophied that he calls himself “the weakest human being alive” and asks Jeannette to open jars around the house. There is nothing in his appearance or manner, in other words, that suggests a flamethrower. He will tell you that all he does is sit at his desk, day and night, wrestling with some strange mountain of data. He will tell you that he would do it for free (his salary is reportedly more than $200,000), and you tend to believe him. He may be an accidental provocateur, but he is a provocateur nonetheless.

He takes particular delight in catching wrongdoers. In one paper, he devised a set of algorithms that could identify teachers in the Chicago public-school system who were cheating. “Cheating classrooms will systematically differ from other classrooms along a number of dimensions,” he and his co-author, Brian Jacob of the Kennedy School of Government, wrote in “Catching Cheating Teachers.” “For instance, students in cheating classrooms are likely to experience unusually large test-score gains in the year of the cheating, followed by unusually small gains or even declines in the following year when the boost attributable to cheating disappears.”

Levitt used test-score data from the Chicago schools that had long been available to other researchers. There were a number of ways, he realized, that a teacher could cheat. If she were particularly brazen (and stupid), she might give students the correct answers. Or, after the test, she might actually erase students’ wrong answers and fill in correct ones. A sophisticated cheater would be careful to avoid conspicuous blocks of identical answers. But Levitt was more sophisticated. “The first step in analyzing suspicious strings is to estimate the probability each child would give a particular answer on each question,” he wrote. “This estimation is done using a multinomial logit framework with past test scores, demographics and socioeconomic characteristics as explanatory variables.”

So by measuring any number of factors—the difficulty of a particular question, the frequency with which students got hard questions right and easy ones wrong, the degree to which certain answers were highly correlated in one classroom—Levitt identified which teachers he thought were cheating. (Perhaps just as valuable, he was also able to identify the good teachers.) The Chicago school system, rather than disputing Levitt’s findings, invited him into the schools for retesting. As a result, the cheaters were fired.

Then there is his forthcoming “Understanding Why Crime Fell in the 1990’s: Four Factors That Explain the Decline and Seven That Do Not.” The entire drop in crime, Levitt says, was due to more police officers, more prisoners, the waning crack epidemic and Roe v. Wade.

One factor that probably didn’t make a difference, he argues, was the innovative policing strategy trumpeted in New York by Rudolph Giuliani and William Bratton. “I think,” Levitt says, “I’m pretty much alone in saying that.”

He comes from a Minneapolis family of high, if unusual, achievers. His father, a medical researcher, is considered a leading authority on intestinal gas. (He bills himself as “The Man Who Gave Status to Flatus and Class to Gas.”) One of Levitt’s great-uncles, Robert May, wrote Rudolph the Red-Nosed Reindeer—the book, that is; another great-uncle, Johnny Marks, later wrote the song.

At Harvard, Levitt wrote his senior thesis on thoroughbred breeding and graduated summa cum laude. (He is still obsessed with horse racing. He says he believes it is corrupt and has designed a betting system—the details of which he will not share—to take advantage of the corruption.) He worked for two years as a management consultant before enrolling at M.I.T. for a doctorate in economics. The M.I.T. program was famous for its mathematical intensity. Levitt had taken exactly one math course as an undergraduate and had forgotten even that. During his first graduate class, he asked the student next to him about a formula on the board: Is there any difference between the derivative sign that’s straight up-and-down and the curly one? “You are in so much trouble,” he was told.

“People wrote him off,” recalls Austan Goolsbee, the Chicago economist who was then a classmate. “They’d say, ‘That guy has no future.’”

Levitt set his own course. Other grad students stayed up all night working on problem sets, trying to make good grades. He stayed up researching and writing. “My view was that the way you succeed in this profession is you write great papers,” he says. “So I just started.”

Sometimes he would begin with a question. Sometimes it was a set of data that caught his eye. He spent one entire summer typing into his computer the results of years’ worth of Congressional elections. (Today, with so much information so easily available on the Internet, Levitt complains that he can’t get his students to input data at all.) All he had was a vague curiosity about why incumbents were so often re-elected.

Then he happened upon a political-science book whose authors claimed that money wins elections, period. “They were trying to explain election outcomes as a function of campaign expenditures,” he recalls, “completely ignoring the fact that contributors will only give money to challengers when they have a realistic chance of winning, and incumbents only spend a lot when they have a chance of losing. They convinced themselves this was the causal story even though it’s so obvious in retrospect that it’s a spurious effect.”

Obvious, at least, to Levitt. Within five minutes, he had a vision of the paper he would write. “It came to me,” he says, “in full bloom.”

The problem was that his data couldn’t tell him who was a good candidate and who wasn’t. It was therefore impossible to tease out the effect of the money. As with the police/crime rate puzzle, he had to trick the data.

Because he himself had typed in the data, he had noticed something: often, the same two candidates faced each other multiple times. By analyzing the data from only those elections, Levitt was able to find a true result. His conclusion: campaign money has about one-tenth the impact as was commonly accepted.

An unknown graduate student, he sent his paper to the Journal of Political Economy—one professor told him he was crazy for even trying—where it was published. He completed his Ph.D. in three years, but because of his priorities, he says, he was “invisible” to the faculty, “a real zero.” Then he stumbled upon what he now calls the turning point in his career.

He had an interview for the Society of Fellows, the venerable intellectual Harvard clubhouse that pays young scholars to do their own work, for three years, with no commitments. Levitt felt he didn’t stand a chance. For starters, he didn’t consider himself an intellectual. He would be interviewed over dinner by the senior fellows, a collection of world-renowned philosophers, scientists and historians. He worried he wouldn’t have enough conversation for even the first course.

Instead, he was on fire. Whatever subject came up—the brain, ants, philosophy—he just happened to remember something pithy he’d read. His wit crackled as it had never crackled before. When he told them about the two summers he spent betting the horses back in Minnesota, they ate it up!

Finally—disquietingly—one of them said: “I’m having a hard time seeing the unifying theme of your work. Could you explain it?”

Levitt was stymied. He had no idea what his unifying theme was, or if he even had one.

Amartya Sen, the future Nobel-winning economist, jumped in and neatly summarized what he saw as Levitt’s theme.

Yes, Levitt said eagerly, that’s my theme.

Another fellow then offered another theme.

You’re right, Levitt said, that’s my theme.

And so it went, like dogs tugging at a bone, until the philosopher Robert Nozick interrupted. If Levitt could have been said to have an intellectual hero, it would be Nozick.

“How old are you, Steve?” he asked.

“Twenty-six.”

Nozick turned to the other fellows: “He’s twenty-six years old. Why does he need to have a unifying theme? Maybe he’s going to be one of those people who’s so talented he doesn’t need one. He’ll take a question and he’ll just answer it, and it’ll be fine.”

The University of Chicago’s economics department had a famous unifying theme—the Gospel of Free Markets, with a conservative twist—and would therefore not have seemed the most likely fit for Levitt. As he sees it, Chicago is about theory, deep thinking and big ideas, while he is about empiricism, clever thinking and “cute but ultimately insubstantial ideas.”

But Chicago also had Gary Becker. To Levitt, Becker is the most influential economist of the past fifty years. Long before it was fashionable, Becker brought microeconomic theory to offbeat topics, the family and crime in particular. For years, Becker was demonized—a single phrase like “the price of children” would set off untold alarms. “I took a lot of heat over my career from people who thought my work was silly or irrelevant or not economics,” Becker says. But Chicago supported him; he persevered, winning the Nobel Prize in 1992; and he became Steven Levitt’s role model.

Becker told Levitt that Chicago would be a great environment for him. “Not everybody agrees with all your results,” he said, “but we agree what you’re doing is very interesting work, and we’ll support you in that.”

Levitt soon found that the support at Chicago went beyond the scholarly. The year after he was hired, his wife gave birth to their first child, Andrew. One day, just after Andrew turned a year old, he came down with a slight fever. The doctor diagnosed an ear infection. When he started vomiting the next morning, his parents took him to the hospital. A few days later he was dead of pneumococcal meningitis.

Amid the shock and grief, Levitt had an undergraduate class that needed teaching. It was Gary Becker—a Nobel laureate nearing his seventieth birthday—who sat in for him. Another colleague, D. Gale Johnson, sent a condolence card that Levitt still quotes from memory.

Levitt and Johnson, an agricultural economist in his eighties, began speaking regularly. Levitt learned that Johnson’s daughter was one of the first Americans to adopt a daughter from China. Soon the Levitts adopted a daughter of their own, whom they named Amanda. In addition to Amanda, they have since had a daughter, now almost three, and a son. But Andrew’s death has played on, in various ways. They have become close friends with the family of the little girl to whom they donated Andrew’s liver. (They also donated his heart, but that baby died.) And, not surprisingly for a scholar who pursues real-life subjects, the death also informed Levitt’s work.

He and Jeannette joined a support group for grieving parents. Levitt was struck by how many children had drowned in swimming pools. They were the kinds of deaths that don’t make the newspaper—unlike, for instance, a child who dies while playing with a gun.

Levitt was curious and went looking for numbers that would tell the story. He wrote up the results as an op-ed article for the Chicago Sun-Times. It featured the sort of plangent counterintuition for which he has become famous: “If you own a gun and have a swimming pool in the yard, the swimming pool is almost 100 times more likely to kill a child than the gun is.”

Trying to get his mind off death, Levitt took up a hobby: rehabbing and selling old houses in Oak Park, where he lives. This experience has led to yet another paper, about the real-estate market. It is his most Chicago-style paper yet, a romp in price theory, a sign that the university’s influence on him is perhaps as strong as his influence on it. But Levitt being Levitt, it also deals with corruption.

While negotiating to buy old houses, he found that the seller’s agent often encouraged him, albeit cagily, to underbid. This seemed odd: didn’t the agent represent the seller’s best interest? Then he thought more about the agent’s role. Like many other “experts” (auto mechanics and stockbrokers come to mind), a real-estate agent is thought to know his field far better than a lay person. A homeowner is encouraged to trust the agent’s information. So if the agent brings in a low offer and says it might just be the best the homeowner can expect, the homeowner tends to believe him. But the key, Levitt determined, lay in the fact that agents “receive only a small share of the incremental profit when a house sells for a higher value.” Like a stockbroker churning commissions or a bookie grabbing his vig, an agent was simply looking to make a deal, any deal. So he would push homeowners to sell too fast and too cheap.

Now if Levitt could only measure this effect. Once again, he found a clever mechanism. Using data from more than 50,000 home sales in Cook County, Ill., he compared the figures for homes owned by real-estate agents with those for homes for which they acted only as agents. The agents’ homes stayed on the market about 10 days longer and sold for 2 percent more.

Late on a summer afternoon, Levitt is in his office, deep inside one of the university’s Gothic behemoths. The ceiling is stained, the plaster around the window crumbling. He is just back from sabbatical at Stanford, and his desk is a holy mess: stacks of books and journals, a green sippy cup and a little orange squeeze hippo.

This is his afternoon to meet with students. Levitt drinks a Mountain Dew and talks softly. Some students come for research assignments, some for advice. One has just written her undergraduate thesis: “The Labor Market Consequence of Graduating College in a Bad Economy.” For a thesis, Levitt tells her, it’s very good. But now she wants to have it published.

“You write like a college student, and that’s a problem,” he says. “The thing is, you’re telling a story. There’s foreshadowing going on, all those tricks. You want the reader going down a particular path so when they get the results, they understand them and believe them. But you also want to be honest about your weaknesses. People are much less harsh on weaknesses that are clear than weaknesses that are hidden—as they should be.”

Be honest about your weaknesses. Has there ever been a prizewinning scholar as honest about his weaknesses as Steven Levitt? He doesn’t understand economics, he claims, or math. He’s a little thinker in a world of big thinkers. He can’t even open a jar of spaghetti sauce at home, poor guy.

Friends say that Levitt’s self-deprecation is as calculated as it is genuine. Within academia, economists take pride in being the most cutthroat of a cutthroat breed. Anyone who writes papers on Weakest Link (contestants discriminate against Latino and elderly peers, Levitt concluded, but not blacks or women) and sumo (to best manage their tournament rankings, wrestlers often conspire to throw matches) had better not also be arrogant.

Or maybe it is not self-deprecation at all. Maybe it is self-flagellation. Maybe what Steven Levitt really wants is to graduate from his “silly” and “trivial” and “shallow” topics.

He thinks he’s onto something with a new paper about black names. He wanted to know if someone with a distinctly black name suffers an economic penalty. His answer—contrary to other recent research—is no. But now he has a bigger question: Is black culture a cause of racial inequality or is it a consequence? For an economist, even for Levitt, this is new turf—“quantifying culture,” he calls it. As a task, he finds it thorny, messy, perhaps impossible and deeply tantalizing.

Driving home to Oak Park that evening, his Cavalier glumly thrumming along the Eisenhower Expressway, he dutifully addresses his future. Leaving academia for a hedge fund or a government job does not interest him (though he might, on the side, start a company to catch cheating teachers). He is said to be at the top of every economics department’s poaching list. But the tree he and Jeannette planted when Andrew died is getting too big to move. You get the feeling he may stay at Chicago awhile.

There are important problems, he says, that he feels ready to address. For instance? “Tax evasion. Money-laundering. I’d like to put together a set of tools that lets us catch terrorists. I mean, that’s the goal. I don’t necessarily know yet how I’d go about it. But given the right data, I have little doubt that I could figure out the answer.”

It might seem absurd for an economist to dream of catching terrorists. Just as it must have seemed absurd if you were a Chicago schoolteacher, called into an office and told that, ahem, the algorithms designed by that skinny man with thick glasses had determined that you are a cheater. And that you are being fired. Steven Levitt may not fully believe in himself, but he does believe in this: teachers and criminals and real-estate agents may lie, and politicians, and even C.I.A. analysts. But numbers don’t.

“Freakonomics” Columns from the New York Times Magazine

UP IN SMOKE

Whatever happened to crack cocaine?

August 7, 2005

If you rely on the news media for your information, you probably think that crack cocaine is a thing of the past. If you rely on data, however, you reach a different conclusion.

Measuring the use and impact of a drug like crack isn’t easy. There is no government Web site to provide crack data, and surveying dealers is bound to be pretty unreliable. So how can you get to the truth of crack use? One way is to look at a variety of imperfect but plausible proxies, including cocaine arrests, emergency-room visits and deaths. Unlike the volume of news coverage, the rates for all of these remain shockingly high. Cocaine arrests, for instance, have fallen only about 15 percent since the crack boom of the late 1980s. Cocaine-related deaths are actually higher now; so are the number of emergency-room visits due to cocaine. When combined in a sensible way, these proxies can be used to construct a useful index of crack.

And what does this index reveal? That crack use was nonexistent until the early 1980s and spiked like mad in 1985, peaking in 1989. That it arrived early on the West Coast, but became most prevalent in the cities of the Northeast and Middle Atlantic States. And that it produced a remarkable level of gun violence, particularly among young black men, who made up the bulk of street-level crack dealers. During the crack boom, the homicide rate among thirteen-to seventeen-year-old blacks more than quadrupled. But perhaps the biggest surprise in the crack index is the fact that, as of 2000—the most recent year for which the index data are available—Americans were still smoking about 70 percent as much crack as they smoked when consumption was at its peak.

If so much crack is still being sold and bought, why aren’t we hearing about it? Because crack-associated violence has largely disappeared. And it was the violence that made crack most relevant to the middle class. What made the violence go away? Simple economics. Urban street gangs were the main distributors of crack cocaine. In the beginning, demand for their product was phenomenal, and so were the potential profits. Most crack killings, it turns out, were not a result of some crackhead sticking up a grandmother for drug money but rather one crack dealer shooting another—and perhaps a few bystanders—in order to gain turf.

But the market changed fast. The destructive effects of the drug became apparent; young people saw the damage that crack inflicted on older users and began to stay away from it. (One recent survey showed that crack use is now three times as common among people in their late thirties as it is among those in their late teens and early twenties.) As demand fell, price wars broke out, driving down profits. And as the amount of money at stake grew smaller and smaller, the violence also dissipated. Young gang members are still selling crack on street corners, but when a corner becomes less valuable, there is less incentive to kill, or be killed, for it.

So how can it be that crack consumption is still so high? Part of the answer may have to do with geography. The index shows that consumption is actually up in states far from the coasts, like Arizona, Minnesota, Colorado and Michigan. But the main answer lies in the same price shift that made the crack trade less violent. The price has fallen about 75 percent from its peak, which has led to an interesting consumption pattern: there are far fewer users, but they are each smoking more crack. This, too, makes perfect economic sense. If you are a devoted crackhead and the price is one-fourth what it used to be, you can afford to smoke four times as much.

But as crack has matured into a drug that causes less social harm, the laws punishing its sale have stayed the same. In 1986, in the national frenzy that followed the death of Len Bias, a first-round N.B.A. draft pick and a cocaine user, Congress passed legislation requiring a five-year mandatory sentence for selling just five grams of crack; you would have to sell 500 grams of powder cocaine to get an equivalent sentence. This disparity has often been called racist, since it disproportionately imprisons blacks.

In fact, the law probably made sense at the time, when a gram of crack did have far more devastating social costs than a gram of powder cocaine. But it doesn’t anymore. Len Bias would now be forty years old, and he would have long outlived his usefulness to the Boston Celtics. It may be time to acknowledge that the law inspired by his death has done the same.

DOES THE TRUTH LIE WITHIN?

One professor’s lifetime of self-experimentation

September 11, 2005

Seth Roberts is a fifty-two-year-old psychology professor at the University of California at Berkeley. If you knew Roberts twenty-five years ago, you might remember him as a man with problems. He had acne, and most days he woke up too early, which left him exhausted. He wasn’t depressed, but he wasn’t always in the best of moods. Most troubling to Roberts, he was overweight: at 5-foot-11, he weighed 200 pounds.

When you encounter Seth Roberts today, he is a clear-skinned, well-rested, entirely affable man who weighs about 160 pounds and looks ten years younger than his age. How did this happen?

It began when Roberts was a graduate student. First he had the clever idea of turning his personal problems into research subjects. Then he decided that he would use his own body as a laboratory. Thus did Roberts embark on one of the longest bouts of scientific self-experimentation known to man—not only poking, prodding and measuring himself more than might be wise but also rigorously recording every data point along the way.

Self-experimentation, though hardly a new idea in the sciences, remains rare. Many modern scientists dismiss it as being not nearly scientific enough: there is no obvious control group, and you can hardly run a double-blind experiment when the researcher and subject are the same person. But might the not-quite-scientific nature of self-experimentation also be a good thing? A great many laboratory-based scientific experiments, especially those in the medical field, are later revealed to have been marred by poor methodology or blatant self-interest. In the case of Roberts, his self-interest is extreme, but at least it is obvious. His methodology is so simple—trying a million solutions until he finds one that works—that it creates the utmost transparency.

In some ways, self-experimentation has more in common with economics than with the hard sciences. Without the ability to run randomized experiments, economists are often left to exploit whatever data they can get hold of. Let’s say you’re an economist trying to measure the effect of imprisonment on crime rates. What you would ideally like to do is have a few randomly chosen states suddenly release 10,000 prisoners, while another few random states lock up an extra 10,000 people. In the absence of such a perfect experiment, you are forced to rely on creative proxies—like lawsuits that charge various states with prison overcrowding, which down the road lead to essentially random releases of large numbers of prisoners. (And yes, crime in those states does rise sharply after the prisoners are released.)

What could be a more opportunistic means of generating data than exploiting your own body? Roberts started small, with his acne, then moved on to his early waking. It took him more than ten years of experimenting, but he found that his morning insomnia could be cured if, on the previous day, he got lots of morning light, skipped breakfast and spent at least eight hours standing.

Stranger yet was the fix he discovered for lifting his mood: at least one hour each morning of TV viewing, specifically life-size talking heads—but never such TV at night. Once he stumbled upon this solution, Roberts, like many scientists, looked back to the Stone Age for explication. Anthropological research suggests that early humans had lots of face-to-face contact every morning but precious little after dark, a pattern that Roberts’s TV viewing now mimicked.

It was also the Stone Age that informed his system of weight control. Over the years, he had tried a sushi diet, a tubular-pasta diet, a five-liters-of-water-a-day diet and various others. They all proved ineffective or too hard or too boring to sustain. He had by now come to embrace the theory that our bodies are regulated by a “set point,” a sort of Stone Age thermostat that sets an optimal weight for each person. This thermostat, however, works the opposite of the one in your home. When your home gets cold, the thermostat turns on the furnace. But according to Roberts’s interpretation of the set-point theory, when food is scarcer, you become less hungry; and you get hungrier when there’s a lot of food around.

This may sound backward, like telling your home’s furnace to run only in the summer. But there is a key difference between home heat and calories: while there is no good way to store the warm air in your home for the next winter, there is a way to store today’s calories for future use. It’s called fat. In this regard, fat is like money: you can earn it today, put it in the bank and withdraw it later when needed.

During an era of scarcity—an era when the next meal depended on a successful hunt, not a successful phone call to Hunan Garden—this set-point system was vital. It allowed you to spend down your fat savings when food was scarce and make deposits when food was plentiful. Roberts was convinced that this system was accompanied by a powerful signaling mechanism: whenever you ate a food that was flavorful (which correlated with a time of abundance) and familiar (which indicated that you had eaten this food before and benefited from it), your body demanded that you bank as many of those calories as possible.

Roberts understood that these signals were learned associations—as dependable as Pavlov’s bell—that once upon a time served humankind well. Today, however, at least in places with constant opportunities to eat, these signals can lead to a big, fat problem: rampant overeating.

So Roberts tried to game this Stone Age system. What if he could keep his thermostat low by sending fewer flavor signals? One obvious solution was a bland diet, but that didn’t interest Roberts. (He is, in fact, a serious foodie.) After a great deal of experimenting, he discovered two agents capable of tricking the set-point system. A few tablespoons of unflavored oil (he used canola or extra light olive oil), swallowed a few times a day between mealtimes, gave his body some calories but didn’t trip the signal to stock up on more. Several ounces of sugar water (he used granulated fructose, which has a lower glycemic index than table sugar) produced the same effect. (Sweetness does not seem to act as a “flavor” in the body’s caloric-signaling system.)

The results were astounding. Roberts lost forty pounds and never gained it back. He could eat pretty much whenever and whatever he wanted, but he was far less hungry than he had ever been. Friends and colleagues tried his diet, usually with similar results. His regimen seems to satisfy a set of requirements that many commercial diets do not: it was easy, built on a scientific theory and, most important, it did not leave Roberts hungry.

In the academic community, Roberts’s self-experimentation has found critics but also serious admirers. Among the latter are the esteemed psychologist Robert Rosenthal, who has praised Roberts for “approaching data in an exploratory spirit more than, or at least in addition to, a confirmatory spirit” and for seeing data analysis “as the opportunity to confront a surprise.” Rosenthal went so far as to envision “a time in the future when ‘self-experimenter’ became a new part-time (or full-time) profession.”

But will Seth Roberts’s strange weight-control solution—he calls it the Shangri-La Diet—really work for the millions of people who need it? We may soon find out. With the Atkins diet company filing for bankruptcy, America is eager for its next diet craze. And a few spoonfuls of sugar may be just the kind of sacrifice that Americans can handle.

CURBING YOUR DOG

Can technology keep New York City scooped?

October 2, 2005

Twenty-five hundred tons. That’s how much manure was produced every day by the 200,000 horses that moved people and goods around New York City in the late nineteenth century. Much of the manure went uncollected, which posed a terrible problem. (This is to say nothing of the horse urine, the deafening clatter of hooves or the carcasses left to rot in the street.) The manure was so widespread and smelly and unsanitary that brownstones were built with their entrances on the second floor so that homeowners might rise above it.

Like so many seemingly overwhelming problems, this one was resolved, quite painlessly, by technology. The electric streetcar and then the automobile led to the disappearance of the horses, and with them went their dung.

Most of the animal dung produced in today’s New York comes from our dogs. (Estimates of the dog population vary widely, but one million is a good guess.) All their poop doesn’t just lie there, of course. In 1978, New York enacted its famous (and widely imitated) “pooper scooper” law, and the city is plainly cleaner, poop-wise, than it was. But with a fine of just $50 for the first offense, the law doesn’t provide much financial incentive to pick up after your dog. Nor does it seem to be vigorously enforced. Let’s pretend that 99 percent of all dog owners do obey the law. That still leaves 10,000 dogs whose poop is left in public spaces each day. Over the last year, the city ticketed only 471 dog-waste violations, which suggests that the typical offender stands a roughly 1-in-8,000 chance of getting a ticket. So here’s a puzzle: why do so many people pick up after their dogs? This would seem to be a case in which social incentives—the hard glare of a passer-by and the offender’s feelings of guilt—are at least as powerful as financial and legal incentives.

If social forces get us most of the way there, how do we deal with the occasional miscreant who fails to scoop? After all, a walk through just about any New York neighborhood confirms that compliance with the law is hardly complete. The Parks Department, meanwhile, which conducts regular cleanliness checks of parks and playgrounds, says that dog poop accounts for 20 percent of its “cleanliness failures.” Dog poop is plainly far less of a nuisance than horse manure ever was. But if you are, say, a parent who walks two kids to school every day and tries to keep all three of you from experiencing that telltale soft smush of a misstep, it is a nuisance nonetheless.

With horses, the solution was simply to eliminate them. Might there be a way to get rid of dog poop without getting rid of the dogs? It might help for a moment to think of a dog as if it were a gun. Using laws to eliminate guns has proved extremely difficult. A given gun lasts a very long time, and as with dogs, guns are widely loved. But getting rid of guns should never have been the point of gun control; the point, rather, ought to be getting rid of the misuse of guns—that is, the use of guns in crimes. Consequently, the most successful policies are those that directly punish misuse, like mandatory prison sentences for any crime involving a gun. In California and elsewhere, such measures have substantially reduced gun crime.

Similarly, the problem in New York is not so much with dogs per se. So perhaps attending to the real problem—their poop—will prompt a solution.

Here’s an idea: DNA sampling. During the licensing procedure, every dog will have to provide a sample of saliva or blood to establish a DNA file. Then, whenever a pile of poop is found on the sidewalk, a sample can be taken to establish the offender’s DNA. (Because stomachs and intestinal walls shed so many cells, poop is in fact a robust DNA source; during a murder trial in Indiana in 2002, the defendant was convicted in large part because the dog poop in his sneaker tread linked him to the scene of the crime.) Once the fecal DNA is matched to a given dog’s DNA file, the dog’s owner will be mailed a ticket. It might cost about $30 million to establish a DNA sample for all the dogs of New York. If people stop violating the law, then New York has spent $30 million for cleaner streets; if not, the $30 million is seed money for a new revenue stream.

Unfortunately, there’s a big drawback to this plan. In order to match a pile of poop with its source, you will need to have every dog’s DNA on file—and in 2003, the most recent year on record, only 102,004 dogs in New York were licensed. Even though a license is legally required, costs a mere $8.50 a year and can be easily obtained by mail, most dog owners ignore the law, and with good reason: last year, only 68 summonses were issued in New York City for unlicensed dogs. So even if the DNA plan were enacted today, most offenders would still go unpunished.

In fact, it stands to reason that the typical licensed dog is less likely to offend than the typical unlicensed dog, since the sort of owner who is responsible enough to license his dog is also most likely responsible enough to clean up after it. How, then, to get all of New York’s dogs licensed? Instead of charging even a nominal fee, the city may want to pay people to license their dogs. And then, instead of treating the licensing law as optional, enforce it for real. Setting up random street checks for dog licenses may offend some New Yorkers, but it certainly dovetails nicely with the Giuliani-era “broken windows” approach to low-level crime.

Before you dismiss the entire dog-DNA idea as idiotic—which, frankly, we were about to do the moment it popped into our heads—consider this: it turns out that civic leaders in Vienna and Dresden have recently floated the same idea. (Indeed, one Vienna politician cited Mayor Giuliani as his inspiration.) Closer to home, an eighth-grade girl in Hoboken, New Jersey, has also proposed the DNA solution.

During a meeting last year of the Hoboken City Council, Lauren Mecka, the daughter of a police captain, argued her dog-poop case. “While adults like yourselves are appalled and disgusted by the sight of the uncollected dog poop that adorns our parks and sidewalks,” she said, “it is children like myself and younger who run the greater risk of contact and exposure. We’re the ones who ride our bikes, throw our balls and roll our blades on the city’s sidewalks. And we’re the ones who have our picnics, stage our adventures and carry out our dragon-slaying fantasies on our parks’ grassy lawns.”

The council, Mecka says today, didn’t seem to take her proposal seriously. Why? “They dismissed it, basically, because I was a twelve-year-old kid.”

WHY VOTE?

There’s no good economic rationale for going to the polls. So what is it that drives the democratic instinct?

November 6, 2005

Within the economics departments at certain universities, there is a famous but probably apocryphal story about two world-class economists who run into each other at the voting booth.

“What are you doing here?” one asks.

“My wife made me come,” the other says.

The first economist gives a confirming nod. “The same.”

After a mutually sheepish moment, one of them hatches a plan: “If you promise never to tell anyone you saw me here, I’ll never tell anyone I saw you.” They shake hands, finish their polling business and scurry off.

Why would an economist be embarrassed to be seen at the voting booth? Because voting exacts a cost—in time, effort, lost productivity—with no discernible payoff except perhaps some vague sense of having done your “civic duty.” As the economist Patricia Funk wrote in a recent paper, “A rational individual should abstain from voting.”

The odds that your vote will actually affect the outcome of a given election are very, very, very slim. This was documented by the economists Casey Mulligan and Charles Hunter, who analyzed more than 56,000 Congressional and state-legislative elections since 1898. For all the attention paid in the media to close elections, it turns out that they are exceedingly rare. The median margin of victory in the Congressional elections was 22 percent; in the state-legislature elections, it was 25 percent. Even in the closest elections, it is almost never the case that a single vote is pivotal. Of the more than 40,000 elections for state legislator that Mulligan and Hunter analyzed, comprising nearly one billion votes, only seven elections were decided by a single vote, with two others tied. Of the more than 16,000 Congressional elections, in which many more people vote, only one election in the past one hundred years—a 1910 race in Buffalo—was decided by a single vote.

But there is a more important point: the closer an election is, the more likely that its outcome will be taken out of the voters’ hands—most vividly exemplified, of course, by the 2000 presidential race. It is true that the outcome of that election came down to a handful of voters; but their names were Kennedy, O’Connor, Rehnquist, Scalia and Thomas. And it was only the votes they cast while wearing their robes that mattered, not the ones they may have cast in their home precincts.

Still, people do continue to vote, in the millions. Why? Here are three possibilities:


Perhaps we are just not very bright and therefore wrongly believe that our votes will affect the outcome.

Perhaps we vote in the same spirit in which we buy lottery tickets. After all, your chances of winning a lottery and of affecting an election are pretty similar. From a financial perspective, playing the lottery is a bad investment. But it’s fun and relatively cheap: for the price of a ticket, you buy the right to fantasize how you’d spend the winnings—much as you get to fantasize that your vote will have some impact on policy.

Perhaps we have been socialized into the voting-as-civic-duty idea, believing that it’s a good thing for society if people vote, even if it’s not particularly good for the individual. And thus we feel guilty for not voting.

But wait a minute, you say. If everyone thought about voting the way economists do, we might have no elections at all. No voter goes to the polls actually believing that her single vote will affect the outcome, does she? And isn’t it cruel to even suggest that her vote is not worth casting?

This is indeed a slippery slope—the seemingly meaningless behavior of an individual, which, in aggregate, becomes quite meaningful. Here’s a similar example in reverse. Imagine that you and your eight-year-old daughter are taking a walk through a botanical garden when she suddenly pulls a bright blossom off a tree.

“You shouldn’t do that,” you find yourself saying.

“Why not?” she asks.

“Well,” you reason, “because if everyone picked one, there wouldn’t be any flowers left at all.”

“Yeah, but everybody isn’t picking them,” she says with a look. “Only me.”

In the old days, there were more pragmatic incentives to vote. Political parties regularly paid voters $5 or $10 to cast the proper ballot; sometimes payment came in the form of a keg of whiskey, a barrel of flour or, in the case of an 1890 New Hampshire Congressional race, a live pig.

Now as then, many people worry about low voter turnout—only slightly more than half of eligible voters participated in the last presidential election—but it might be more worthwhile to stand this problem on its head and instead ask a different question: considering that an individual’s vote almost never matters, why do so many people bother to vote at all?

The answer may lie in Switzerland. That’s where Patricia Funk discovered a wonderful natural experiment that allowed her to take an acute measure of voter behavior.

The Swiss love to vote—on parliamentary elections, on plebiscites, on whatever may arise. But voter participation had begun to slip over the years (maybe they stopped handing out live pigs there too), so a new option was introduced: the mail-in ballot. Whereas each voter in the U.S. must register, that isn’t the case in Switzerland. Every eligible Swiss citizen began to automatically receive a ballot in the mail, which could then be completed and returned by mail.

From a social scientist’s perspective, there was beauty in the setup of this postal voting scheme: because it was introduced in different cantons (the twenty-six statelike districts that make up Switzerland) in different years, it allowed for a sophisticated measurement of its effects over time.

Never again would any Swiss voter have to tromp to the polls during a rainstorm; the cost of casting a ballot had been lowered significantly. An economic model would therefore predict voter turnout to increase substantially. Is that what happened?

Not at all. In fact, voter turnout often decreased, especially in smaller cantons and in the smaller communities within cantons. This finding may have serious implications for advocates of Internet voting—which, it has long been argued, would make voting easier and therefore increase turnout. But the Swiss model indicates that the exact opposite might hold true.

Why is this the case? Why on earth would fewer people vote when the cost of doing so is lowered?

It goes back to the incentives behind voting. If a given citizen doesn’t stand a chance of having her vote affect the outcome, why does she bother? In Switzerland, as in the U.S., “there exists a fairly strong social norm that a good citizen should go to the polls,” Funk writes. “As long as poll-voting was the only option, there was an incentive (or pressure) to go to the polls only to be seen handing in the vote. The motivation could be hope for social esteem, benefits from being perceived as a cooperator or just the avoidance of informal sanctions. Since in small communities, people know each other better and gossip about who fulfills civic duties and who doesn’t, the benefits of norm adherence were particularly high in this type of community.”

In other words, we do vote out of self-interest—a conclusion that will satisfy economists—but not necessarily the same self-interest as indicated by our actual ballot choice. For all the talk of how people “vote their pocketbooks,” the Swiss study suggests that we may be driven to vote less by a financial incentive than a social one. It may be that the most valuable payoff of voting is simply being seen at the polling place by your friends or co-workers.

Unless, of course, you happen to be an economist.

THE ECONOMY OF DESIRE

Can fear of AIDS change sexual preference?

December 11, 2005

What is a price?

Unless you’re an economist, you probably think of a price as simply the amount you pay for a given thing—the number of dollars you surrender for, let’s say, Sunday brunch at your favorite neighborhood restaurant. But to an economist, price is a much broader concept. The 20 minutes you spend waiting for a table is part of the price. So, too, is any nutritional downside of the meal itself: a cheeseburger, as the economist Kevin Murphy has calculated, costs $2.50 more than a salad in long-term health implications. There are moral and social costs to tally as well—for instance, the look of scorn delivered by your vegan dining partner as you order the burger. While the restaurant’s menu may list the price of the cheeseburger at $7.95, that is clearly just the beginning.

The most fundamental rule of economics is that a rise in price leads to less quantity demanded. This holds true for a restaurant meal, a real-estate deal, a college education or just about anything else you can think of. When the price of an item rises, you buy less of it (which is not to say, of course, that you want less of it).

But what about sex? Sex, that most irrational of human pursuits, couldn’t possibly respond to rational price theory, could it?

Outside of a few obvious situations, we generally don’t think about sex in terms of prices. Prostitution is one such situation; courtship is another: certain men seem to consider an expensive dinner a prudent investment in pursuit of a sexual dividend.

But how might price changes affect sexual behavior? And might those changes have something to tell us about the nature of sex itself?

Here is a stark example: A man who is sent to prison finds that the price of sex with a woman has spiked—talk about a supply shortage—and he becomes much more likely to start having sex with men. The reported prevalence of oral sex among affluent American teenagers would also seem to illustrate price theory: because of the possibility of disease or pregnancy, intercourse is expensive—and it has come to be seen by some teenagers as an unwanted and costly pledge of commitment. In this light, oral sex may be viewed as a cheaper alternative.

In recent decades, we have witnessed the most exorbitant new price associated with sex: the H.I.V. virus. Because AIDS is potentially deadly and because it can be spread relatively easily by sex between two men, the onset of AIDS in the early 1980s caused a significant increase in the price of gay sex. Andrew Francis, a graduate student in economics at the University of Chicago, has tried to affix a dollar figure to this change. Setting the value of an American life at $2 million, Francis calculated that in terms of AIDS-related mortality, it cost $1,923.75 in 1992 (the peak of the AIDS crisis) for a man to have unprotected sex once with a random gay American man versus less than $1 with a random woman. While the use of a condom greatly reduces the risk of contracting AIDS, a condom is, of course, yet another cost associated with sex. In a study of Mexican prostitution, the Berkeley economist Paul Gertler and two co-authors showed that when a client requested sex without a condom, a prostitute was typically paid a 24 percent premium over her standard fee.

Francis, in a draft paper titled “The Economics of Sexuality,” tries to go well beyond dollar figures. He puts forth an empirical argument that may fundamentally challenge how people think about sex.

As with any number of behaviors that social scientists try to measure, sex is a tricky subject. But Francis discovered a data set that offered some intriguing possibilities. The National Health and Social Life Survey, sponsored by the U.S. government and a handful of foundations, asked almost 3,500 people a rather astonishing variety of questions about sex: the different sexual acts received and performed and with whom and when; questions about sexual preference and identity; whether they knew anyone with AIDS. As with any self-reported data, there was the chance that the survey wasn’t reliable, but it had been designed to ensure anonymity and generate honest replies.

The survey was conducted in 1992, when the disease was much less treatable than it is today. Francis first looked to see if there was a positive correlation between having a friend with AIDS and expressing a preference for homosexual sex. As he expected, there was. “After all, people pick their friends,” he says, “and homosexuals are more likely to have other homosexuals as friends.”

But you don’t get to pick your family. So Francis next looked for a correlation between having a relative with AIDS and expressing a homosexual preference. This time, for men, the correlation was negative. This didn’t seem to make sense. Many scientists believe that a person’s sexual orientation is determined before birth, a function of genetic fate. If anything, people in the same family should be more likely to share the same orientation. “Then I realized, Oh, my God, they were scared of AIDS,” Francis says.

Francis zeroed in on this subset of about 150 survey respondents who had a relative with AIDS. Because the survey compiled these respondents’ sexual histories as well as their current answers about sex, it allowed Francis to measure, albeit crudely, how their lives may have changed as a result of having seen up close the costly horrors of AIDS.

Here’s what he found: Not a single man in the survey who had a relative with AIDS said he had had sex with a man in the previous five years; not a single man in that group declared himself to be attracted to men or to consider himself homosexual. Women in that group also shunned sex with men. For them, rates of recent sex with women and of declaring homosexual identity and attraction were more than twice as high as those who did not have a relative with AIDS.

Because the sample size was so small—simple chance suggests that no more than a handful of men in a group that size would be attracted to men—it is hard to reach definitive conclusions from the survey data. (Obviously, not every single man changes his sexual behavior or identity when a relative contracts AIDS.) But taken as a whole, the numbers in Francis’s study suggest that there may be a causal effect here—that having a relative with AIDS may change not just sexual behavior but also self-reported identity and desire.

In other words, sexual preference, while perhaps largely predetermined, may also be subject to the forces more typically associated with economics than biology. If this turns out to be true, it would change the way that everyone—scientists, politicians, theologians—thinks about sexuality. But it probably won’t much change the way economists think. To them, it has always been clear: whether we like it or not, everything has its price.

HOODWINKED?

Does it matter if an activist who exposes the inner workings of the Ku Klux Klan isn’t open about how he got those secrets?

January 8, 2006

Our book Freakonomics includes a chapter titled “How Is the Ku Klux Klan Like a Group of Real-Estate Agents?” This chapter was our effort to bring to life the economic concept known as information asymmetry, a state wherein one party to a transaction has better information than another party. It is probably obvious that real-estate agents typically have better information than their clients. The Klan story was perhaps less obvious. We argued that the Klan’s secrecy—its rituals, made-up language, passwords and so on—formed an information asymmetry that furthered its aim of terrorizing blacks and others.

But the Klan was not the hero of our story. The hero was a man named Stetson Kennedy, a white Floridian from an old-line family who from an early age sought to assail racial and social injustices. Out of all of his crusades—for unionism, voting rights and numberless other causes—Kennedy is best known for taking on the Klan in the 1940s. In his book The Klan Unmasked (originally published in 1954 as I Rode with the Ku Klux Klan), Kennedy describes how he adopted a false identity to infiltrate the Klan’s main chapter in Atlanta, was chosen to serve as a “klavalier” (a Klan strong-arm man) and repeatedly found himself at the center of astonishing events, all the while courting great personal risk.

What did Kennedy do with all the secret Klan information he gathered? He disseminated it like mad: to state prosecutors, to human rights groups and even to broadcasters like Drew Pearson and the producers of the Superman radio show, who publicly aired the Klan’s heretofore hidden workings. Kennedy took an information asymmetry and dumped it on its head. And in doing so, we wrote, he played a significant role in quashing the renaissance of the Klan in postwar America.

Kennedy has been duly celebrated for his activism: his friend Woody Guthrie once wrote a song about him, and a Stetson Kennedy Day was recently declared in St. John’s County, Florida, where Kennedy, eighty-nine, still lives. That is where we interviewed him nearly two years ago; our account of his amazing true story was based on those interviews, The Klan Unmasked and a small mountain of history books and newspaper articles.

But is Kennedy’s story as true as it is amazing?

That was the disturbing question that began to haunt another Florida author, Ben Green, who in 1992 began writing a book about Harry T. Moore, a black civil rights advocate who was murdered in1951. For a time, Stetson Kennedy was a collaborator on the book. Although Green was only tangentially interested in Kennedy’s Klan infiltration—it wasn’t central to the Moore story—he eventually checked out Kennedy’s voluminous archives, held in libraries in New York and Atlanta.

These papers charted the extraordinarily colorful life of a man who had been, among other things, a poet, a folklorist, a muckraking journalist and a union activist. But Green was dismayed to find that the story told in Kennedy’s own papers seemed to be quite different from what Kennedy wrote in The Klan Unmasked.

In The Klan Unmasked, Kennedy posed as an encyclopedia salesman named John S. Perkins who, in one of his first undercover maneuvers, visits the former governor of Georgia—a reputed Klan sympathizer—and ingratiates himself by offering to distribute some hate literature. A document in Kennedy’s archives, however, suggests that Kennedy had indeed met the ex-governor, but not in any undercover capacity. Rather, he had interviewed him for a book he was writing—nor did this document mention any hate literature.

A close examination of Kennedy’s archives seems to reveal a recurrent theme: legitimate interviews that he conducted with Klan leaders and sympathizers would reappear in The Klan Unmasked in different contexts and with different facts. In a similar vein, the archives offer evidence that Kennedy covered public Klan events as a reporter but then recast them in his book as undercover exploits. Kennedy had also amassed a great deal of literature about the Klan and other hate groups that he joined, but his own archives suggest that he joined most of these groups by mail.

So did Kennedy personally infiltrate the Klan in Atlanta, as portrayed in The Klan Unmasked?

In his archives are a series of memos that were submitted to the Anti-Defamation League, one of several civil rights groups to which Kennedy reported. Some of the memos were written by him; others were written by a man identified as John Brown, a union worker and former Klan official who had changed his ways and offered to infiltrate the Klan. “This worker is joining the Klan for me,” Kennedy wrote in one memo in early 1946. “I am certain that he can be relied on.”

In Kennedy’s subsequent memos—indeed, in hundreds of pages of Kennedy’s various correspondence from the era—he matter-offactly attributed some of his most powerful Klan information to John Brown: one of the memos he declared “a report from my informant inside the Klan on the meeting of Atlanta Klan No. 1 on August 12 and Atlanta Klan No. 297 on August 15.” As John Brown fed inside information to Kennedy, Kennedy would then relay it to groups like the A.D.L., as well as to prosecutors and journalists. It wasn’t until he wrote The Klan Unmasked, several years later, that Kennedy placed himself, Zelig-like, at the center of all the action.

Ben Green, despite months spent immersed in Kennedy’s archives, could not identify the man once known as John Brown. Green did manage to interview Dan Duke, a former state prosecutor who, as rendered in The Klan Unmasked, worked closely with Kennedy. Duke agreed that Kennedy “got inside of some [Klan] meetings” but openly disputed Kennedy’s dramatized account of their relationship. “None of that happened,” he told Green. In 1999, when Green finally published his Harry T. Moore book, Before His Time, it contained a footnote labeling The Klan Unmasked “a novelization.”

Green is not the only person to have concluded that Kennedy has bent the truth. Jim Clark, who teaches history at the University of Central Florida, says that Kennedy “built a national reputation on many things that didn’t happen.” Meredith Babb, director of the University Press of Florida, which has published four of Kennedy’s books, now calls Kennedy “an entrepreneurial folklorist.” But except for Green’s footnote, they all kept quiet until the retelling of Kennedy’s exploits in Freakonomics produced a new round of attention. Why? “It would be like killing Santa Claus,” Green says. “To me, the saddest part of this story is that what he actually did wasn’t enough for him, and he has felt compelled to make up, embellish or take credit for things he didn’t do.”

When presented with documents from his own archives and asked outright, several weeks ago over lunch near his Florida home, if The Klan Unmasked was “somewhat conflated or fictionalized,” Kennedy said no. “There may have been a bit of dialogue that was not as I remembered it,” he answered. “But beyond that, no.” When pressed, Kennedy did concede that “in some cases I took the reports and actions of this other guy and incorporated them into one narrative.” As it turns out, Kennedy has made such an admission at least once before. Peggy Bulger, director of the American Folklife Center in the Library of Congress, wrote a 1992 dissertation called “Stetson Kennedy: Applied Folklore and Cultural Advocacy,” based in part on extensive interviews with her subject. In an endnote, Bulger writes that “Kennedy combined his personal experiences undercover with the narratives provided by John Brown in writing ‘I Rode with the Ku Klux Klan’ in 1954.”

We weren’t very happy, of course, to learn that a story we included in Freakonomics was built on such shaky foundations—especially since the book is devoted to upending conventional wisdoms rather than reinforcing them, and concerning Stetson Kennedy, the most conventional wisdom of all is his reputation as a Klan infiltrator.

There is also the fact that in our work we make a point of depending less on anecdote in favor of data, the idea being that numbers tend to lie less baldly than people do. But the story of Stetson Kennedy was one long series of anecdotes—which, no matter how many times they were cited over the decades, were nearly all generated by the same self-interested source.

Perhaps Kennedy’s long life of fighting the good fight is all that matters. Perhaps, to borrow Peggy Bulger’s phraseology, a goal of “cultural advocacy” calls for the use of “applied folklore” rather than the sort of forthrightness that should be more typical of history or journalism. One thing that does remain true is that Kennedy was certainly a master of information asymmetry. Until, that is, the data caught up with him.

FILLING IN THE TAX GAP

Why Americans should be clamoring for the I.R.S. to do more audits, not fewer

April 2, 2006

This is the time of year when American citizens inevitably think about the Internal Revenue Service and, also inevitably, about how deeply they hate it. But most people who hate the I.R.S. probably do so for the wrong reasons. They think it is a tough and cruel agency, but in fact it is not nearly as tough and cruel as it should be.

The first thing to remember is that the I.R.S. doesn’t write the tax code. The agency is quick to point its finger at the true villain: “In the United States, the Congress passes tax laws and requires taxpayers to comply,” its mission statement says. “The I.R.S. role is to help the large majority of compliant taxpayers with the tax law, while ensuring that the minority who are unwilling to comply pay their fair share.”

So the I.R.S. is like a street cop or, more precisely, the biggest fleet of street cops in the world, who are asked to enforce laws written by a few hundred people on behalf of a few hundred million people, a great many of whom find these laws too complex, too expensive and unfair.

And yet most Americans say they are proud to pay their taxes. In an independent poll conducted last year for the I.R.S. Oversight Board, 96 percent of the respondents agreed with the statement “It is every American’s civic duty to pay their fair share of taxes,” while 93 percent agreed that everyone “who cheats on their taxes should be held accountable.” On the other hand, when asked what influences their decision to report and pay taxes honestly, 62 percent answered “fear of an audit,” while 68 percent said it was the fact that their income was already being reported to the I.R.S. by third parties. For all the civic duty floating around, it would seem that most compliance is determined by good old-fashioned incentives.

So which of these incentives work and which do not? To find out, the I.R.S. conducted the National Research Program, a three-year study during which 46,000 randomly selected 2001 tax returns were intensively reviewed. (The I.R.S. doesn’t specify what these 46,000 people were subjected to, but it may well have been the kind of inquisition that has earned the agency its horrid reputation.) Using this sample, the study found a tax gap—the difference between taxes owed and taxes actually paid—of $345 billion, or nearly one-fifth of all taxes collected by the I.R.S. This sum happens to be just a few billion dollars less than the projected federal budget deficit for 2007; it also amounts to more than $1,000 worth of cheating by every man, woman and child in the United States.

But most people aren’t cheating. And when you take a look at who does cheat and who doesn’t, it becomes pretty clear just why people pay their taxes at all. The key statistic in the I.R.S.’s study is called the Net Misreporting Percentage. It measures the amount that was misreported on every major line item on those 46,000 returns. In the “wages, salaries, tips” category, for instance, Americans are underreporting only 1 percent of their actual income. Meanwhile, in the “nonfarm proprietor income” category—think of self-employed workers like a restaurateur or the boss of a small construction crew—57 percent of the income goes unreported. That’s $68 billion in unpaid taxes right there.

Why such a huge difference between the wage earner and a restaurateur? Simple: The only person reporting the restaurateur’s income to the I.R.S. is the restaurateur himself; for the wage earner, his employer is generating a W2 to let the I.R.S. know exactly how much he has been paid. And the wage earner’s taxes are automatically withheld from his every check, while the restaurateur has all year to decide if, and how much, he will pay.

Does this mean that the average self-employed worker is less honest than the average wage earner? Not necessarily. It’s just that he has much more incentive to cheat. He knows that the only chance the I.R.S. has of learning his true income and expenditures is to audit him. And all he has to do is look at the I.R.S.’s infinitesimal audit rate—last year, the agency conducted face-to-face audits on just 0.19 percent of all individual taxpayers—to feel pretty confident to go ahead and cheat.

So why do people really pay their taxes: because it is the right thing to do, or because they fear getting caught if they don’t? It sure seems to be the latter. A combination of good technology (employer reporting and withholding) and poor logic (most people who don’t cheat radically overestimate their chances of being audited) makes the system work. And while it sounds bad to hear that Americans underpay their taxes by nearly one-fifth, the tax economist Joel Slemrod estimates that the U.S. is easily within the upper tier of worldwide compliance rates.

Still, unless you are personally cheating by one-fifth or more, you should be mad at the I.R.S.—not because it’s too vigilant, but because it’s not nearly vigilant enough. Why should you pay your fair share when the agency lets a few hundred billion dollars of other people’s money go uncollected every year?

The I.R.S. itself would love to change this dynamic. In the past few years, it has significantly increased its enforcement revenue and its audit rate, despite a budget that is only fractionally larger. A main task of any I.R.S. commissioner (the current one is Mark Everson) is to beg Congress and the White House for resources. For all the obvious appeal of having the I.R.S. collect every dollar owed to the government, it is just as obviously unappealing for most politicians to advocate a more vigorous I.R.S. Michael Dukakis tried this during his 1988 presidential campaign, and—well, it didn’t work.

Left to enforce a tax code no one likes upon a public that knows it can practically cheat at will, the I.R.S. does its best to fiddle around the edges. Once in a while, it hits pay dirt.

In the early 1980s, an I.R.S. research officer in Washington named John Szilagyi had seen enough random audits to know that some taxpayers were incorrectly claiming dependents for the sake of an exemption. Sometimes it was a genuine mistake (a divorced wife and husband making duplicate claims on their children), and sometimes the claims were comically fraudulent (Szilagyi recalls at least one dependent’s name listed as Fluffy, who was quite obviously a pet rather than a child).

Szilagyi decided that the most efficient way to clean up this mess was to simply require taxpayers to list their children’s Social Security numbers. “Initially, there was a lot of resistance to the idea,” says Szilagyi, now 66 and retired to Florida. “The answer I got was that it was too much like ‘1984.’” The idea never made its way out of the agency.

A few years later, however, with Congress clamoring for more tax revenue, Szilagyi’s idea was dug up, rushed forward and put into law for tax year 1986. When the returns started coming in the following April, Szilagyi recalls, he and his bosses were shocked: seven million dependents had suddenly vanished from the tax rolls, some incalculable combination of real pets and phantom children. Szilagyi’s clever twist generated nearly $3 billion in revenues in a single year.

Szilagyi’s immediate bosses felt he should get some kind of reward for his idea, but their superiors weren’t convinced. So Szilagyi called his congressman, who got the reward process back on track. Finally, five years after his brainstorm became the law, Szilagyi, who earned about $80,000 annually at the time, was given a check for $25,000. By this point, his idea had generated roughly $14 billion.

Which suggests at least one legitimate reason to dislike the I.R.S.: if the agency hadn’t been so stingy with Szilagyi’s reward back then, it probably would have attracted a lot more of the anti-cheating wizards it really needs today.

From the Freakonomics Blog


The following excerpts are inevitably pockmarked with incomplete thoughts (at the very least), since blog writing is by nature more impetuous, more colloquial, even more random than what one would write in a book or a newspaper. But hopefully such casual discourse provides its own sort of value. The excerpts here have been slightly edited, mostly to compensate for the fact that, unlike a website, a book that is printed on paper, cannot (yet) allow you to click here to read further. The excerpts are divided into four categories:


Ruminations on Freakonomics itself, and its aftermath

A continuation of the abortion/crime discussion presented in Freakonomics

Random reflections on random subjects, most of them related to Freakonomics in some loose way—in the way, perhaps, that “kosher style” food isn’t quite kosher but also isn’t shrimp

Rants and raves of a more personal nature

These postings represent perhaps 3 percent of what we’ve written on our blog since it began, and we haven’t included any readers’ comments, which are often far more involved (and entertaining) than our own posts. The entire blog can be found at www.freakonomics.com/ blog/.

Another major difference between the blog and our book is that all but the first two excerpts that follow were written by one of us, not both of us, and are accordingly notated with a signoff of either “SDL” (Levitt) or “SJD” (Dubner).

1. ON FREAKONOMICS ITSELF

A brief compendium of thoughts about how the book was written, published, and received.

“Unleashing Our Baby”

Every parent thinks he has the most beautiful baby in the world. Evolution, it seems, has molded our brains so that if you stare at your own baby’s face day after day after day, it starts to look beautiful. When other people’s children have food clotted on their faces, it looks disgusting; with your own kid, it’s somehow endearing.

Well, we’ve been staring at the Freakonomics manuscript so much that it now looks beautiful to us—warts, clotted food, and all. So we started to think that maybe some people would actually want to read it, and after reading it, might even want to express their opinions about it. Thus, the birth of this website. We hope it’s a happy (or at least happily contentious) home for some time to come.

—SDL & SJD (March 30, 2005)

“Does Freakonomics Suck?”

Our publisher has been busily promoting and selling Freakonomics—which, of course, is its job, and which we, not surprisingly, applaud. When something good happens—a nice review in the Wall Street Journal, for instance, or an upcoming appearance on The Daily Show with Jon Stewart—the publisher assiduously spreads the word. But we think it’s worth considering some alternative views. That, after all, is the spirit of Freakonomics—examining the data, whatever it may be, and following it through, wherever that may lead. So here are some people who think that Freakonomics is, in part or in sum, a big fat stink bomb:

Felix Salmon, a journalist and blogger, wrote a lengthy and exasperated review calling Freakonomics “a series of disjointed chapters” in which “Levitt and Dubner like to get holier-than-thou” and “lap up the conventional wisdom” Steve Sailer, who has vigorously argued against the link between Roe v. Wade and falling crime (a Google search of “Sailer” and “Freakonomics” will turn up a wide variety of comments); a Newsday review (Apr. 24, 2005), by Scott McLemee, which chided the book’s “style of evasive lucidity” a review in Time magazine (May 2, 2005), by Amanda Ripley, who writes that the “unfortunately titled Freakonomics” has “no unifying theory … which is a shame.” In fairness to ourselves, we should note that both the Time and Newsday reviews were largely positive. But we should also note that one well-known American writer of non-fiction, when sent an early copy of Freakonomics for a blurb, refused to endorse it on the grounds that “the one thing missing from the section on crime is a sense of humility.”

Do these comments make us unhappy? On a personal level, sure. But on a Freakonomics level, no. Years ago, the Harvard law professor Alan Dershowitz opened a kosher deli in Harvard Square, which came under protest on various grounds. Dershowitz, known as much for his embrace of free speech as his legal acumen, said—and here we are paraphrasing loosely at best—that nothing was more precious to him than the right of people to protest his deli.

So please don’t take our word that Freakonomics is a good book. Don’t believe the good reviews either. Feel free to make up your own mind—you can poke around a good bit here, on this very website. Maybe you will decide that Freakonomics is, after all, a piece of trash. We cherish your right to think so.

—SDL & SJD (April 26, 2005)

“A Freakonomics Roundtable”

There has been a lot written about Freakonomics, but in terms of thoughtfulness, nothing matches the collection of essays assembled at the blog Crooked Timber (http://crookedtimber.org/2005/05/23/ steven-levitt-seminar-introduction/). There you will find five discussions of Freakonomics done by academics from a range of disciplines, along with my response to these essays.

I’ve also cut and pasted my response here, which basically makes sense even if you haven’t read the original essays.

Let’s start with the title. Freakonomics. We debated endlessly over the title. From a naming perspective, the difficulty with this book is that it doesn’t have a theme. We thought about a question title (“What Do Sumo Wrestlers and Schoolteachers Have in Common?”), some non-threatening titles (“The Hidden Side of Everything” or “Ain’t Necessarily So”), and some loopy titles (“E-Ray Vision,” with the “E” standing for economics).

In the end, though, Freakonomics became the obvious choice, for reasons anchored in the contrast between my own research on first names and that of others. Let’s just assume that my research is right and it is really true that a name on a résumé does matter for getting a job callback, but not for long-term life outcomes. This probably implies that names matter a little for first impressions, but then quickly get swept aside in importance once we gain some familiarity. When’s the last time you thought to yourself, Oprah is a ridiculous name, I certainly won’t watch her show? Or, The Beatles…what a ridiculous name for a band. No one would ever buy their records.

In naming a book, you need something attention-grabbing to cut through the clutter of the thousands of competing books, but as shocking as Freakonomics sounds the first time you hear it, by the twentieth time it becomes familiar, like Oprah. My guess is that the Crooked Timber commenters were already softening their hatred for the title by the time they finished writing. And a year from now, they may even forget that they ever hated the title. At least, that is what happened with our publisher, which initially dismissed the title out of hand, only allowed it at the eleventh hour, and now are telling us we need to sign up with them for a second book because no one else can market our books as well as they do. And if there is a second book, we have a title in mind that is so outrageous it will have to be loved.

So how about the absence of a unifying theme in the book? My own hunch, borne out by the public response to this book, is that nobody really cares about or even wants a unifying theme in a book. Everyone is just afraid not to have one, since almost all books do. (In this respect, I think unifying themes in books are a lot like campaign spending: all candidates feel compelled to spend a lot of money for fear of the disastrous consequences that could result if they take a chance and don’t spend, spend, spend.) But when I read Malcolm Gladwell’s incredible books, I don’t care about the theme, I just love his stories. His books top the charts because he has really good taste and he is the best storyteller going. For me, and others I talk to, the unifying themes sometimes get in the way of his stories which are individually so amazingly interesting. Books of short stories, similarly, have no unifying theme. I certainly don’t feel cheated by that either. More valuable than anything else I or Dubner ever does, perhaps, would be to make the world safe for books that have great stories but no unifying theme.

All of the Crooked Timber commentaries spent some time discussing where I fit into economics and the social sciences more broadly. If I got to make three wishes, perhaps one of them would be that I might turn into a truly interdisciplinary social scientist who uses data to inform human behavior in ways that both shed light on and draw upon not only economics, but sociology, political science, and psychology as well. But let’s be realistic. I’m having trouble even mastering the tools of my own discipline. If you ask my students whether I know calculus, they will say “not very well.” I’m not proud of that fact, but I am a realist. If you ask the really great economic thinkers like Gary Becker or Kevin Murphy how often I’m right when I try to apply Chicago price theory, they will simply tell you that I am showing a lot of improvement, because they are kind. The only things I’m good at, really and honestly, are asking questions that people seem to find interesting, and figuring out how to trick data into answering those questions. I will never be even a passable sociologist, political scientist, or psychologist. But that is okay. I think the thing that gets a lot of economists into trouble is the false belief that they can be good at everything.

A few years back, when I was on sabbatical at the Center for Advanced Study of Behavioral Sciences at Stanford, I gave a talk to the other fellows on my research. Some in the audience were indignant, asking why I called myself an economist given what I did. They said I was really a sociologist. One only had to look at the horror on the faces of the sociologists in the room to see that I wasn’t a sociologist either. But by starting from the position that I don’t know much, I am open-minded enough to co-author with an ethnographer (Sudhir Venkatesh), an econometrician ( Jack Porter), a political scientist (Tim Groseclose), and now a journalist (Stephen Dubner). And maybe, in addition to making it safe in the future for someone to publish a book without a theme, I will make it easier for academics from all social sciences to follow the sort of “adisciplinary” (as opposed to interdisciplinary) path I’m on.

Next, there is the question of incentives. In the same way that “utility maximization” can be turned into a tautology, the commenters point out that our use of the term “incentives” is moving in that direction as well. By widening incentives, as we did in Freakonomics, to encompass not only financial but also social and moral incentives, we have covered just about everything. Still, I think there isn’t really another choice. To focus just on financial incentives would obviously be misguided. On the flip side, for me—and I think this is the thing that makes me an economist ultimately—I just can’t get away from the idea that people are active decision makers trying to get what they want in a reasonably sophisticated fashion. The most real sense in which I think incentives are the unifying theme of my research (even in cases where they aren’t obviously present, as in the abortion-crime stuff ) is that whenever I try to answer a question, I put myself in the shoes of the actors and I ask myself, “What would I do if I were in that situation?” I am the kind of person who is always trying to concoct some scheme to beat the system or avoid getting scammed, so I presume the people I’m studying are thinking the same way. So when I think about legalized abortion, I think it sounds like a really sick form of insurance policy against an unwanted pregnancy. When I see that one sumo wrestler has more to gain from a win than the other foregoes by losing, I figure they’ll make a deal. When I think about real-estate agents, I’m constantly paranoid they are trying to screw me.

I am the first to admit that if all economists were like me, the field would probably be a disaster. But the fact that other economists more or less like me in spite of this fact tells me that there is plenty more room for rogue economists in the profession.

—SDL (May 23, 2005)

“Our California Trip”

Last week, we went to California. Our publisher, William Morrow/HarperCollins, had determined that Freakonomics wasn’t selling as well there as elsewhere. It may have been a simple case of late adoption—Levitt and I are based in Chicago and New York, respectively, two cities where the book started strong—but Harper was taking no chances. So, having to this point avoided anything resembling a book tour, we were shipped west for three days. This was more of a hardship for Levitt than for me. He hates human interaction (or so he says). Our first day there, in L.A., he constantly claimed to be feeling suicidal. But he said this casually, and with a bit of a smile. I felt like Mandy Patinkin in The Princess Bride, when he tells Wally Shawn, “I don’t think that word means what you think it means.” But hey: Levitt is more of a numbers guy than a word guy. Maybe what he meant was “homicidal.”

On the final day, we visited Google headquarters in Mountain View. The Google folks later asked us to write up our impressions, to be posted on the Google blog. Here’s what we had to say:


To: All Googlers

From: The Freakonomics Guys

Date: Aug. 4, 2005

Re: Our visit last week

We didn’t know quite what to expect at Google. A few months ago, we had been invited to give some kind of presentation at Google while we were in California. Were we interested? Sure, we said. When something is that far away, you’ll generally agree to it without much thought.

Because we got to the Googleplex late—we were coming from a meeting with some people who may want to turn Freakonomics into a board game (!)—our tour was cut a little short. Still, we did manage to see:


—Your Google-logo-colored pylons at the extremely low-key “security” post.

—Your very user-friendly name-tag-generating/sign-in computer.

—Your very, very fancy toilets.

—Your rack of primordial servers with the thin layers of cork that used to make the fire department so nervous.

—Your roaming dogs, one friendlier than the next.

—Your beautiful scrolling query screens, which are a great piece of conceptual art: Hilary Duff…pits puppies…Yenifer Lopez…Spanish Dictionary. (We were a little disappointed to not catch a glimpse of “Freakonomics” but maybe it got caught in your filter: people sometimes give it some pretty deviant spellings.)

—Your quartz rug, robust cacti, fancy yurts, and ecologically sound staircase in the Africa building.

Then it was time for our “presentation.” Our guide, Hunter Walk, walked us over to the room where we’d be speaking: Whomp! It wasn’t some little room, with a conference table and a couple dozen people, as we’d imagined. It was a big big room, rows and rows of chairs, all of them filled with Googlers, and many many more Googlers sitting on the floor and standing in the back and—well, not exactly hanging from the rafters but it felt like it. The walls were black, the stage lights white-hot, the room alive with chatter. This wasn’t a presentation; this was a presentation. It was a Sally Field moment: They like us! They really like us! (We realize, of course, that the average Googler is far too young to catch this reference. Don’t worry; it’s not very funny anyway.) As we picked our way through the floor-sitting Googlers, it felt like we should have been carrying a couple of Telecasters; it was likely the closest that either of us will ever get to having a rock-star moment. (In truth, I was a minor-league rock star but that was in the late 1980s, so it doesn’t really count.)

The other thing is, Hunter had ordered a few hundred copies of Freakonomics from Amazon* and passed them around, so now, looking across the long rows of chairs, you could see one Googler after the next with the open book in his/her lap, as if preparing to hear a speech from Chairman Mao. It was, well, freaky. A bit like happening upon your own funeral.

We had to talk things over to decide what kind of talk to give. We are not very practiced at this. Hunter was encouraging, and patient. There was one podium and one microphone, so we decided to do a tag-team talk, to discuss the book (why crack dealers still live with their moms, e.g.) and to tell a few stories based on research that’s happened since the book (monkey prostitution at Yale, e.g.). We seemed to do okay, based on the fact that you all laughed a lot, although it’s quite possible you were just laughing at us. The biggest laugh came when Levitt mentioned that we spoke at Yahoo! a day earlier, and got a much smaller crowd. The funny thing is, that was really true. Your turnout was about double Yahoo!’s. On the other hand, that means Google may have lost twice the productivity—unless you think that our Freakonomics talk may have somehow increased productivity, in which case you thought a lot more of it than we did. The best question of the day was this: “What would you do with our data if we could give it to you?” Believe us, we’ve thought about that quite a bit. We’ll get back to you.

After our talk, we had a few minutes to hang around and talk with miscellaneous Googlers. This was the most impressive slice of the day. Not only were you all smart and inquisitive and friendly, but you were so damn happy. First of all, there is surely no company in the world where so many employees wear T-shirts with their company logo, which we took to be a sign of true pride (or perhaps simply a deep, deep discount). But the happiness shone through in a dozen other ways. It seems this is the by-product of doing interesting work with smart colleagues in beautiful environs, all with a profound sense of mission. A $297 stock price probably doesn’t hurt, either.

—SJD (Aug. 19, 2005)

2. ROE V. WADE AND CRIME, CONT’D.

Of all the topics covered in Freakonomics, one would have thought that the theory linking the legalization of abortion to a drop in crime would have engendered the most hate mail. But that wasn’t the case at all. It seems that when people read for themselves the argument as laid out in chapter 4, and see that it isn’t a remotely political or religious argument, they weigh for themselves how they feel about the theory and seldom resort to an overheated defense of their beliefs, wherever those beliefs may lie.

The same cannot be said for some of the other stories in the book. The study about real-estate agents, for instance, provoked hundreds of angry e-mails, most of them from Realtors who were unhappy with our description of how the incentive structure of their business encourages agents to exploit their own clients. There were also plenty of e-mails from teachers who didn’t like hearing about teachers who cheat; from parents who couldn’t accept some of our conclusions about parenting; and from readers who thought the entire chapter on first names was downright idiotic.

But if the abortion-crime story didn’t produce much reader outrage, it certainly did resonate in the media and elsewhere. This was never more true than when William Bennett cited the book in the process of creating a huge racial controversy for himself. Here are two blog postings addressing different elements of the abortion-crime debate. The first is an assessment of Bennett’s statements. The second is a response to an academic challenge to the abortion-crime theory; it is fairly technical (the faint of heart may wish to read just the last three paragraphs), but key to understanding the original research.

“Bill Bennett and Freakonomics”

Bill Bennett and I have a fair amount in common. We’ve both written about crime (his “superpredator” theory gets a quick discussion in Freakonomics), we have both thought a lot about illegal drugs and education (he was the original “drug czar” and is a former secretary of education), and we both love to gamble (although it seems I do it for much lower stakes and perhaps with greater success).

Now we also share the fact that we have made controversial statements about the link between abortion and crime.

Here’s what Bennett said during the Sept. 28 broadcast of Salem Radio Network’s Bill Bennett’s Morning in America:

CALLER: I noticed the national media, you know, they talk a lot about the loss of revenue, or the inability of the government to fund Social Security, and I was curious, and I’ve read articles in recent months here, that the abortions that have happened since Roev. Wade, the lost revenue from the people who have been aborted in the last thirty-something years, could fund Social Security as we know it today. And the media just doesn’t—never touches this at all.

BENNETT: Assuming they’re all productive citizens?

CALLER: Assuming that they are. Even if only a portion of them were, it would be an enormous amount of revenue.

BENNETT: Maybe, maybe, but we don’t know what the costs would be, too. I think—does abortion disproportionately occur among single women? No.

CALLER: I don’t know the exact statistics, but quite a bit are, yeah.

BENNETT: All right, well, I mean, I just don’t know. I would not argue for the pro-life position based on this, because you don’t know. I mean, it cuts both—you know, one of the arguments in this book Freakonomics that they make is that the declining crime rate, you know, they deal with this hypothesis, that one of the reasons crime is down is that abortion is up. Well—

CALLER: Well, I don’t think that statistic is accurate.

BENNETT: Well, I don’t think it is either, I don’t think it is either, because first of all, I think there’s just too much that you don’t know. But I do know that it’s true that if you wanted to reduce crime, you could—if that were your sole purpose, you could abort every black baby in this country, and your crime rate would go down. That would be an impossible, ridiculous, and morally reprehensible thing to do, but your crime rate would go down. So these far-out, these far-reaching, extensive extrapolations are, I think, tricky.

Bennett’s comments have, not surprisingly, ignited a furor—in the media and even at the White House, which denounced his statement.

Here are my thoughts on this exchange:


People should bear in mind that this took place on an unscripted radio show in response to a caller’s question. It was clearly off-the-cuff. This is a very different situation than, say, Bennett’s writing an op-ed piece.

Race is not an important part of the abortion-crime argument that John Donohue and I have made in academic papers and that Dubner and I discuss in Freakonomics. It is true that, on average, crime involvement in the U.S. is higher among blacks than whites. Importantly, however, once you control for income, the likelihood of growing up in a female-headed household, having a teenage mother, and how urban the environment is, the importance of race disappears for all crimes except homicide. (As we’ve written, the homicide gap is partly explained by crack markets.) In other words, for most crimes, a white person and a black person who grow up next door to each other with similar incomes and the same family structure would be predicted to have the same crime involvement. Empirically, what matters is the fact that abortions are disproportionately used on unwanted pregnancies, and disproportionately by teenage women and single women.

Some people might think that my comments in point 2 above are just ducking the race issue because it is politically correct to do so. Anyone who has read Freakonomics knows that I am not afraid to take issues of race head-on. Much of the book deals with challenging issues of race (e.g., black-white test score gaps, black naming patterns, etc.). I mean it when I say that, from a purely fact-based and statistical perspective, race is not in any way central to our arguments about abortion and crime.

When a woman gets an abortion, for the most part it is not changing the total number of children she has; rather, it is shifting the timing so those births come later in life. This is an important fact to remember. One in four pregnancies ends in abortion and this has been true for thirty years in the U.S. But the impact of abortion on the overall birth rate has been quite small.

In light of point 4 above, it is hard to even know what Bennett means when he says “you could abort every black baby in this country, and your crime rate would go down.” Implicit in his comment is the idea that some external force, like a government, is forcing blacks to have abortions. This is obviously a completely different situation than abortion as we know it today, in which a woman chooses whether or not to have an abortion now, and then starts her family later in life, when her situation is more stable and conducive. The distinction between a woman choosing to control her fertility and the government choosing to limit her fertility is fundamental, and people often seem to lose sight of that.

If we lived in a world in which the government chooses who gets to reproduce, then Bennett would be correct in saying that “you could abort every black baby in this country, and your crime rate would go down.” Of course, it would also be true that if we aborted every white, Asian, male, Republican, and Democratic baby in that world, crime would also fall. Immediately after he made the statement about blacks, he followed it up by saying, “That would be an impossible, ridiculous, and morally reprehensible thing to do, but your crime rate would go down.” He made a factual statement (if you prohibit any group from reproducing, then the crime rate will go down), and then he noted that just because a statement is true, it doesn’t mean that it is desirable or moral. That is, of course, an incredibly important distinction and one that we make over and over in Freakonomics.

There is one thing I would take Bennett to task for: first saying that he doesn’t believe our abortion-crime hypothesis but then revealing that he does believe it with his comments about black babies. You can’t have it both ways.

As an aside, the caller’s initial hypothesis is completely wrong. If abortion were illegal, our Social Security problems would not be solved. As noted above, most abortions just shift a child from being born today to a child being born to the same mother a few years later.

—SDL (Sept. 30, 2005)

“Back to the Drawing Board for Our Latest Critics”

Thanks to recent articles in the Wall Street Journal and the Economist, a working paper by Chris Foote and Chris Goetz that is sharply critical of John Donohue and me has gotten an enormous amount of attention.

In that working paper, Foote and Goetz criticized the analysis underlying one of the tables in our original article that suggested a link between legalized abortion and crime. (It is worth remembering that the approach they criticize was one of four distinct pieces of evidence we presented in that paper; they offer no criticisms of the other three approaches.)

Foote and Goetz made two basic changes to the original analysis we did. First, they correctly noted that the text of our article stated that we had included state-year interactions in our regression specifications, when indeed the table that got published did not include these state-year interactions. Second, they correctly argue that without controlling for changes in cohort size, the original analysis we performed provided a test of whether cohorts exposed to high rates of legalized abortion did less crime, but did not directly afford a test of whether “unwantedness” was one of the channels through which this crime reduction operated. (Note: we didn’t claim that this particular analysis was a direct test of the “unwantedness” hypothesis. This last section of the paper was the most speculative analysis of everything that we did, and frankly we were surprised it worked at all, given the great demands it put on the data.) They found that once you made those changes, the results in our original Table 7 essentially disappear.

There is, however, a fundamental problem with the Foote and Goetz analysis. The abortion data that are available are likely to be quite noisy. As one adds more and more control variables (e.g., nearly 1,000 individual state-year interactions), the meaningful variation in abortion rates gets eaten away. The signal-to-noise ratio in what remains of the variation in measured abortions gets worse and worse. That will lead the measured impact of abortions on crime to dwindle. Because this work uses a state/year/single year of age (e.g., 19-year-olds in Ohio in 1994) as the unit of analysis, the analyses performed are highly saturated with interactions: state-age interactions, age-year interactions, and state-year interactions. Together, these interactions account for more than 99 percent of the variance in arrest rates and more than 96 percent of the variation in the abortion proxy. It is an exercise that is very demanding of the data.

In light of this, it seems uncontroversial that one would want to do the best one could in measuring abortion when carrying out such an exercise.

The abortion measure used by Foote and Goetz is one that is produced by the Alan Guttmacher Institute. The Alan Guttmacher Institute makes estimates based on surveys of abortion providers of the number of abortions performed per live birth in each state and year.

To proxy for the abortion exposure of, say, 19-year-olds arrested in California in 1993, Foote and Goetz use the abortion rate in California in 1973. This is not an unreasonable first approximation (and indeed is the one we used in most parts of our original paper because it is simple and transparent), but it is just an approximation for a number of reasons:


There is a great deal of cross-state mobility. Therefore, many of the 19-year-olds arrested in California in 1993 were not born in California. They were born in other states, or possibly other countries. Indeed, I believe that recent figures suggest that more than 30 percent of those in their late teens do not reside in the state in which they were born.

Using a date of 20 years earlier to proxy for the abortion exposure of a 19-year-old induces an enormous amount of noise. If I am a 19-year-old sometime in 1993, I may have been born as early as Jan. 2, 1973 (that would make me still 19 on Jan. 1, 1993) or as late as Dec. 31, 1974 (that would have me turning 19 on Dec. 31, 1993). Abortions occur sometime in advance of birthdays, typically about 13 weeks into a pregnancy. So the relevant date (roughly) of when those who are 19 in 1993 would have been exposed to legalized abortion is about six months before they were born, or July 2, 1972, through June 30, 1974. While that window overlaps with the year 1973 (which is what Foote and Goetz use as their time period of abortion exposure), note that it also includes half of 1972 and half of 1974!

A non-trivial fraction of abortions performed in the United States, especially in the time when legalization was taking place, involved women crossing state lines to get an abortion. As a consequence, measuring abortions in terms of the state in which the abortion is performed (that’s what the Foote/Goetz data does), rather than the state of residence of the woman getting the abortion, induces further measurement error into their abortion proxy.

The Alan Guttmacher abortion numbers are, even by the admission of the people who collect the data, far from perfect. Indeed, the correlation between these abortion estimates and another time series collected by the CDC is well below one, suggesting that even if problems 1, 2, and 3 didn’t exist, there would be substantial measurement error. The correlation between the Alan Guttmacher measure and the CDC measure, not surprisingly, gets lower and lower the more control variables that are included. This is exactly what one would expect if the controls are taking the signal out of the abortion measures and leaving behind mostly noise.

What John Donohue and I have done (with fantastic research assistance from Ethan Lieber) is to attempt to address as best we can these four problems with the abortion measure that Foote and Goetz are using. In particular, we do the following:


As we describe in our original paper on abortion, one can deal with cross-state mobility by using the decennial censuses to determine the state of birth for the current residents of a state. (The results from carrying out this correction in our crime regressions are reported in Table 5 of the original 1999 paper.) This is possible to do because the census micro data reports the state of birth and current state of residence for a 5 percent sample of the U.S. population. Note that the correction we are able to make is unlikely to be perfect, so it may not fully solve the problem, but it clearly moves us in the right direction.

Given that the window of abortion exposure faced by 19-year-olds in 1993 spans the years 1972 to 1974, the obvious solution to this problem is to allow abortions performed in 1972, 1973, and 1974 to influence arrests of 19-year-olds in 1993. It is straightforward to work out roughly the weights that one wants to put on the different years’ abortion rates—or one can do it non-parametrically and let the data decide; the answers are virtually identical.

In order to deal with the fact that many women were crossing state lines to get abortions in the 1970s, we use the Guttmacher Institute’s estimates of abortions performed on women residing in a state relative to live births in that state. (We were unaware of the existence of these better data when we wrote the initial paper, otherwise we would have used them at that time.) There is little question that measuring abortions by state of residence is superior to measuring them by where the procedure is performed.

The standard solution to measurement error is to perform an instrumental-variables analysis in which you use one noisy proxy of the phenomenon that is poorly measured as an instrument for another noisy proxy. (I recognize that most readers of this blog will not understand what I mean by this.) In this setting, the CDC’s independently generated measure of legalized abortions is likely to be an excellent instrument. Because there is so much noise in each of the measures, the standard errors increase when doing this I.V. procedure, but under a standard set of assumptions, the estimates obtained will be purged of the attentuation bias that will be present due to measurement error.

I think that just about any empirical economist would tend to believe that each of these four corrections we make to the abortion measure will lead us closer to capturing the true impact of legalized abortion on crime. So the question becomes: What happens when we replicate the specifications reported in Foote and Goetz, but with this improved abortion proxy?

The results are summarized in the following table, which has two panels. The top panel shows the results for violent crime. The bottom panel corresponds to property crime.

Starting with the first panel, the top row reports the same specifications as Foote and Goetz (I don’t bother showing their estimates excluding state-age interactions because it makes no sense to exclude these and they themselves say that their preferred specifications include state-age interactions). We are able to replicate their results. As can be seen, the coefficients shrink as one adds state-year interactions and population controls.

The second row of the table presents the coefficients one obtains with our more thoughtfully constructed abortion measure (changes 1–3 on pages 259–60 having been made to their abortion measure). With a better measure of abortion, as expected, all the estimated abortion impacts increase across the board. The results are now statistically significant in all of the Foote and Goetz specifications. Even in the final, most demanding specification, the magnitude of the coefficient is about the same as in the original results we published that didn’t control for state-year interactions or population. The only difference between what Foote and Goetz did and what we report in row 2 is that we have done a better job of really measuring abortion. Everything else is identical.

The third row of the table reports the results of instrumental variables estimates using the CDC abortion measure as an instrument for our (more thoughtfully constructed) Guttmacher Institute proxy of abortions. The results all get a little bigger but are more imprecisely estimated.

The bottom panel of the table shows results for property crime. Moving from Foote and Goetz’s abortion measure in the top row to our more careful one in the second row (leaving everything else the same), the coefficients become more negative in three of the four specifications. Doing the instrumental variables estimation has a bigger impact on property crime than on violent crime. All four of the instrumental variables estimates of legalized abortion on property crime are negative (although again less precisely estimated).

The simple fact is that when you do a better job of measuring abortion, the results get much stronger. This is exactly what you expect of a theory that is true: doing empirical work closer to the theory should yield better results than empirical work more loosely reflecting the theory. The estimates without population controls, but including state-year interactions, are as big or bigger than what is in our original paper. As would be expected (since the unwantedness channel is not the only channel through which abortion is acting to reduce crime), the coefficients we obtain shrink when we include population controls. But, especially for violent crime, a large impact of abortion persists even when one measures arrests per capita.

The results we show in this new table are consistent with the impact of abortion on crime that we find in the three other types of analyses we presented in the original paper using different sources of variation. These results are consistent with the unwantedness hypothesis.

No doubt there will be future research that attempts to overturn our evidence on legalized abortion. Perhaps they will even succeed. But this one does not.

—SDL (Dec. 5, 2005)

3. WHAT DO THE KANSAS CITY ROYALS HAVE IN COMMON WITH AN iPOD?

One useful purpose of the Freakonomics blog (of any blog, really) is to make random reflections on random subjects—including, as it turns out, the subject of randomness itself.

“What Do the K.C. Royals and My iPod Have in Common?” On the surface, not much. The Royals have lost 19 straight games and are threatening to break the all-time record for futility in major-league baseball. My iPod, on the other hand, has quickly become one of my most beloved material possessions.

So what do they have in common? They both can teach us a lesson about randomness.

The human mind does badly with randomness. If you ask the typical person to generate a series of “heads” and “tails” to mimic a random sequence of coin tosses, the series doesn’t really look like a randomly generated sequence at all. You can try it yourself. First, before you read further, write down what you expect a random series of 20 coin tosses to look like. Then spend 15 or 20 minutes flipping coins (or use a random number generator in Excel). If you are like the typical person, the “random” sequence you generated will have many fewer long streaks of “all heads” or “all tails” than actually arise in real life.

My iPod shuffle reminds me of this every time I use it. I’m consistently surprised at how often it plays two, three, or even four songs by the same artist, even though I have songs by dozens of different artists on it. On a number of occasions, I’ve even become mistakenly convinced I don’t have the iPod on shuffle, but rather I’m playing all the songs by one artist. If someone is really bored, maybe they can repeatedly have the iPod shuffle the songs, record the data, and see if the shuffle function really is random. My guess is that it is, because what would be the point of Apple doing something different? I have a friend Tim Groseclose, a professor of political science at UCLA, who was convinced that the random button on his CD player knew which songs were his favorites and disproportionally played those. So I bet him one day, made him name his favorite songs in advance, and won lunch.

Which brings us to the Kansas City Royals. It seems like, when a team loses 19 games, that is so extreme that it can’t reasonably be the result of randomness. Clearly coaches, sportswriters, and most fans believe that to be true. How often have you heard of a coach holding a closed-door meeting to try to turn a team around? But if you look at it statistically, you expect 19-game losing streaks to occur, simply by randomness, about as often as they do.

The following calculations are admittedly crude, but they give you the basic idea. Each year, there are about two teams in the major leagues that have a winning percentage of around 35. (Sometimes no team is that bad, and in other years there are real stinkers like Detroit in 2003—they won only 26.5 percent of their games.) For a team that has a 35 percent chance of winning each game, the chance of losing its next 19 games is about one in 4,000. Each team plays 162 games a year and so has 162 chances to start such a streak. (They count streaks that begin in one year and end in the next year, so it is correct to use all 162 games.)

So each year, for these two bad teams that win 35 percent of their games, there are a total of 324 chances to have a 19-game losing streak. It takes about 12 or 13 years for these two bad teams to have a total of 4,000 chances for a 19-game losing streak. Thus we would expect a losing streak this long a little less than once a decade.

In practice, we see, if anything, slightly fewer long losing streaks than expected based on these calculations. The last really long losing streak was that of the Cubs in 1996–1997, which was 16 games. (There is actually a good reason that long streaks occur a little less than in the simple model I was using. It is because a team that wins 35 percent of their games doesn’t have the same likelihood of winning every game: sometimes it has a 50 percent chance and sometimes a 20 percent chance; that sort of variability lessens the likelihood of long streaks.)

So, one doesn’t need to resort to explanations like “lack of concentration,” being “snakebit,” or “demoralization” to explain why the Royals are losing so many games in a row. It’s just that they are a bad team getting some bad luck.

—SDL (Aug. 20, 2005)

“Wikipedia? Feh!”

I know, I know, I know: Wikipedia is one of the wonders of the online world. But if anyone ever needs a reason to be deeply skeptical of Wikipedia’s dependability, I urge you to click on the entry for “List of Economists,” which is introduced thusly: “This is an alphabetical list of well-known economists. Economists are scholars conducting research in the field of economics.”

It is true that the list includes George Akerlof and Paul Samuelson and Jeffrey Sachs and even Steve Levitt. But if you want to see how truly pathetic Wikipedia can be, check out the sixth “economist” listed under “D”: that’s right, yours truly. Although some of my best friends are economists, I am very much not. (Note: soon after I posted this entry, a reader was helpful/mischievous enough to quickly amend the Wikipedia entry, deleting my name.) The point is that the greatest strength of Wikipedia is also its greatest weakness: pretty much anyone can contribute anything anytime to an “encyclopedia” that most casual users will assume is in fact encyclopedic, but which in fact changes regularly, depending on the input of its users. For instance:

Freakonomics, we make a passing reference to the Chicago Black Sox, the name given to the Chicago White Sox after eight players were found to have colluded with gamblers to throw the 1919 World Series.

A reader recently wrote: The 1919 white sox were not known as the black sox because they threw the world weries [sic]. They were called that because their owner (whose name i do not have) was too stingy to have their uniforms cleaned regularly so that they frequently showed up on the diamond in dirty uniforms. You’re welcome.

This was in fact the second reader to write with this same correction. We had asked the first reader for his source; he said he thought he “heard it once on ESPN,” but couldn’t be sure. After receiving this second e-mail, I decided to investigate. Here is my reply to reader no. 2, and to anyone else who may care:


I looked into the Black Sox thing. It is true that the Wikipedia entry says this: Although many believe the Black Sox name to be related to the dark and corrupt nature of the consipiracy [sic], the term Black Sox had already existed before the fix was investigated. The name Black Sox was given because parsimonious owner Charles Comiskey refused to pay for the players’ uniforms to be laundered, instead insisting that the players themselves pay for the cleaning. The players refused, and the subsequent series of games saw the White Sox play in progressivly [sic] dirtier uniforms, as dust, sweat, and grime collected on the white, woollen [sic] uniforms until they took on a much darker shade. (does anyone have proof of this? sounds like urban legend to me)

Two things to say about this: 1. The parenthetical phrase at the end was just added—by me. 2. In other words, let’s remember that Wikipedia is an open-access “encyclopedia” that can be contributed to (or vandalized) at will.

Here’s a more reliable source: Eight Men Out: The Black Sox and the 1919 World Series, by Eliot Asinof (Holt, Rinehart and Winston,1963). Asinof writes that White Sox owner Charles Comiskey was indeed cheap when it came to his players: His generosity here [with reporters] was unmatched. Yet his great ball club might run out on the field in the filthiest uniforms the fans had ever seen: Comiskey had given orders to cut down on the cleaning bills.

So is it possible that the White Sox were known, even slightly or colloquially, as the Black Sox before the 1919 scandal?

Sure, it’s possible, but Asinof makes no such insinuation throughout the book. In fact, once you get past the book’s opening pages, I didn’t find the words “Black Sox”, where Asinof writes of the aftermath of the World Series scandal: “As the impact of the confessions sank in, the American people were at first shocked, then sickened. There was hardly a major newspaper that did not cry out its condemnation and despair. Henceforth, the ballplayers involved were called the Black Sox.”

Note the key word: henceforth. Is it possible that Asinof was wrong? Sure. But his book is a good one, commonly accepted as the definitive biography of the affair. I don’t feel compelled to check this out further until someone comes up with contrary evidence that’s more reliable than Wikipedia. But if you do, I’ll be happy to research further, or make a change in future editions of Freakonomics.

So please, dear blog readers: let us know if we’re right or wrong re the Black Sox. We’ll be a little sad to have been wrong, but a lot happy to correct the mistake. A Freakonomics T-shirt goes to the first person who offers hard evidence of the dirty-socks theory.

—SJD (May 20, 2005, and Aug. 5, 2005)

“‘Peak Oil’: Welcome to the Media’s New Version of Shark Attacks”

The cover story of the Aug. 21 New York Times Magazine, written by Peter Maass, is about “Peak Oil.” The idea behind “peak oil” is that the world has been on a path of increasing oil production for many years, and now we are about to peak and go into a situation where there are dwindling reserves, leading to triple-digit prices for a barrel of oil, an unparalleled worldwide depression, and as one web page puts it, “Civilization as we know it is coming to an end soon.”

One might think that doomsday proponents would be chastened by the long history of people of their ilk being wrong: Nostradamus, Malthus, Paul Ehrlich, et al. Clearly they are not.

What most of these doomsday scenarios have gotten wrong is the fundamental idea of economics: people respond to incentives. If the price of a good goes up, people demand less of it, the companies that make it figure out how to make more of it, and everyone tries to figure out how to produce substitutes for it. Add to that the march of technological innovation (like the green revolution, birth control, etc.). The end result: markets figure out how to deal with problems of supply and demand.

Which is exactly the situation with oil right now. I don’t know much about world oil reserves. I’m not even necessarily arguing with their facts about how much the output from existing oil fields is going to decline, or that world demand for oil is increasing. But these changes in supply and demand are slow and gradual—a few percent each year. Markets have a way of dealing with situations like this: prices rise a little bit. That is not a catastrophe; it is a message that some things that used to be worth doing at low oil prices are no longer worth doing. Some people will switch from SUVs to hybrids, for instance. Maybe we’ll be willing to build some nuclear power plants, or it will become worthwhile to put solar panels on more houses.

But the New York Times article totally flubs the economics time and again. Here is one example:


The consequences of an actual shortfall of supply would be immense. If consumption begins to exceed production by even a small amount, the price of a barrel of oil could soar to triple-digit levels. This, in turn, could bring on a global recession, a result of exorbitant prices for transport fuels and for products that rely on petrochemicals—which is to say, almost every product on the market. The impact on the American way of life would be profound: cars cannot be propelled by roof-borne windmills. The suburban and exurban lifestyles, hinged to two-car families and constant trips to work, school and Wal-Mart, might become unaffordable or, if gas rationing is imposed, impossible. Car-pools would be the least imposing of many inconveniences; the cost of home heating would soar—assuming, of course, that climate-controlled habitats do not become just a fond memory.

If oil prices rise, consumers of oil will be (a little) worse off. But we are talking about needing to cut demand by a few percent a year. That doesn’t mean putting windmills on cars, it means cutting out a few low-value trips. It doesn’t mean abandoning North Dakota, it means keeping the thermostat a degree or two cooler in the winter.

A little later, the author writes:


The onset of triple-digit prices might seem a blessing for the Saudis—they would receive greater amounts of money for their increasingly scarce oil. But one popular misunderstanding about the Saudis—and about OPEC in general—is that high prices, no matter how high, are to their benefit. Although oil costing more than $60 a barrel hasn’t caused a global recession, that could still happen: it can take a while for high prices to have their ruinous impact. And the higher above $60 that prices rise, the more likely a recession will become. High oil prices are inflationary; they raise the cost of virtually everything—from gasoline to jet fuel to plastics and fertilizers—and that means people buy less and travel less, which means a drop-off in economic activity. So after a brief windfall for producers, oil prices would slide as recession sets in and once-voracious economies slow down, using less oil. Prices have collapsed before, and not so long ago: in 1998, oil fell to $10 a barrel after an untimely increase in OPEC production and a reduction in demand from Asia, which was suffering through a financial crash.

Oops, there goes the whole peak-oil argument. When the price rises, demand falls, and oil prices slide. What happened to “the end of the world as we know it”? Now we are back to $10-a-barrel oil. Without realizing it, the author just invoked basic economics to invalidate the entire premise of the article!

Just for good measure, he goes on to write:


High prices can have another unfortunate effect for producers. When crude costs $10 a barrel or even $30 a barrel, alternative fuels are prohibitively expensive. For example, Canada has vast amounts of tar sands that can be rendered into heavy oil, but the cost of doing so is quite high. Yet those tar sands and other alternatives, like bioethanol, hydrogen fuel cells and liquid fuel from natural gas or coal, become economically viable as the going rate for a barrel rises past, say, $40 or more, especially if consuming governments choose to offer their own incentives or subsidies. So even if high prices don’t cause a recession, the Saudis risk losing market share to rivals into whose nonfundamentalist hands Americans would much prefer to channel their energy dollars.

As he notes, high prices lead people to develop substitutes. Which is exactly why we don’t need to panic over peak oil in the first place.

So why do I compare peak oil to shark attacks? It is because shark attacks mostly stay about constant, but fear of them goes up sharply when the media decides to report on them. The same thing, I bet, will now happen with peak oil. I expect tons of copycat journalism stoking the fears of consumers about oil-induced catastrophe, even though nothing fundamental has changed in the oil outlook in the last decade.

—SDL (Aug. 21, 2005)

“Is America Ready for an Organ-Donor Market?”

Probably not. But, in what is either a very odd coincidence or some kind of concerted effort to get out the organ-market message, there are op-eds in both the New York Times and Wall Street Journal today arguing the case.

The first one, headlined “Death’s Waiting List,” is by Sally Satel, a psychiatrist and American Enterprise Institute scholar. Satel herself received a kidney transplant and is now arguing that the delivery system is terrible and that the Institute of Medicine’s new report, “Organ Donation: Opportunities for Action,” is even worse. “Unfortunately,” Satel writes, “the report more properly should be subtitled ‘Recommendations for Inaction.’” Satel’s main point is that the conventional argument against an organ market—i.e., that no part of the human body should ever be “for sale”—has been made obsolete, and then some, by the “markets for human eggs, sperm and surrogate mothers.”

The WSJ piece, headlined “Kidney Beancounters,” is by Richard Epstein, the University of Chicago legal scholar and Hoover Institution fellow. Epstein is even more hostile to the IOM’s report (though maybe the Journal just let him get away with more than the Times let Satel get away with), saying the report is “so narrowminded and unimaginative that it should have been allowed to die inside the IOM.” Epstein writes further that “the major source of future improvement lies only in financial incentives; yet the IOM committee (which contains one lawyer but no economist) dismisses these incentives out of hand. … The key lesson in all this is that we should look with deep suspicion on any blanket objection to market incentives—especially from the high-minded moralists who have convinced themselves that their aesthetic sensibilities and instinctive revulsion should trump any humane efforts to save lives.”

Though his op-ed doesn’t say so, I am pretty sure that Epstein is an advisor to LifeSharers, a self-described “non-profit voluntary network of organ donors” that seeks to use non-financial incentives to encourage organ donation. A while ago, we received an e-mail from David Undis, the executive director of LifeSharers. He wrote:


Incentives are missing in organ donation. That’s one of the reasons so many people are dying waiting for organ transplants.

A free market in human organs would save thousands of lives a year, but politically speaking it’s a pipe dream. There’s very little likelihood Congress will legalize buying and selling organs in the foreseeable future.

I formed LifeSharers to introduce a legal non-monetary incentive to donate organs—if you agree to donate your organs when you die then you’ll receive a better chance of getting an organ if you ever need one to live.

It is surprising to me, and to many people much closer to the subject than me, that so little headway has been made in reforming the organ-donation process. I have never heard a single person say they were happy with the way things are—and, while I am sure Undis is right when he writes that a free market in organs is, politically speaking, a pipe dream, it seems that things are starting to move at least a bit in that direction. As Satel writes in her Times piece today, “Ethics committees of the United Network for Organ Sharing, the American Society of Transplant Surgeons and the World Transplant Congress, along with the President’s Council on Bioethics and others, have begun discussing the virtues” of offering organ donors incentives such as “tax breaks, guaranteed health insurance, college scholarships for their children, deposits in their retirement accounts, and so on.”

It is interesting that, while all these incentives are financial, none of them are in the form of cold hard cash, which may make them more palatable.

I wouldn’t be surprised if, between these two op-eds, at least a few minds are changed today.

—SJD (May 15, 2006)

4. WHY PAY $36.09 FOR RANCID CHICKEN?

A blog can also be a nice place to get things off your chest—rants (and, occasionally, raves) of a more personal nature.

“Why Pay $36.09 for Rancid Chicken?”

An old friend came to town not long ago and we met for a late lunch on the Upper West Side. Trilby ordered a burger, no bread, with Brie; I ordered half a roasted chicken with mashed potatoes. The food was slow in coming but we had so much catching up to do that we didn’t mind.

My chicken, when it arrived, didn’t look good but I took a bite. It was so rancid I had to spit it out into a napkin. Absolutely disgusting gagging rotten rancid. I summoned the waitress, who made a suitably horrified expression, then took the food away.

The manager appeared. She was older than the waitress, with long dark hair and a French accent. She apologized, said the chefs were checking out the dish now, trying to determine if perhaps the herbs or the butter had caused the problem.

“I don’t think so,” I told her. “I think your chicken is rotten. I cook a lot of chicken, and I know what rotten chicken smells like.” Trilby agreed: you could smell this plate across the table, probably across the restaurant.

The manager was reluctant to concede. They had just gotten the shipment of chicken that morning, she said, which struck me as not really relevant, like saying: No, so-and-so couldn’t have committed a murder today because he didn’t commit one yesterday.

The manager left and, five minutes later, returned. “You’re right!” she said. “The chicken was bad.” She said the chefs had checked the chicken, found it rotten, and were throwing it away. Victory! But for whom? The manager apologized again, asked if I’d like a free dessert or drink. “Well,” I said, “first of all let me try to find some food on your menu that doesn’t seem disgusting after that chicken.” I ordered a carrot-ginger-orange soup, some French fries, and sautéed spinach.

Trilby and I then ate, fairly happily, though the taste of the rancid chicken remained with me; in fact, it remains with me still. Trilby had had a glass of wine before we ordered, and took another glass with her meal, sauvignon blanc. I drank water. When the waitress cleared our plates, she asked again if we wanted free dessert. Just coffee, we said.

As Trilby and I talked, I mentioned that not long ago I had interviewed Richard Thaler, the godfather of behavioral economics, a fairly new field of study that tries to explain why the psychology of money is so complicated. I mentioned the behavioralists’ concept of “anchoring”—a concept that used-car salesmen in particular know so well: establish a price that may be 100 percent more than what you need in order to ensure that you’ll still walk away with, say, a 50 percent profit.

Talk turned to what we might say when our check came. There seemed two good options: “We don’t care for any free dessert, thanks, but considering what happened with the chicken, we’d like you to comp our entire meal.” That would establish an anchor at 0 percent of the check. Or this option: “We don’t care for any free dessert, thanks, but considering what happened with the chicken, would you please ask the manager what you can do about the check.” That would establish an anchor at 100 percent of the check.

Just then the waitress brought the check. It was for $31.09. Perhaps out of shyness, or haste, or—most likely—a desire to not appear cheap (when it comes to money, things are never simple), I blurted out option 2: Please see what the manager “can do about the check.”

The waitress replied, smiling, that we had already been given the two glasses of wine for free. To me in particular this felt like slim recompense, since it was Trilby who had drunk the wine while it was I who still radiated with the flavor of rancid chicken. But the waitress, still smiling, duly took the check and headed toward the manager. She zipped right over, also smiling.

“Considering what happened with the chicken,” I said, “I wonder what you can do about the check.”

“We didn’t charge you for the wines,” she said, with great kindness.

“Is that the best that you’re prepared to offer me?” I said (still unable to establish an anchor at 0 percent).

She looked at me intently, still friendly. Here she was making a calculation, preparing to make the sort of slight gamble that is both financial and psychological, the sort of gamble that each of us makes every day. She was about to gamble that I was not the kind of person who would make a scene. After all, I had been friendly throughout our dilemma, never raising my voice or even uttering aloud the words “vomit” or “rancid.” And she plainly thought this behavior would continue. She was gambling that I wouldn’t throw back my chair and holler, that I wouldn’t stand outside the restaurant telling potential customers that I’d gagged on my chicken, that the whole lot of it was rancid, that the chefs either must have smelled it and thought they could get away with it, or, if they hadn’t smelled it, were so spaced out that who knows what else—a spoon, a sliver of thumb, a dollop of disinfectant—might find its way into the next meal. And so, making this gamble, she said “Yes”: as in Yes, that is the best that she was prepared to offer me. “All right,” I said, and she walked away. I left a $5 tip—no sense penalizing the poor waitress, right?—walked outside and put Trilby in a cab. The manager had gambled that I wouldn’t cause trouble, and she was right.

Until now.

The restaurant, should you care to note, is called French Roast, and is on the northeast corner of 85th and Broadway, in Manhattan.

Last I checked, the roast chicken was still on the menu. Bon appétit.

—SJD (May 8, 2005)

“Making Profits from Incivility on the Roads”

I hardly ever drive anymore since I moved close to where I work. So whenever I do, the incivility on the roads leaps out at me. People do things in cars they would never do in other settings. Honking. Swearing. Cutting to the front of the line. And that is just my wife. The other drivers are far meaner.

One obvious reason is that you don’t have to live with the consequences for any length of time. If you cut in line at airport security, you will be in close proximity for quite some time to the people you insulted. But with a car, you make a quick getaway.

When I used to commute, there was one particular interchange where incivility ruled. (For those who know Chicago, it is where the Dan Ryan feeds into the Eisenhower.) There are two lanes when you exit the highway. One lane goes to the other highway, and one to a surface street. Hardly anyone ever wants to go to that surface street. There can be a half-mile backup of cars waiting patiently to get on the highway, and about 20 percent of the drivers rudely and illegally cut in at the last second after pretending they are heading toward the surface street. Every honest person that waits in line is delayed 15 minutes or more because of the cheaters.

Social scientists sometimes talk about the concept of “identity.” It is the idea that you have a particular vision of the kind of person you are, and you feel awful when you do things that are out of line with that vision. That leads you to take actions that are seemingly not in your short-run best interest. In economics, George Akerlof and Rachel Kranton popularized the idea. I had read their papers, but in general have such a weak sense of identity that I never really understood what they were talking about. The first time I really got what they meant was when I realized that a key part of my identity was that I was not the kind of person who would cut in line to shorten my commute, even though it would be easy to do so, and even though it seemed crazy to wait 15 minutes in this long line. But if I were to cut in line, I would have to fundamentally rethink the kind of person I was.

The fact that I don’t mind when my taxi driver cuts in these lines (actually, I kind of enjoy it) probably shows that I have a long ways to go in my moral development.

All this is actually just a rambling prelude to my main point. I was in New York City the other day and my taxi driver bypassed a long line of cars exiting the freeway to cut in at the last second. As usual, I enjoyed being an innocent bystander/beneficiary to this little crime. But what happened next was even more gratifying to the economist in me. A police officer was standing in the middle of the road, waving every car that cut in line—including my taxi—over to the shoulder, where a second officer was handing out tickets as if on an assembly line. By my rough estimate, these two officers were giving out 30 tickets an hour, at $115 a pop. At more than $1,500 per officer per hour (assuming the tickets get paid), this was a fantastic moneymaking proposition for the city. And it nails just the right people. Speeding doesn’t really hurt other people very much, except indirectly. So to my mind, it makes much more sense to go directly after the mean-spirited behavior like cutting in line. This is very much in the spirit of William Bratton’s “broken windows” policing philosophy. I’m not sure it cuts down the number of cheaters on the roads in any fundamental way, since the probability of getting caught remains vanishingly small. Still, the beauty of it is that a) every driver that follows the rules feels a rush of glee over the rude drivers getting nailed; and b) it is a very efficient way of taxing bad behavior.

So, my policy recommendation to police departments across the country is to find the spots on the roads that lend themselves to this sort of policing and let the fun begin.

—SDL (Nov. 18, 2005)

“Vegas Rules”

So Levitt and I were in Las Vegas this weekend, doing some research. (Seriously.) We had a little downtime and decided to play blackjack. It was New Year’s Eve, at Caesars Palace, about 9 p.m. We sat down at an empty table where the dealer, a nice young woman from Michigan, was very patient in teaching us the various fine points that neither of us knew and which indicated that we were both inexperienced. Keep one hand in your lap, e.g. When you want a card, just flick your cards twice on the felt. When you’re standing, tuck one card under your chip/s. And so on.

At one point, Levitt kind of gasped. He had had 21 but somehow had asked for another card. The last card was a 2. It wasn’t that he didn’t know how to play, or count; he was just distracted—talking to me, he’d later claim—and the dealer had seen him do something, or fail to do something else, that indicated he wanted another card. So here he was with 4 cards: a face card, a 4, a 7, and a 2. The dealer looked sympathetic. I vouched for Levitt, told her he wasn’t an idiot and surely wouldn’t have hit on 21 intentionally. She seemed to believe us. She said she’d call over her supervisor to see what could be done.

She called the supervisor’s name over her shoulder. I could see the supervisor, and I could see that he couldn’t hear her. Remember, this is a casino on New Year’s Eve; it was pretty noisy. She keeps calling, and I keep seeing that he’s not hearing her, but she won’t turn around to call him. That would mean turning her back on her table full of chips and even if Levitt were dumb enough to hit on 21, he presumably was smart enough to grab a bunch of chips and run. (Or maybe, she was thinking, he’s actually dumb like a fox and used this hitting-on-21 trick all the time, to get the dealer to turn her back on the table.)

Finally, I went over and fetched the supervisor myself. The dealer explained the situation. He seemed to accept Levitt’s explanation. Then he looked at me. “Did you want the card?” he asked, meaning the 2 that Levitt drew.

“Well, now that I see it, sure I want it,” I said. I had 17; I certainly wouldn’t have hit on 17, but a 2 would give me a lovely 19.

“Here,” he said, and gave me the 2. “Happy New Year.”

Then the dealer took a card and busted.

I don’t know much about gambling, but I do know that the next time I’m in Vegas and feel compelled to play some blackjack, I’m going to Caesars.

And just so you don’t think that Levitt really is a complete gambling idiot: the next day, we sat down at the sports book and he grabbed a Daily Racing Form and studied it for about 10 minutes and then went up and placed a bet. He found a horse, going off at 7/2, that had never run a race. But he saw something that he liked. He bet the horse to win and win only. And then we watched the race on one of the jumbo screens. It took a good 60 seconds for his horse to settle into the gate—we thought it would be scratched—but then it got in and the gates opened and his horse led wire to wire. It was a good bit more impressive than his blackjack.

—SJD (Jan. 3, 2006)

“I Almost Got Sent to Guantánamo”

I arrived at the West Palm Beach airport yesterday, trying to make my way back to Chicago, only to see my flight time listed on the departure board as simply “delayed.” They weren’t even pretending it was leaving in the foreseeable future. With a little detective work, I found another flight that could get me home on a different airline, bought a one-way ticket, and headed for airport security.

Of course, the last-minute purchase of a one-way ticket sets off the lights and buzzers for the TSA. So I’m pulled out of the line and searched. First the full-body search. Then the luggage.

It didn’t occur to me that my latest research was going to get me into trouble. I’ve been thinking a lot about terrorism lately. Among the things I had in my carry-on was a detailed description of the 9/11 terrorists’ activities, replete with pictures of each of the terrorists and information about their background. Also, pages of my scribblings on terrorist incentives, potential targets, etc. This stuff was the first thing the screener pulled out of my bag. The previously cheery mood turned dark. Four TSA employees suddenly surrounded me. They didn’t seem very impressed with my explanation. When the boss arrived, one of the screeners said, “He claims to be an economics professor who studies terrorism.”

They proceeded to take every last item out of both my bags. It has been longer since I cleaned out my book bag than since I updated my personal web page. This is a book bag with 12 separate pockets, all of which are filled with junk.

“What is this?” the screener asked.

“It is a Monsters, Inc. lip gloss and key chain,” I responded.

And so it went for 30 minutes. Other than the lip gloss, he was particularly interested in my passport (luckily it was really mine), my PowerPoint presentation, the random pills floating among the crevices of my bag (covered with lint and pencil lead from years in purgatory), and a beat-up book (When Bad Things Happen to Good People).

Finally satisfied that I was playing for the home team, he allowed me to board a plane to Chicago. Thank God I had left at home my copy of the terrorist handbook that I blogged about recently, or I would have instead been flying straight to Cuba.

—SDL (July 14, 2005)

“Nobel Prize Winner Thomas Schelling”

I’ve changed addresses 10 times since I graduated from college. And each time I’ve moved, I’ve looked at the battered old box of college notebooks and debated whether it was time to throw the box out. After all, it has been more than 15 years and the box has never once been opened.

Thomas Schelling winning the Nobel Prize in Economics finally gave me a reason to open the box. My sophomore year in college, I took Econ 1030 from Schelling. I believe the course was entitled something like “Conflict and Strategy.” I still have vivid memories of the course. A crew-cut Schelling paced back and forth across the stage (never with any notes, if I remember correctly), spewing forth story after story that illuminated the application of simple game-theory concepts in everyday life. The pauses between the stories were long enough that I had the impression he was coming up with them on the spot, although my own experience as a teacher makes me think otherwise.

For me, this first introduction to game theory was inspirational. For someone who thinks strategically, or would like to think strategically, the basic tools of game theory are essential. The beauty of Schelling’s class was how easy the math was and how readily it applied to real world settings. The topics of the course were basic: the Prisoner’s Dilemma in lecture 1; Schelling’s own “tipping point” model in lectures 2 and 3; the tragedy of the commons and public-goods games after that; then commitment devices, credible and non-credible threats, and the strategy and tactics of controlling one’s own behavior. (For those who are unaware, Schelling coined the term “tipping point” thirty years before Malcolm Gladwell made it popular.)

Any economist could have taught the subjects in the class, but no one would have taught it as Schelling did. Each concept was accompanied by a barrage of examples. My notes are so poorly done—I would write down only a few key words—that now I can only guess at what the story was behind the words: “when Rhodesia became Zimbabwe,” “VHS vs. Beta,” “the quality of play in bridge leagues,” “choosing colleges,” “Dulles vs. National airports,” “Bear Bryant should not have voted for USC,” “good weatherman takes fair bets,” “tailgating,” “Landon vs. Roosevelt,” “randomly flushing the toilet,” etc.

I even remember attempting to put Schelling’s lessons immediately into practice. People who know me know that I can fall asleep anywhere, anytime. I would guess that I slept through some portion of 90 percent of my college classes. So when Schelling taught us about commitment, I decided I would start sitting in the front row of class as a way of committing myself not to sleep. Unfortunately, the urge to sleep often proved all too powerful. If Schelling were to remember me, it would be as the only kid in the first row who always fell asleep.

To my mind, Schelling represents the very best of game theory. He was a pioneer in the field, a man of ideas. Unfortunately for game theory, the simple ideas that are so alluring were quickly mined. What followed was less interesting. Modern game theory has become extremely mathematical, notation heavy, and removed from everyday life. Many of my colleagues would not agree with me, but I think game theory has failed to deliver on its enormous initial promise. I’m not the only one who feels this way. I was recently speaking with a prominent game theorist. He told me that if he knew what he knew and he were just getting started in the profession today, no way would he be a game theorist.

Schelling was an early inspiration to me. His course and writings were one of the big influences pushing me toward economics. My approach to economics shares much with his approach. I was saying this last year to one of my colleagues, who happened to run into Schelling and told Schelling he should count me as one of his students. Schelling was unmoved.

—SDL (Oct. 20, 2005)
